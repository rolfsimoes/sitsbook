# (PART) Overview {-}

# Introduction

```{r, include = FALSE}
source("common.R")
```

---

This chapter present an overview of sits. For detailed description of the functions, please see the following chapters. 

---

Earth observation (EO) satellites provide a common and consistent set of information about the planet's land and oceans. Recently, most space agencies have adopted open data policies, making unprecedented amounts of satellite data available for research and operational use. This data deluge has brought about a significant challenge: *How to design and build technologies that allow the Earth observation community to analyze big data sets?* 

In this book, we present **sits**, an open-source R package for satellite image time series analysis. It provides support on how to use machine learning techniques with image time series. The package supports the complete cycle of data analysis for time series classification, including data acquisition, visualization, filtering, clustering, classification, validation, and post-classification.

Satellite image time series are obtained by taking calibrated and comparable measures of the same location in Earth at different times[@Tan2017]. These measures can come from a single sensor (e.g., MODIS) or by combining various sensors (e.g., Landsat 8 and Sentinel-2). If acquired by frequent revisits, these data set can capture significant land use changes. As argued by [@Woodcock2020], *"dense time series analysis is providing new information on the timing of landscape changes, as well as improving the quality and accuracy of information being derived from remote sensing. The result is a paradigm shift away from change detection, typically using two points in time, to monitoring, or an attempt to track change continuously in time."*

```{r, echo = FALSE, out.width = "90%", out.height = "90%", fig.align="center", fig.cap="Using time series for land classification (source: Tan et al., 2017)"}
knitr::include_graphics("images/time_series_general_view.png")
```

Time series of remote sensing data show that land cover can occur not only progressively and gradually, but they may also show discontinuities with abrupt changes [@Lambin2003]. Analyses of multiyear time series of land surface attributes, their fine-scale spatial pattern, and their seasonal evolution lead to a broader view of land-cover change. Satellite image time series have  been used in applications such as mapping for detecting forest disturbance [@Kennedy2010], land disturbance [@Zhu2020], ecological dynamics [@Pasquarella2016], agricultural intensification [@Galford2008], and deforestation monitoring [@Arvor2012]. Algorithms for processing image time series include BFAST for detecting breaks [@Verbesselt2010], TIMESAT for extraction phenological attributes [@Jonsson2004], CCDC for continuous change detection [@Arevalo2020], and methods based on Dynamic Time Warping (DTW) for land use and land cover classification [@Petitjean2012, @Maus2019].

Compared with existing tools, sits has distinctive features:

1. A consistent API that encapsulates the entire land classification workflow in a few commands.
2. Integration with data cubes and Earth observation image collections available in cloud services such as AWS and Microsoft.
3. A single interface for different machine learning and deep learning algorithms.
4. Internal support for parallel processing, without requiring users to learn how to improve the performance of their scripts.
5. Support for efficient processing of large areas in a user-transparent way.
6. Innovative methods for sample quality control and post-processing.
7. Capacity to run on virtual machines in cloud environments.

Designing software to do big Earth observation data analysis requires balancing between flexibility, interoperability, efficiency, and ease of use. The approach taken in *sits* is different from software such as Google Earth Engine (GEE) [@Gorelick2017] and  Open Data Cube (ODC) [@Lewis2017]. Similar to GEE, *sits* provides a compact API. However, GEE is restricted to the Google environment and does not provide direct support for deep learning using image time series. Since *sits* runs in different cloud providers, users are flexible to choose the working environment that better fits their needs and resources. 

As for the ODC, the approach taken by its designers is that of maximum flexibility. ODC provides users with data organized into a python *xarray* structure. Since ODC does not provide an API to work with xarrays, specialists have to develop their applications by themselves. This choice by ODC designers aims at allowing experts to develop many different types of applications. Such flexibility comes a cost for studies involving large areas that require parallel processing. In these cases, experts have to learn how to parallelize their scripts. By contrast, the *sits* API focus on providing a simple and powerful environment for land classification. Parellel processing is done internally and is user-transparent.  We consider that the combination of user-centered design and interoperability is *sits* is well suited for experts that want to use satellite data to land-related data analysis.

## Workflow and API

The main aim of sits is to support land cover and land change classification of image data cubes using machine learning methods. As such, the basic workflow and API is:

1. Create a data cube based on analysis-ready data image collections available in the cloud or local machines using `sits_cube`.
2. Provide a description of the training data as a CSV or SHP file.
3. Obtain the time series for the training samples using `sits_get_data`.
4. Validate the samples using `sits_kfold_validate`.
5. Improve the quality of the samples using `sits_som_map` and `sits_som_clean_samples`.
6. Train a machine learning model with `sits_train`.
7. Classify the data cubes with `sits_classify`, producing a cube with class probabilities.
8. Smoothen the probability data to reduce classification noise with `sits_smooth`.
9. Label the probability cube with `sits_label_classification`. 
10. Assess the quality of the classification with `sits_accuracy`.

Figure \@ref(fig:workflow) shows a generic vision of the workflow for land classification.
```{r workflow, echo = FALSE, out.width = "80%", out.height = "80%", fig.align="center", fig.cap="Workflow of using satellite image time series for classification"}

knitr::include_graphics("images/sits_workflow.png")
```

Figure \@ref(fig:api) shows how the *sits* API is used to perform the main steps of the workflow. These functions are: (a) `sits_cube` which creates a cube; (b) `sits_get_data` which extracts training data from the cube; (c) `sits_train` that trains a machine learning model; (d) `sits_classify` which classifies the cube; (e) `sits_label_classication` that produces the final labelled image. These five functions encapsulate the core of the *sits* package.  

```{r api, echo = FALSE, out.width = "80%", out.height = "80%", fig.align="center", fig.cap="Main functions of the SITS API"}

knitr::include_graphics("images/sits_api.png")
```

## Handling Data Cubes in sits

Currently, *sits* supports data cubes available in the following cloud services:

1. Sentinel-2/2A level 2A images in Amazon Web Services (AWS);
2. Collections of Sentinel, Landsat, and CBERS images in the Brazil Data Cube (BDC);
3. Collections available in Digital Earth Africa;
4. Data cubes produced by the [gdalcubes package](https://github.com/appelmar/gdalcubes);
5. Local image collections.

The user defines a data cube by selecting a cloud service collection and determining a space-time extent. The code below shows the definition of a data cube using AWS Sentinel-2/2A images to exemplify how it is used. In the example, the user selects the "Sentinel-2 Level 2" collection in the AWS cloud services. The data cube's geographical area is defined by the tile "20LKP" and the temporal extent by a start and end date. Access to other cloud services works in similar ways (See [Chapter 2](https://e-sensing.github.io/sitsbook/earth-observation-data-cubes.html) for more details).

```{r, eval = FALSE, echo = TRUE}
s2_cube <- sits_cube(
    source        = "AWS",
    name          = "T20LKP_2018_2019",
    collection    = "sentinel-s2-l2a",
    bands         = c("B08", "SCL"),
    tiles         = "20LKP",
    start_date    = as.Date("2018-07-18"),
    end_date      = as.Date("2018-08-18"),
    s2_resolution = 60
)
```

To define a data cube using plain files, all image files should have the same spatial resolution and same projection and be in the same directory. Each file should contain a single image band and cover single date. Since timeline and band information is deduced from filenames, users should provide parsing information to allow *sits* to extract the band and the date. As an example, for the file `CBERS-4_AWFI_B13_2018-02-02.tif`, the parsing info is `c("X1", "X2", "band", "date")`.

```{r}
library(sits)
# Create a cube based on a stack of CBERS data
data_dir <- system.file("extdata/raster/cbers", package = "sits")

# files are named using the convention 
# "CBERS-4_AWFI_B13_2018-02-02.tif"
cbers_cube <- sits_cube(
      source = "LOCAL",
      name = "022024",
      satellite = "CBERS-4",
      sensor = "AWFI",
      data_dir = data_dir,
      parse_info = c("X1", "X2", "band", "date")
)

# print the timeline of the cube
sits_timeline(cbers_cube)

```

## Handling satellite image time series in **sits**

### Data structure

Training a machine learning model in **sits** requires a set of time series describing properties in spatiotemporal locations of interest. This set consists of samples provided by experts that take in situ field observations or recognize land classes using high-resolution images for land use classification. 

The package uses a `sits tibble` to organize time series data with associated spatial information for handling time series. As an example of how the sits tibble works, the following code shows the first three lines of a tibble containing $1,218$ labeled samples of land cover in the Mato Grosso state of Brazil, with four classes: "Forest", "Cerrado", "Pasture", "Soybean-Corn". Training samples organized as a tibble can be used to training a machine learning model with `sits_train`.  

```{r}
# data set of samples
data("samples_modis_4bands")
samples_modis_4bands[1:3,]
```

A `sits tibble` contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The spatial location is given in longitude and latitude coordinates for the "WGS84" ellipsoid. For example, the first sample has been labeled "Pasture" at location ($-55.1852$, $-10.8378$) and is valid for the period (2013-09-14, 2014-08-29).  The `time_series` column contains the actual data. 

### Obtaining time series data

To get a time series in *sits*, user should create a data cube and then extract one or more time series from the cube using `sits_get_data()`. Users should provide a CSV or SHP file that includes latitude and longitude of the desired location, bands, and start date and end date of the time series. 

```{r, fig.align="center", fig.height=3.1, fig.width=5, fig.cap="A one year time series of MOD13Q1 data for bands NDVI and EVI"}
library(sits)
data_dir <- system.file("extdata/raster/mod13q1", package = "sits")
modis_cube <- sits_cube(
    source = "LOCAL",
    name = "sinop-2014",
    satellite = "TERRA",
    sensor = "MODIS",
    data_dir = data_dir,
    delim = "_",
    parse_info = c("X1", "X2", "band", "date")
)
# obtain a set of locations defined by a CSV file
csv_raster_file <- system.file("extdata/samples/samples_sinop_crop.csv",
                               package = "sits")
# retrieve the points from the data cube
points <- sits_get_data(modis_cube, file = csv_raster_file)
# plot the first point
plot(points[1,])
```

## Sample quality control using clustering

One of the key challenges of machine learning is improving the quality of the training samples. Good samples lead to good classification maps. It is recommended that users apply pre-processing methods to identify samples that might have been wrongly labeled or have low discriminatory power. *sits* provides support for two clustering methods to evaluate sample quality: (a) Agglomerative Hierarchical Clustering (AHC); (b)  Self-organizing Maps (SOM).  Full details of the clustering methods are available in [Chapter 4 ](https://e-sensing.github.io/sitsbook/time-series-clustering-to-improve-the-quality-of-training-samples.html). 

## Classification using machine learning 

The package provides functionality to explore the full depth of satellite image time series data, treating time series as a feature vector. It uses as many temporal attributes as possible, increasing the classification space's dimension. The `sits_train` function provides a common interface to all machine learning (ML) models, with two parameters: the input samples and the ML method (`ml_method`). After the model is estimated, users can classify individual time series or full data cubes with `sits_classify`. In the example below, we train a ML model to classify a MODIS time series (described above). The classification results can be shown in text format using `sits_show_prediction` or graphically `plot`.

```{r, fig.align="center", fig.height=3.4, fig.width=5.5, fig.cap="Random forest classification of a $16$ years time series. The location (latitude, longitude) shown at the top of the graph is in geographic coordinate system (WGS84 {\\it datum})."}
#select the data for classification
# Train a machine learning model using Random Forest
rfor_model <- sits_train(data = samples_modis_4bands, 
                    ml_method = sits_rfor(num_trees = 1000))
# get a point to be classified (select 4 bands to match the model)
point_4bands <- sits_select(point_mt_6bands, 
                            bands = c("NDVI", "EVI", "NIR", "MIR"))
# Classify using random forest model
class <- sits_classify(point_4bands, rfor_model)
# plot the results of the prediction
plot(class)
```

The following machine learning methods are available:

* Linear discriminant analysis (`sits_lda`)
* Quadratic discriminant analysis (`sits_qda`)
* Multinomial logit and its variants 'lasso' and 'ridge' (`sits_mlr`)
* Support vector machines (`sits_svm`)
* Random forests (`sits_rfor`)
* Extreme gradient boosting (`sits_xgboost`)
* Deep learning (DL) using multi-layer perceptrons (`sits_deeplearning`)
* DL combining 1D CNN and multi-layer perceptron networks (`sits_tempCNN`)
* DL using 1D version of ResNet (`sits_ResNet`).

For more details, please see [Chapter 5](https://e-sensing.github.io/sitsbook/machine-learning-for-data-cubes-using-the-sits-package.html). 

## Cube classification

Given a data cube and a trained ML model, the function `sits_classify()` assigns class probability to each time series. To optimize processing time, users can adjust the function according to the server's capabilities, configuring the available memory (`memsize`) and the number of cores to be used (`multicores`). 

```{r, fig.align="center", fig.cap="Class probabilities for each pixel"}
# create a data cube to be classified
# composed of MOD13Q1 images from the Sinop region in Mato Grosso (Brazil)
data_dir <- system.file("extdata/raster/mod13q1", package = "sits")
sinop <- sits_cube(
    source = "LOCAL",
    name = "sinop-2014",
    satellite = "TERRA",
    sensor = "MODIS",
    data_dir = data_dir,
    parse_info = c("X1", "X2", "band", "date")
)

# Retrieve the set of samples for the Mato Grosso region 
# these samples have already been quality controlled 
samples_2bands  <- sits_select(samples_modis_4bands, 
                               bands = c("NDVI", "EVI"))
# build a machine learning model for this area
svm_model <- sits_train(data = samples_2bands, ml_method = sits_svm())

# Classify the raster cube, generating a probability file
probs_cube <- sits_classify(sinop, 
                            ml_model = svm_model, 
                            output_dir = tempdir(),
                            memsize = 16,
                            multicores = 4,
                            verbose = FALSE)
# plot the probabilities cube
plot(probs_cube)
```

When applied to a data cube, `sits_classify` produces one probability image for each land class. These images are grouped in a probability cube. The value of each pixel of each class is proportional to the class probability estimated by the machine learning classifier, as shown in the Figure above. 

## Smoothing and Labelling of raster data after classification

Post-processing is a desirable step in any classification process. Most statistical classifiers use training samples derived from single pixels. However, images contain many mixed pixels irrespective of the resolution. Also, there is a considerable degree of data variability in each class. Smoothing methods available in `sits_smooth` use the neighborhood information to remove outliers and enhance consistency in the resulting product. By default, this function selects the most likely class for each pixel using a Bayesian estimator that considers the neighbors. Alternatives are gaussian and bilateral smoothing. The resulting cube can be labeled using `sits_label_classification`. This function selects the most likely class in each pixel and assigns it to the final classified image. 

```{r, out.width = "90%", out.height = "90%", fig.align="center", fig.cap="Classified image post-processed with Bayesian smoothing"}
# smooth the result with a bayesian filter
sinop_bayes <- sits_smooth(probs_cube, output_dir = tempdir())
# label the resulting image
label_bayes <- sits_label_classification(sinop_bayes, output_dir = tempdir())
# plot the result
plot(label_bayes)
```
## Validation techniques

Validation is a process undertaken on models to estimate their prediction errors. **sits** supports the *k-fold cross-validation* approach, probably the most used validation technique [@Hastie2009] using the function `sits_kfold_validate`. The following code performs a five-fold validation using the SVM classification model as a default classifier. By default, `sits_kfold_validate` provides the overall accuracy metrics. The detailed metrics 

```{r}
# perform a five fold validation for the "cerrado_2classes" data set
# Random Forest machine learning method using default parameters
acc <- sits_kfold_validate(cerrado_2classes, 
                           folds = 5, 
                           ml_method = sits_rfor(num_trees = 1000))
```

```{r, eval = FALSE}
# print detailed validation metrics (not shown here to reduce space)
sits_accuracy_summary(acc)
```

More details about validation are available in [Chapter 8](https://e-sensing.github.io/sitsbook/validation-and-accuracy-measurements-in-sits.html)

## Final remarks

The **sits** package provides an API to build EO data cubes from image collections available in cloud services, and to perform land classification of data cubes using machine learning. The classification models are built based on  satellite image time series extracted from the cubes. The package provides additional function for sample quality control, post-processing and validation. The design of the API tries to reduce complexity for users and hide details such as how to do parallel processing, and to handle data cubes composed by tiles of different timelines.
