<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="5.5 Random forests | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series" />
<meta property="og:type" content="book" />

<meta property="og:image" content="images/cover.png" />
<meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />



<meta name="date" content="2021-03-30" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>">

<title>5.5 Random forests | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li class="has-sub"><a href="1-introduction.html#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="1-1-workflow.html#workflow"><span class="toc-section-number">1.1</span> Workflow</a></li>
<li class="has-sub"><a href="1-2-handling-data-cubes-in-sits.html#handling-data-cubes-in-sits"><span class="toc-section-number">1.2</span> Handling Data Cubes in <strong>sits</strong></a><ul>
<li><a href="1-2-handling-data-cubes-in-sits.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><span class="toc-section-number">1.2.1</span> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li><a href="1-2-handling-data-cubes-in-sits.html#using-stac-to-access-image-data-cubes"><span class="toc-section-number">1.2.2</span> Using STAC to Access Image Data Cubes</a></li>
<li><a href="1-2-handling-data-cubes-in-sits.html#defining-a-data-cube-using-files"><span class="toc-section-number">1.2.3</span> Defining a data cube using files</a></li>
</ul></li>
<li class="has-sub"><a href="1-3-handling-satellite-image-time-series-in-sits.html#handling-satellite-image-time-series-in-sits"><span class="toc-section-number">1.3</span> Handling satellite image time series in <strong>sits</strong></a><ul>
<li><a href="1-3-handling-satellite-image-time-series-in-sits.html#data-structure"><span class="toc-section-number">1.3.1</span> Data structure</a></li>
<li><a href="1-3-handling-satellite-image-time-series-in-sits.html#obtaining-time-series-data"><span class="toc-section-number">1.3.2</span> Obtaining time series data</a></li>
</ul></li>
<li><a href="1-4-filtering-techniques.html#filtering-techniques"><span class="toc-section-number">1.4</span> Filtering techniques</a></li>
<li><a href="1-5-clustering-for-sample-quality-control-using-self-organizing-maps.html#clustering-for-sample-quality-control-using-self-organizing-maps"><span class="toc-section-number">1.5</span> Clustering for sample quality control using self-organizing maps</a></li>
<li><a href="1-6-classification-using-machine-learning.html#classification-using-machine-learning"><span class="toc-section-number">1.6</span> Classification using machine learning</a></li>
<li><a href="1-7-validation-techniques.html#validation-techniques"><span class="toc-section-number">1.7</span> Validation techniques</a></li>
<li class="has-sub"><a href="1-8-cube-classification.html#cube-classification"><span class="toc-section-number">1.8</span> Cube classification</a><ul>
<li><a href="1-8-cube-classification.html#steps-for-cube-classification"><span class="toc-section-number">1.8.1</span> Steps for cube classification</a></li>
<li><a href="1-8-cube-classification.html#adjustments-for-improved-performance"><span class="toc-section-number">1.8.2</span> Adjustments for improved performance</a></li>
</ul></li>
<li><a href="1-9-smoothing-and-labelling-of-raster-data-after-classification.html#smoothing-and-labelling-of-raster-data-after-classification"><span class="toc-section-number">1.9</span> Smoothing and Labelling of raster data after classification</a></li>
<li><a href="1-10-final-remarks.html#final-remarks"><span class="toc-section-number">1.10</span> Final remarks</a></li>
<li><a href="1-11-acknowledgements.html#acknowledgements"><span class="toc-section-number">1.11</span> Acknowledgements</a></li>
</ul></li>
<li class="has-sub"><a href="2-acessing-time-series-information-in-sits.html#acessing-time-series-information-in-sits"><span class="toc-section-number">2</span> Acessing time series information in SITS</a><ul>
<li><a href="2-1-data-structures-for-satellite-time-series.html#data-structures-for-satellite-time-series"><span class="toc-section-number">2.1</span> Data structures for satellite time series</a></li>
<li><a href="2-2-utilities-for-handling-time-series.html#utilities-for-handling-time-series"><span class="toc-section-number">2.2</span> Utilities for handling time series</a></li>
<li><a href="2-3-time-series-visualisation.html#time-series-visualisation"><span class="toc-section-number">2.3</span> Time series visualisation</a></li>
<li><a href="2-4-obtaining-time-series-data-from-data-cubes.html#obtaining-time-series-data-from-data-cubes"><span class="toc-section-number">2.4</span> Obtaining time series data from data cubes</a></li>
</ul></li>
<li class="has-sub"><a href="3-satellite-image-time-series-filtering-with-sits.html#satellite-image-time-series-filtering-with-sits"><span class="toc-section-number">3</span> Satellite Image Time Series Filtering with SITS</a><ul>
<li><a href="3-1-filtering-techniques-in-sits.html#filtering-techniques-in-sits"><span class="toc-section-number">3.1</span> Filtering techniques in SITS</a></li>
<li class="has-sub"><a href="3-2-common-interface-to-sits-filter-functions.html#common-interface-to-sits-filter-functions"><span class="toc-section-number">3.2</span> Common interface to SITS filter functions</a><ul>
<li><a href="3-2-common-interface-to-sits-filter-functions.html#savitzkygolay-filter"><span class="toc-section-number">3.2.1</span> Savitzky–Golay filter</a></li>
<li><a href="3-2-common-interface-to-sits-filter-functions.html#whittaker-filter"><span class="toc-section-number">3.2.2</span> Whittaker filter</a></li>
<li><a href="3-2-common-interface-to-sits-filter-functions.html#envelope-filter"><span class="toc-section-number">3.2.3</span> Envelope filter</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-time-series-clustering-to-improve-the-quality-of-training-samples.html#time-series-clustering-to-improve-the-quality-of-training-samples"><span class="toc-section-number">4</span> Time Series Clustering to Improve the Quality of Training Samples</a><ul>
<li><a href="4-1-clustering-for-sample-quality-control.html#clustering-for-sample-quality-control"><span class="toc-section-number">4.1</span> Clustering for sample quality control</a></li>
<li class="has-sub"><a href="4-2-hierachical-clustering-for-sample-quality-control.html#hierachical-clustering-for-sample-quality-control"><span class="toc-section-number">4.2</span> Hierachical clustering for Sample Quality Control</a><ul>
<li><a href="4-2-hierachical-clustering-for-sample-quality-control.html#creating-a-dendogram"><span class="toc-section-number">4.2.1</span> Creating a dendogram</a></li>
<li><a href="4-2-hierachical-clustering-for-sample-quality-control.html#using-a-dendrogram-to-evaluate-sample-quality"><span class="toc-section-number">4.2.2</span> Using a dendrogram to evaluate sample quality</a></li>
</ul></li>
<li class="has-sub"><a href="4-3-using-self-organizing-maps-for-sample-quality.html#using-self-organizing-maps-for-sample-quality"><span class="toc-section-number">4.3</span> Using Self-organizing Maps for Sample Quality</a><ul>
<li><a href="4-3-using-self-organizing-maps-for-sample-quality.html#introduction-to-self-organizing-maps"><span class="toc-section-number">4.3.1</span> Introduction to Self-organizing Maps</a></li>
<li><a href="4-3-using-self-organizing-maps-for-sample-quality.html#using-som-for-removing-class-noise"><span class="toc-section-number">4.3.2</span> Using SOM for removing class noise</a></li>
<li><a href="4-3-using-self-organizing-maps-for-sample-quality.html#comparing-global-accuracy-of-original-and-clean-samples"><span class="toc-section-number">4.3.3</span> Comparing Global Accuracy of Original and Clean Samples</a></li>
</ul></li>
<li><a href="4-4-conclusion.html#conclusion"><span class="toc-section-number">4.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="5-machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-for-data-cubes-using-the-sits-package"><span class="toc-section-number">5</span> Machine Learning for Data Cubes using the SITS package</a><ul>
<li><a href="5-1-machine-learning-classification.html#machine-learning-classification"><span class="toc-section-number">5.1</span> Machine learning classification</a></li>
<li><a href="5-2-data-used-in-the-machine-learning-examples.html#data-used-in-the-machine-learning-examples"><span class="toc-section-number">5.2</span> Data used in the machine learning examples</a></li>
<li><a href="5-3-visualizing-samples.html#visualizing-samples"><span class="toc-section-number">5.3</span> Visualizing Samples</a></li>
<li><a href="5-4-common-interface-to-machine-learning-and-deeplearning-models.html#common-interface-to-machine-learning-and-deeplearning-models"><span class="toc-section-number">5.4</span> Common interface to machine learning and deeplearning models</a></li>
<li><a href="5-5-random-forests.html#random-forests"><span class="toc-section-number">5.5</span> Random forests</a></li>
<li><a href="5-6-support-vector-machines.html#support-vector-machines"><span class="toc-section-number">5.6</span> Support Vector Machines</a></li>
<li><a href="5-7-extreme-gradient-boosting.html#extreme-gradient-boosting"><span class="toc-section-number">5.7</span> Extreme Gradient Boosting</a></li>
<li><a href="5-8-deep-learning-using-multi-layer-perceptrons.html#deep-learning-using-multi-layer-perceptrons"><span class="toc-section-number">5.8</span> Deep learning using multi-layer perceptrons</a></li>
<li><a href="5-9-d-convolutional-neural-networks.html#d-convolutional-neural-networks"><span class="toc-section-number">5.9</span> 1D Convolutional Neural Networks</a></li>
<li><a href="5-10-residual-1d-cnn-networks-resnet.html#residual-1d-cnn-networks-resnet"><span class="toc-section-number">5.10</span> Residual 1D CNN Networks (ResNet)</a></li>
</ul></li>
<li class="has-sub"><a href="6-classification-of-images-in-data-cubes-using-satellite-image-time-series.html#classification-of-images-in-data-cubes-using-satellite-image-time-series"><span class="toc-section-number">6</span> Classification of Images in Data Cubes using Satellite Image Time Series</a><ul>
<li><a href="6-1-image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis-1.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis-1"><span class="toc-section-number">6.1</span> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li><a href="6-2-defining-a-data-cube-using-files-organised-as-raster-bricks.html#defining-a-data-cube-using-files-organised-as-raster-bricks"><span class="toc-section-number">6.2</span> Defining a data cube using files organised as raster bricks</a></li>
<li><a href="6-3-classification-using-machine-learning-1.html#classification-using-machine-learning-1"><span class="toc-section-number">6.3</span> Classification using machine learning</a></li>
<li class="has-sub"><a href="6-4-cube-classification-1.html#cube-classification-1"><span class="toc-section-number">6.4</span> Cube classification</a><ul>
<li><a href="6-4-cube-classification-1.html#steps-for-cube-classification-1"><span class="toc-section-number">6.4.1</span> Steps for cube classification</a></li>
<li><a href="6-4-cube-classification-1.html#adjustments-for-improved-performance-1"><span class="toc-section-number">6.4.2</span> Adjustments for improved performance</a></li>
</ul></li>
<li><a href="6-5-final-remarks-1.html#final-remarks-1"><span class="toc-section-number">6.5</span> Final remarks</a></li>
</ul></li>
<li class="has-sub"><a href="7-post-classification-smoothing-using-bayesian-techniques-in-sits.html#post-classification-smoothing-using-bayesian-techniques-in-sits"><span class="toc-section-number">7</span> Post classification smoothing using Bayesian techniques in SITS</a><ul>
<li><a href="7-1-introduction-1.html#introduction-1"><span class="toc-section-number">7.1</span> Introduction</a></li>
<li class="has-sub"><a href="7-2-overview-of-bayesian-estimattion.html#overview-of-bayesian-estimattion"><span class="toc-section-number">7.2</span> Overview of Bayesian estimattion</a><ul>
<li><a href="7-2-overview-of-bayesian-estimattion.html#smmothing-using-bayes-rule"><span class="toc-section-number">7.2.1</span> Smmothing using Bayes’ rule</a></li>
</ul></li>
<li><a href="7-3-use-of-bayesian-smoothing-in-sits.html#use-of-bayesian-smoothing-in-sits"><span class="toc-section-number">7.3</span> Use of Bayesian smoothing in SITS</a></li>
</ul></li>
<li class="has-sub"><a href="8-validation-and-accuracy-measurements-in-sits.html#validation-and-accuracy-measurements-in-sits"><span class="toc-section-number">8</span> Validation and accuracy measurements in SITS</a><ul>
<li><a href="8-1-validation-techniques-1.html#validation-techniques-1"><span class="toc-section-number">8.1</span> Validation techniques</a></li>
<li><a href="8-2-comparing-different-validation-methods.html#comparing-different-validation-methods"><span class="toc-section-number">8.2</span> Comparing different validation methods</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">5.5</span> Random forests</h2>
<p>The Random forest uses the idea of <em>decision trees</em> as its base model. It combines many decision trees via <em>bootstrap</em> procedure and <em>stochastic feature selection</em>, developing a population of somewhat uncorrelated base models. The final classification model is obtained by a majority voting schema. This procedure decreases the classification variance, improving prediction of individual decision trees.</p>
<p>Random forest training process is essentially nondeterministic. It starts by growing trees through repeatedly random sampling-with-replacement the observations set. At each growing tree, the random forest considers only a fraction of the original attributes to decide where to split a node, according to a <em>purity criterion</em>. This criterion is used to identify relevant features and to perform variable selection. This decreases the correlation among trees and improves the prediction performance. Two often-used impurity criteria are the <em>Gini</em> index and the <em>permutation</em> measure. The Gini index considers the contribution of each variable which improves the spliting criteria for building tress. Permutation increases the importance of variables that have a positive effect on the prediction accuracy. The splitting process continues until the tree reaches some given minimum nodes size or a minimum impurity index value.</p>
<p>One of the advantages of the random forest model is that the classification performance is mostly dependent on the number of decision trees to grow and of the “importance” parameter, which controls the purity variable importance measures. SITS provides a <code>sits_rfor</code> function which is a front-end to the <code>ranger</code> package<span class="citation">(Wright and Ziegler 2017)</span>; its two main parameters are: <code>num_trees</code> (number of trees to grow) and <code>importance</code>, the variable importance criterion. Possible values for <code>importance</code> are: <code>none</code>, <code>impurity</code> (Gini index), and <code>permutation</code>, the default being <code>impurity</code>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="5-5-random-forests.html#cb65-1"></a><span class="co"># Retrieve the set of samples (provided by EMBRAPA) from the </span></span>
<span id="cb65-2"><a href="5-5-random-forests.html#cb65-2"></a><span class="co"># Mato Grosso region for train the Random Forest model.</span></span>
<span id="cb65-3"><a href="5-5-random-forests.html#cb65-3"></a>rfor_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, <span class="kw">sits_rfor</span>())</span>
<span id="cb65-4"><a href="5-5-random-forests.html#cb65-4"></a><span class="co"># Classify using Random Forest model and plot the result</span></span>
<span id="cb65-5"><a href="5-5-random-forests.html#cb65-5"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb65-6"><a href="5-5-random-forests.html#cb65-6"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-7"><a href="5-5-random-forests.html#cb65-7"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.2</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-8"><a href="5-5-random-forests.html#cb65-8"></a><span class="st">    </span><span class="kw">sits_classify</span>(rfor_model)</span>
<span id="cb65-9"><a href="5-5-random-forests.html#cb65-9"></a><span class="co"># plot classification</span></span>
<span id="cb65-10"><a href="5-5-random-forests.html#cb65-10"></a>class.tb <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-11"><a href="5-5-random-forests.html#cb65-11"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/eval-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="5-5-random-forests.html#cb66-1"></a><span class="co"># show the results of the prediction</span></span>
<span id="cb66-2"><a href="5-5-random-forests.html#cb66-2"></a><span class="kw">sits_show_prediction</span>(class.tb)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 17 x 3
#&gt;    from       to         class   
#&gt;    &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   
#&gt;  1 2000-09-13 2001-08-29 Forest  
#&gt;  2 2001-09-14 2002-08-29 Forest  
#&gt;  3 2002-09-14 2003-08-29 Forest  
#&gt;  4 2003-09-14 2004-08-28 Pasture 
#&gt;  5 2004-09-13 2005-08-29 Pasture 
#&gt;  6 2005-09-14 2006-08-29 Pasture 
#&gt;  7 2006-09-14 2007-08-29 Pasture 
#&gt;  8 2007-09-14 2008-08-28 Pasture 
#&gt;  9 2008-09-13 2009-08-29 Pasture 
#&gt; 10 2009-09-14 2010-08-29 Soy_Corn
#&gt; 11 2010-09-14 2011-08-29 Soy_Corn
#&gt; 12 2011-09-14 2012-08-28 Soy_Corn
#&gt; 13 2012-09-13 2013-08-29 Soy_Corn
#&gt; 14 2013-09-14 2014-08-29 Soy_Corn
#&gt; 15 2014-09-14 2015-08-29 Soy_Corn
#&gt; 16 2015-09-14 2016-08-28 Soy_Corn
#&gt; 17 2016-09-13 2017-08-29 Soy_Corn</code></pre>
<p>The result shows the tendency of the random forest classifier to be robust to outliers and to be able to deal with irrelevant inputs <span class="citation">(Hastie, Tibshirani, and J. 2009)</span>. Performs internal variable selection helps the results be robust to outliers and noise, a common feature in image time series. However, despite being robust, random forest tend to overemphasize some variables and thus rarely turn out to be the classifier with the smallest error. One reason is that the performance of random forest tends to stabilise after a part of the trees are grown <span class="citation">(Hastie, Tibshirani, and J. 2009)</span>. Random forest classifiers can be quite useful to provide a baseline to compare with more sophisticated methods.</p>
</div>
<p style="text-align: center;">
<a href="5-4-common-interface-to-machine-learning-and-deeplearning-models.html"><button class="btn btn-default">Previous</button></a>
<a href="5-6-support-vector-machines.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
