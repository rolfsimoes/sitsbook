<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Time Series Clustering to Improve the Quality of Training Samples | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</title>
  <meta name="description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="generator" content="bookdown 0.21.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Time Series Clustering to Improve the Quality of Training Samples | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Time Series Clustering to Improve the Quality of Training Samples | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  
  <meta name="twitter:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Rolf Simoes" />
<meta name="author" content="Gilberto Camara" />
<meta name="author" content="Felipe Souza" />
<meta name="author" content="Lorena Santos" />
<meta name="author" content="Pedro R. Andrade" />
<meta name="author" content="Alexandre Carvalho" />
<meta name="author" content="Karine Ferreira" />
<meta name="author" content="Gilberto Queiroz" />
<meta name="author" content="Victor Maus" />


<meta name="date" content="2021-04-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="working-with-time-series.html"/>
<link rel="next" href="machine-learning-for-data-cubes-using-the-sits-package.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SITS Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#publications-using-sits"><i class="fa fa-check"></i>Publications using sits</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reproducible-papers-used-in-building-sits-functions"><i class="fa fa-check"></i>Reproducible papers used in building sits functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a><ul>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html#docker-images"><i class="fa fa-check"></i>Docker images</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="part"><span><b>I Overview</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#workflow-and-api"><i class="fa fa-check"></i><b>1.1</b> Workflow and API</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#handling-data-cubes-in-sits"><i class="fa fa-check"></i><b>1.2</b> Handling Data Cubes in sits</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#handling-satellite-image-time-series-in-sits"><i class="fa fa-check"></i><b>1.3</b> Handling satellite image time series in <strong>sits</strong></a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#data-structure"><i class="fa fa-check"></i><b>1.3.1</b> Data structure</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#obtaining-time-series-data"><i class="fa fa-check"></i><b>1.3.2</b> Obtaining time series data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sample-quality-control-using-clustering"><i class="fa fa-check"></i><b>1.4</b> Sample quality control using clustering</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#classification-using-machine-learning"><i class="fa fa-check"></i><b>1.5</b> Classification using machine learning</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#cube-classification"><i class="fa fa-check"></i><b>1.6</b> Cube classification</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#smoothing-and-labelling-of-raster-data-after-classification"><i class="fa fa-check"></i><b>1.7</b> Smoothing and Labelling of raster data after classification</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#validation-techniques"><i class="fa fa-check"></i><b>1.8</b> Validation techniques</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#final-remarks"><i class="fa fa-check"></i><b>1.9</b> Final remarks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html"><i class="fa fa-check"></i><b>2</b> Earth observation data cubes</a><ul>
<li class="chapter" data-level="2.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="2.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-and-image-collections-in-sits"><i class="fa fa-check"></i><b>2.2</b> Accessing Data Cubes and Image Collections in SITS</a><ul>
<li class="chapter" data-level="2.2.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-in-amazon-web-services"><i class="fa fa-check"></i><b>2.2.1</b> Accessing data cubes in Amazon Web Services</a></li>
<li class="chapter" data-level="2.2.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-the-brazil-data-cube"><i class="fa fa-check"></i><b>2.2.2</b> Accessing the Brazil Data Cube</a></li>
<li class="chapter" data-level="2.2.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#defining-a-data-cube-using-files"><i class="fa fa-check"></i><b>2.2.3</b> Defining a data cube using files</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#regularizing-data-cubes"><i class="fa fa-check"></i><b>2.3</b> Regularizing data cubes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="working-with-time-series.html"><a href="working-with-time-series.html"><i class="fa fa-check"></i><b>3</b> Working with time series</a><ul>
<li class="chapter" data-level="3.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#data-structures-for-satellite-time-series"><i class="fa fa-check"></i><b>3.1</b> Data structures for satellite time series</a></li>
<li class="chapter" data-level="3.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#utilities-for-handling-time-series"><i class="fa fa-check"></i><b>3.2</b> Utilities for handling time series</a></li>
<li class="chapter" data-level="3.3" data-path="working-with-time-series.html"><a href="working-with-time-series.html#time-series-visualisation"><i class="fa fa-check"></i><b>3.3</b> Time series visualisation</a></li>
<li class="chapter" data-level="3.4" data-path="working-with-time-series.html"><a href="working-with-time-series.html#obtaining-time-series-data-from-data-cubes"><i class="fa fa-check"></i><b>3.4</b> Obtaining time series data from data cubes</a></li>
<li class="chapter" data-level="3.5" data-path="working-with-time-series.html"><a href="working-with-time-series.html#filtering-techniques-for-time-series"><i class="fa fa-check"></i><b>3.5</b> Filtering techniques for time series</a><ul>
<li class="chapter" data-level="3.5.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#savitzkygolay-filter"><i class="fa fa-check"></i><b>3.5.1</b> Savitzky–Golay filter</a></li>
<li class="chapter" data-level="3.5.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#whittaker-filter"><i class="fa fa-check"></i><b>3.5.2</b> Whittaker filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Clustering</b></span></li>
<li class="chapter" data-level="4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html"><i class="fa fa-check"></i><b>4</b> Time Series Clustering to Improve the Quality of Training Samples</a><ul>
<li class="chapter" data-level="4.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.1</b> Clustering for sample quality control</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#hierachical-clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.2</b> Hierachical clustering for Sample Quality Control</a><ul>
<li class="chapter" data-level="4.2.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#creating-a-dendogram"><i class="fa fa-check"></i><b>4.2.1</b> Creating a dendogram</a></li>
<li class="chapter" data-level="4.2.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-a-dendrogram-to-evaluate-sample-quality"><i class="fa fa-check"></i><b>4.2.2</b> Using a dendrogram to evaluate sample quality</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-self-organizing-maps-for-sample-quality"><i class="fa fa-check"></i><b>4.3</b> Using Self-organizing Maps for Sample Quality</a><ul>
<li class="chapter" data-level="4.3.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#introduction-to-self-organizing-maps"><i class="fa fa-check"></i><b>4.3.1</b> Introduction to Self-organizing Maps</a></li>
<li class="chapter" data-level="4.3.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-som-for-removing-class-noise"><i class="fa fa-check"></i><b>4.3.2</b> Using SOM for removing class noise</a></li>
<li class="chapter" data-level="4.3.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#comparing-global-accuracy-of-original-and-clean-samples"><i class="fa fa-check"></i><b>4.3.3</b> Comparing Global Accuracy of Original and Clean Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>III Classification</b></span></li>
<li class="chapter" data-level="5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html"><i class="fa fa-check"></i><b>5</b> Machine Learning for Data Cubes using the SITS package</a><ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-classification"><i class="fa fa-check"></i><b>5.1</b> Machine learning classification</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#visualizing-samples"><i class="fa fa-check"></i><b>5.2</b> Visualizing Samples</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#common-interface-to-machine-learning-and-deeplearning-models"><i class="fa fa-check"></i><b>5.3</b> Common interface to machine learning and deeplearning models</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#random-forests"><i class="fa fa-check"></i><b>5.4</b> Random forests</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#support-vector-machines"><i class="fa fa-check"></i><b>5.5</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#extreme-gradient-boosting"><i class="fa fa-check"></i><b>5.6</b> Extreme Gradient Boosting</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#deep-learning-using-multi-layer-perceptrons"><i class="fa fa-check"></i><b>5.7</b> Deep learning using multi-layer perceptrons</a></li>
<li class="chapter" data-level="5.8" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#combined-1d-cnn-and-multi-layer-perceptron-networks"><i class="fa fa-check"></i><b>5.8</b> Combined 1D CNN and multi-layer perceptron networks</a></li>
<li class="chapter" data-level="5.9" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#d-convolutional-neural-networks"><i class="fa fa-check"></i><b>5.9</b> 1D Convolutional Neural Networks</a></li>
<li class="chapter" data-level="5.10" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#residual-1d-cnn-networks-resnet"><i class="fa fa-check"></i><b>5.10</b> Residual 1D CNN Networks (ResNet)</a></li>
<li class="chapter" data-level="5.11" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#lstm-convolutional-networks-for-time-series-classification"><i class="fa fa-check"></i><b>5.11</b> LSTM Convolutional Networks for Time Series Classification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><i class="fa fa-check"></i><b>6</b> Classification of Images in Data Cubes using Satellite Image Time Series</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#data-cube-classification"><i class="fa fa-check"></i><b>6.1</b> Data cube classification</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#adjustments-for-improved-performance"><i class="fa fa-check"></i><b>6.2</b> Adjustments for improved performance</a></li>
<li class="chapter" data-level="6.3" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#final-remarks-1"><i class="fa fa-check"></i><b>6.3</b> Final remarks</a></li>
</ul></li>
<li class="part"><span><b>IV Post classification</b></span></li>
<li class="chapter" data-level="7" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html"><i class="fa fa-check"></i><b>7</b> Post classification smoothing</a><ul>
<li class="chapter" data-level="7.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bayesian-smoothing"><i class="fa fa-check"></i><b>7.2</b> Bayesian smoothing</a><ul>
<li class="chapter" data-level="7.2.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#derivation-of-bayesian-parameters-for-spatiotemporal-smoothing"><i class="fa fa-check"></i><b>7.2.1</b> Derivation of bayesian parameters for spatiotemporal smoothing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#use-of-bayesian-smoothing-in-sits"><i class="fa fa-check"></i><b>7.3</b> Use of Bayesian smoothing in SITS</a></li>
<li class="chapter" data-level="7.4" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bilateral-smoothing"><i class="fa fa-check"></i><b>7.4</b> Bilateral smoothing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html"><i class="fa fa-check"></i><b>8</b> Validation and accuracy measurements in SITS</a><ul>
<li class="chapter" data-level="8.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#validation-techniques-1"><i class="fa fa-check"></i><b>8.1</b> Validation techniques</a></li>
<li class="chapter" data-level="8.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#comparing-different-machine-learning-methods-using-k-fold-validation"><i class="fa fa-check"></i><b>8.2</b> Comparing different machine learning methods using k-fold validation</a></li>
<li class="chapter" data-level="8.3" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#accuracy-assessment"><i class="fa fa-check"></i><b>8.3</b> Accuracy assessment</a><ul>
<li class="chapter" data-level="8.3.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#time-series"><i class="fa fa-check"></i><b>8.3.1</b> Time series</a></li>
<li class="chapter" data-level="8.3.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#classified-images"><i class="fa fa-check"></i><b>8.3.2</b> Classified images</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>sits</strong>: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="time-series-clustering-to-improve-the-quality-of-training-samples" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Time Series Clustering to Improve the Quality of Training Samples</h1>
<hr />
<p>One of the key challenges when using samples to train machine learning classification models is assessing their quality. Noisy and imperfect training samples can have a negative effect on classification performance. Therefore, it is useful to apply pre-processing methods to improve the quality of the samples and to remove those that might have been wrongly labeled or that have low discriminatory power. Representative samples lead to good classification maps. <code>sits</code> provides support for two clustering methods to test sample quality, which is agglomerative hierarchical clustering (AHC) and self-organizing maps (SOM).</p>
<hr />
<div id="clustering-for-sample-quality-control" class="section level2">
<h2><span class="header-section-number">4.1</span> Clustering for sample quality control</h2>
<p>Recent results show that it is feasible to apply machine learning methods to SITS analysis in large areas of 100 million ha or more <span class="citation">(Picoli et al. 2018; Simoes et al. 2020; Parente et al. 2019; Griffiths et al. 2019)</span>. Experience with machine learning methods has established that the limiting factor in obtaining good results is the number and quality of training samples. Large and accurate data sets are better, no matter the algorithm used <span class="citation">(Maxwell, Warner, and Fang 2018)</span>; increasing the training sample size results in better classification accuracy <span class="citation">(Thanh Noi and Kappas 2018)</span>. Therefore, using machine learning for SITS analysis requires large and good quality training sets.</p>
<p>One of the key challenges when using samples to train machine learning classification models is assessing their quality. Noisy and imperfect training samples can have a negative effect on classification performance <span class="citation">(Frenay and Verleysen 2014)</span>. There are two main sources of noise and errors in satellite image time series. Feature noise is caused by clouds and inconsistencies in data calibration. Class noise occurs when the label assigned to the sample is wrongly attributed. Class noise effects are common on large data sets. In particular, interpreters tend to group samples with different properties in the same category. For this reason, one needs good methods for quality control of large training data sets associated with satellite image time series.</p>
<p>Many factors lead to class noise. One of the main problems is the inherent variability of class signatures in space and time. When training data is collected over a large geographic region, natural variability of vegetation phenology can result in different patterns being assigned to the same label. Phenological patterns can vary spatially across a region and are strongly correlated with climate variations. A related issue is the limitation of crisp boundaries to describe the natural world. Class definition use idealized descriptions (e.g., “a savanna woodland has tree cover of 50% to 90% ranging from 8 to 15 meters in height”). However, in practice, the boundaries between classes are fuzzy and sometimes overlap, making it hard to distinguish between them. Class noise can also result from labeling errors. Even trained analysts can make errors in class attributions. Despite the fact that machine learning techniques are robust to errors and inconsistencies in the training data, quality control of training data can make a significant difference in the resulting maps.</p>
<p>Therefore, it is useful to apply pre-processing methods to improve the quality of the samples and to remove those that might have been wrongly labeled or that have low discriminatory power. Representative samples lead to good classification maps. The package provides support for two clustering methods to test sample quality: (a) Agglomerative Hierarchical Clustering (AHC); (b) Self-organizing Maps (SOM).</p>
</div>
<div id="hierachical-clustering-for-sample-quality-control" class="section level2">
<h2><span class="header-section-number">4.2</span> Hierachical clustering for Sample Quality Control</h2>
<div id="creating-a-dendogram" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Creating a dendogram</h3>
<p>Cluster analysis is used in <em>sits</em> as a way to improve training data to feed machine learning classification models by identifying anomalous samples <span class="citation">(Frenay and Verleysen 2014)</span>.
The package uses <em>agglomerative hierarchical clustering</em> (AHC) to compute the dissimilarity between any two elements from a data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. AHC approach is suitable for the purposes of samples data exploration due to its visualization power and ease of use <span class="citation">(Keogh, Lin, and Truppel 2003)</span>. Moreover, AHC does not require a predefined number of clusters as an initial parameter. This is an important feature in satellite image time series clustering since defining the number of clusters present in a set of multi-attribute time series is not straightforward <span class="citation">(Aghabozorgi, Shirkhorshidi, and Wah 2015)</span>.</p>
<p>The main result of the AHC method is a <em>dendrogram</em>. It is the ultrametric relation formed by the successive merges in the hierarchical process that can be represented by a tree. Dendrograms are quite useful to decide the number of clusters to partition the data. It shows the height where each merging happens, which corresponds to the minimum distance between two clusters defined by a <em>linkage criterion</em>. The most common linkage criteria are: <em>single-linkage</em>, <em>complete-linkage</em>, <em>average-linkage</em>, and <em>Ward-linkage</em>. Complete-linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples. Complete-linkage clustering can be sensitive to outliers, which can increase the resulting intracluster data variance. As an alternative, Ward proposes criteria to minimize the data variance by means of either <em>sum-of-squares</em> or <em>sum-of-squares-error</em> <span class="citation">(Ward 1963)</span>. Ward’s intuition is that clusters of multivariate observations, such as time series, should be approximately elliptical in shape <span class="citation">(Hennig 2015)</span>. In <code>sits</code>, a dendrogram can be generated by <code>sits_dendrogram()</code>. The following codes illustrate how to create, visualize, and cut a dendrogram (for details, see <code>?sits_dendrogram()</code>).</p>
</div>
<div id="using-a-dendrogram-to-evaluate-sample-quality" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Using a dendrogram to evaluate sample quality</h3>
<p>After creating a dendrogram, an important question emerges: <em>where to cut the dendrogram?</em> The answer depends on what are the purposes of the cluster analysis. We need to balance two objectives: get clusters as large as possible, and get clusters as homogeneous as possible with respect to their known classes. To help this process, <code>sits</code> provides <code>sits_dendro_bestcut()</code> function that computes an external validity index <em>Adjusted Rand Index</em> (ARI) for a series of the different number of generated clusters. This function returns the height where the cut of the dendrogram maximizes the index.</p>
<p>In this example, the height optimizes the ARI and generates <span class="math inline">\(6\)</span> clusters. The ARI considers any pair of distinct samples and computes the following counts:
(a) the number of distinct pairs whose samples have the same label and are in the same cluster;
(b) the number of distinct pairs whose samples have the same label and are in different clusters;
(c) the number of distinct pairs whose samples have different labels and are in the same cluster; and
(d) the number of distinct pairs whose samples have the different labels and are in different clusters.
Here, <span class="math inline">\(a\)</span> and <span class="math inline">\(d\)</span> consist in all agreements, and <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> all disagreements. The ARI is obtained by:</p>
<p><span class="math display">\[
ARI=\frac{a+d-E}{a+d+b+c-E},
\]</span>
where <span class="math inline">\(E\)</span> is the expected agreement, a random chance correction calculated by
<span class="math display">\[
E=(a+b)(b+c)+(c+d)(b+d).
\]</span></p>
<p>Unlike other validity indexes such as Jaccard (<span class="math inline">\({J=a/(a+b+c)}\)</span>), Fowlkes-Mallows (<span class="math inline">\({FM=a/(a^2+a(b+c)+bc)^{1/2}}\)</span>), and Rand (the same as ARI without the <span class="math inline">\(E\)</span> adjustment) indices, ARI is more appropriate either when the number of clusters is outweighed by the number of labels (and <em>vice versa</em>) or when the number of samples in labels and clusters are imbalanced <span class="citation">(Hubert and Arabie 1985)</span>, which is usually the case.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-1"></a><span class="co"># take a set of patterns for 2 classes</span></span>
<span id="cb40-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-2"></a><span class="co"># create a dendrogram, plot, and get the optimal cluster based on ARI index</span></span>
<span id="cb40-3"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-3"></a>clusters &lt;-<span class="st"> </span>sits<span class="op">::</span><span class="kw">sits_cluster_dendro</span>(cerrado_2classes, </span>
<span id="cb40-4"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-4"></a>                                         <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span>
<span id="cb40-5"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-5"></a></span>
<span id="cb40-6"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-6"></a><span class="co"># show clusters samples frequency</span></span>
<span id="cb40-7"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb40-7"></a>sits<span class="op">::</span><span class="kw">sits_cluster_frequency</span>(clusters)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt;          
#&gt;             1   2   3   4   5   6 Total
#&gt;   Cerrado 203  13  23  80   1  80   400
#&gt;   Pasture   2 176  28   0 140   0   346
#&gt;   Total   205 189  51  80 141  80   746</code></pre>
<p><img src="sitsbook_files/figure-html/dendrogram-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note in this example that almost all clusters have a predominance of either “Cerrado” or “Pasture” classes with the exception of cluster <span class="math inline">\(3\)</span>. The contingency table plotted by <code>sits_cluster_frequency()</code> shows how the samples are distributed across the clusters and help to identify two kinds of confusion. The first is relative to those small amounts of samples in clusters dominated by another class (<em>e.g.</em> clusters <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(4\)</span>, <span class="math inline">\(5\)</span>, and <span class="math inline">\(6\)</span>), while the second is relative to those samples in non-dominated clusters (<em>e.g.</em> cluster <span class="math inline">\(3\)</span>). These confusions can be an indication of samples with poor quality, and inadequacy of selected parameters for cluster analysis, or even a natural confusion due to the inherent variability of the land classes.</p>
<p>The result of the <code>sits_cluster</code> operation is a <code>sits_tibble</code> with one additional column, called “cluster”. Thus, it is possible to remove clusters with mixed classes using standard <code>R</code> such as those in the <code>dplyr</code> package. In the example above, removing cluster <span class="math inline">\(3\)</span> can be done using the <code>dplyr::filter</code> function.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb42-1"></a><span class="co"># remove cluster 3 from the samples</span></span>
<span id="cb42-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb42-2"></a>clusters_new &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(clusters, cluster <span class="op">!=</span><span class="st"> </span><span class="dv">3</span>)</span>
<span id="cb42-3"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb42-3"></a></span>
<span id="cb42-4"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb42-4"></a><span class="co"># show new clusters samples frequency</span></span>
<span id="cb42-5"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb42-5"></a>sits<span class="op">::</span><span class="kw">sits_cluster_frequency</span>(clusters_new)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt;          
#&gt;             1   2   4   5   6 Total
#&gt;   Cerrado 203  13  80   1  80   377
#&gt;   Pasture   2 176   0 140   0   318
#&gt;   Total   205 189  80 141  80   695</code></pre>
<p>The resulting clusters still contained mixed labels, possibly resulting from outliers. In this case, users may want to remove the outliers and leave only the most frequent class. To do this, one can use <code>sits_cluster_clean()</code>, which removes all minority samples, as shown below.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb44-1"></a><span class="co"># clear clusters, leaving only the majority class in each cluster</span></span>
<span id="cb44-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb44-2"></a>clean &lt;-<span class="st"> </span>sits<span class="op">::</span><span class="kw">sits_cluster_clean</span>(clusters)</span>
<span id="cb44-3"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb44-3"></a><span class="co"># show clusters samples frequency</span></span>
<span id="cb44-4"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb44-4"></a><span class="kw">sits_cluster_frequency</span>(clean)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt;          
#&gt;             1   2   3   4   5   6 Total
#&gt;   Cerrado 203   0   0  80   0  80   363
#&gt;   Pasture   0 176  28   0 140   0   344
#&gt;   Total   203 176  28  80 140  80   707</code></pre>
</div>
</div>
<div id="using-self-organizing-maps-for-sample-quality" class="section level2">
<h2><span class="header-section-number">4.3</span> Using Self-organizing Maps for Sample Quality</h2>
<p><a href="https://www.kaggle.com/brazildatacube/sits-clustering-r" target="_blank"><img src="https://img.shields.io/badge/example-available-green"/></a></p>
<div id="introduction-to-self-organizing-maps" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Introduction to Self-organizing Maps</h3>
<p>As an alternative for hierarchical clustering for quality control of training samples, SITS provides a clustering technique based on self-organizing maps (SOM). SOM is a dimensionality reduction technique <span class="citation">(Kohonen 1990)</span>, where high-dimensional data is mapped into two dimensions, keeping the topological relations between data patterns. The input data is a set of training samples that are typical of a high dimension. For example, a time series of 25 instances of 4 spectral bands is a 100-dimensional data set. The general idea of SOM-based clustering is that, by projecting the high-dimensional data set of training samples into a 2D map, the units of the map (called “neurons”) compete for each sample. It is expected that good quality samples of each class should be close together in the resulting map. The neighbors of each neuron of a SOM map provide information on intra-class and inter-class variability.</p>
<p>The main steps of our proposed method for quality assessment of satellite image time series are shown in the figure below. The method uses self-organizing maps (SOM) to perform dimensionality reduction while preserving the topology of original datasets. Since SOM preserves the topological structure of neighborhoods in multiple dimensions, the resulting 2D map can be used as a set of clusters. Training samples that belong to the same class will usually be neighbors in 2D space. The neighbors of each neuron of a SOM map are also expected to be similar.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-38"></span>
<img src="images/methodology_bayes_som.png" alt="Using SOM for class noise reduction" width="90%" height="90%" />
<p class="caption">
Figure 4.1: Using SOM for class noise reduction
</p>
</div>
<p>As the figure shows, a SOM grid is composed of units called . The algorithm computes the distances of each member of the training set to all neurons and finds the neuron closest to the input, called the best matching unit (BMU). The weights of the BMU and its neighbors are updated so as to preserve their similarity <span class="citation">(Kohonen 2013)</span>. This mapping and adjustment procedure is done in several iterations. At each step, the extent of the change in the neurons diminishes until a convergence threshold is reached. The result is a 2D mapping of the training set, where similar elements of the input are mapped to the same neuron or to nearby ones. The resulting SOM grid combines dimensionality reduction with topological preservation.</p>
</div>
<div id="using-som-for-removing-class-noise" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Using SOM for removing class noise</h3>
<p>The process of clustering with SOM is done by <code>sits_som_map()</code>, which creates a self-organizing map and assesses the quality of the samples. This function uses the “kohonen” R package <span class="citation">(<span class="citeproc-not-found" data-reference-id="Wehrens2108"><strong>???</strong></span>)</span> to compute a SOM grid. Each sample is assigned to a neuron, and neurons are placed in the grid based on similarity. The second step is the quality assessment. Each neuron will be associated with a discrete probability distribution. Homogeneous neurons (those with a single class) are assumed to be composed of good quality samples. Heterogeneous neurons (those with two or more classes with significant probability) are likely to contain noisy samples.</p>
<p>Considering that each sample of the training set is assigned to a neuron, the algorithm computes two values for each sample:</p>
<ul>
<li>prior probability: the probability that the label assigned to the sample is correct, considering only the samples in the same neuron. For example, if a neuron has 20 samples, of which 15 are labeled as “Pasture” and 5 as “Forest”, all samples labeled “Forest” are assigned a prior probability of 25%. This is an indication that the “Forest” samples in this neuron are not of good quality.</li>
<li>posterior probability: the probability that the label assigned to the sample is correct, considering the neighboring neurons. Take the case of the above-mentioned neuron whose samples labeled “Pasture” have a prior probability of 75%. What happens if all the neighboring samples have “Forest” as a majority label? Are the samples labeled “Pasture” in this neuron noisy? To answer this question, we use information from the neighbours. Bayesian inference we estimate if these samples are noisy based on the samples of the neighboring neurons [Santos2021].</li>
</ul>
<p>As an example of the use of SOM clustering for quality control of samples, we take a dataset containing a tibble with time series samples for the Cerrado region of Brazil, the second largest biome in South America with an area of more than 2 million km2. The training samples were collected by ground surveys and high-resolution image interpretation by experts from the Brazilian National Institute for Space Research (INPE) team and partners. This set ranges from 2000 to 2017 and includes 61,073 land use and cover samples divided into 14 classes: Natural Non-vegetated, Fallow-Cotton, Millet-Cotton, Soy-Corn, Soy-Cotton, Soy-Fallow, Pasture, Shrublands (in Portuguese “Cerrado Rupestre”), Savanna (“Cerrado”), Dense Tree Savanna (“Cerradão”), Open Savanna (“Campo Cerrado”), Planted Forest, and (14) Wetlands. In the example below, we take only 10% of the samples for faster processing. Users are encouraged to run the example with the full set of samples.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-1"></a><span class="co"># take only 10% of the samples</span></span>
<span id="cb46-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-2"></a>samples_cerrado_mod13q1_reduced &lt;-<span class="st"> </span><span class="kw">sits_sample</span>(samples_cerrado_mod13q1, <span class="dt">frac =</span> <span class="fl">0.1</span>)</span>
<span id="cb46-3"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-3"></a><span class="co"># clustering time series using SOM</span></span>
<span id="cb46-4"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-4"></a>som_cluster &lt;-</span>
<span id="cb46-5"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-5"></a><span class="st">    </span><span class="kw">sits_som_map</span>(</span>
<span id="cb46-6"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-6"></a>        samples_cerrado_mod13q1_reduced,</span>
<span id="cb46-7"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-7"></a>        <span class="dt">grid_xdim =</span> <span class="dv">15</span>,</span>
<span id="cb46-8"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-8"></a>        <span class="dt">grid_ydim =</span> <span class="dv">15</span>,</span>
<span id="cb46-9"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-9"></a>        <span class="dt">alpha =</span> <span class="fl">1.0</span>,</span>
<span id="cb46-10"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-10"></a>        <span class="dt">distance =</span> <span class="st">&quot;euclidean&quot;</span>,</span>
<span id="cb46-11"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-11"></a>        <span class="dt">rlen =</span> <span class="dv">100</span></span>
<span id="cb46-12"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb46-12"></a>    )</span></code></pre></div>
<p>The output of the <code>sits_som_map</code> is a list with 4 tibbles:</p>
<ul>
<li>the original set of time series with two additional columns for each time series: <code>id_sample</code> (the original id of each sample) and <code>id_neuron</code> (the id of the neuron to which it belongs).</li>
<li>a tibble with information on the neuron. For each neuron, it gives the prior and posterior probabilities of all labels which occur in the samples assigned to it.</li>
<li>the SOM grid
To plot the SOM grid, use <code>plot()</code>. The neurons are labelled using the majority voting.</li>
</ul>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb47-1"></a><span class="kw">plot</span>(som_cluster)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-40-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Looking at the SOM grid, one can see that most of the neurons of a class are located close to each other. There are outliers, e.g., some “Open Savanna” neurons are located amidst “Shrublands” neurons. This mixture is a consequence of the continuous nature of natural vegetation cover in the Brazilian Cerrado. The transition between areas of open savanna and shrublands is not always well defined; moreover, it is dependent on factors such as climate and latitude.</p>
<p>To identifies noisy samples, we take the result of the <code>sits_som_map</code> function as the first argument to the function <code>sits_som_clean_samples</code>. This function finds out which samples are noisy, those that are clean, and some that need to be further examined by the user. It uses the <code>prior_threshold</code> and <code>posterior_threshold</code> parameters according to the following rules:</p>
<ul>
<li>If the prior probability of a sample is less than <code>prior_threshold</code>, the sample is assumed to be noisy and tagged as “remove”;</li>
<li>If the prior probability is greater or equal to <code>prior_threshold</code> and the posterior probability is greater or equal to <code>posterior_threshold</code>, the sample is assumed not to be noisy and thus is tagged as “clean”;</li>
<li>If the prior probability is greater or equal to <code>prior_threshold</code> and the posterior probability is less than <code>posterior_threshold</code>, we have a situation the sample is part of the majority level of those assigned to its neuron, but its label is not consistent with most of its neighbors. This is an anomalous condition and is tagged as “analyze”. Users are encouraged to inspect such samples to find out whether they are in fact noisy or not.</li>
</ul>
<p>The default value for both <code>prior_threshold</code> and <code>posterior_threshold</code> is 60%. The <code>sits_som_clean_samples</code> has an additional parameter (<code>keep</code>) which indicates which samples should be kept in the set based on their prior and posterior probabilities of being noisy and the assigned label. The default value for <code>keep</code> is <code>c("clean", "analyze")</code>. As a result of the cleaning, about 900 samples have been considered to be noisy and thus removed.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-1"></a>new_samples &lt;-<span class="st"> </span><span class="kw">sits_som_clean_samples</span>(som_cluster, </span>
<span id="cb48-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-2"></a>                                      <span class="dt">prior_threshold =</span> <span class="fl">0.6</span>,</span>
<span id="cb48-3"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-3"></a>                                      <span class="dt">posterior_threshold =</span> <span class="fl">0.6</span>,</span>
<span id="cb48-4"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-4"></a>                                      <span class="dt">keep =</span> <span class="kw">c</span>(<span class="st">&quot;clean&quot;</span>, <span class="st">&quot;analyze&quot;</span>))</span>
<span id="cb48-5"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-5"></a><span class="co"># find out how many samples are evaluated as &quot;clean&quot; or &quot;analyze&quot;</span></span>
<span id="cb48-6"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-6"></a>new_samples <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb48-7"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-7"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">group_by</span>(eval) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb48-8"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb48-8"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(<span class="dt">count =</span> dplyr<span class="op">::</span><span class="kw">n</span>(), <span class="dt">.groups =</span> <span class="st">&quot;drop&quot;</span>)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt; # A tibble: 2 x 2
#&gt;   eval    count
#&gt;   &lt;chr&gt;   &lt;int&gt;
#&gt; 1 analyze   652
#&gt; 2 clean    4416</code></pre>
</div>
<div id="comparing-global-accuracy-of-original-and-clean-samples" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Comparing Global Accuracy of Original and Clean Samples</h3>
<p>To compare the accuracy of the original and clean samples, we run
a 5-fold validation on the original and on the cleaned sample. We use the function
<code>sits_kfold_validate</code>. As the results show, the SOM procedure is useful, since
the global accuracy improves from 91% to 95%.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb50-1"></a>assess_orig &lt;-<span class="st"> </span><span class="kw">sits_kfold_validate</span>(samples_cerrado_mod13q1_reduced, </span>
<span id="cb50-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb50-2"></a>                                   <span class="dt">ml_method =</span> <span class="kw">sits_svm</span>())</span></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb51-1"></a>assess_new &lt;-<span class="st"> </span><span class="kw">sits_kfold_validate</span>(new_samples, </span>
<span id="cb51-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb51-2"></a>                                   <span class="dt">ml_method =</span> <span class="kw">sits_svm</span>())</span></code></pre></div>
<p>An additional way of evaluating the quality of samples is to examine the internal
mixture inside neurons with the same label. We call a group of neurons sharing
the same label as a “cluster”. Given a SOM map, the function <code>sits_som_evaluate_cluster</code>
examines all clusters to find out the percentage of samples contained in it which do not share its label. This information is saved as a tibble and can also
be visualized.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb52-1"></a><span class="co"># evaluate the misture in the SOM clusters</span></span>
<span id="cb52-2"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb52-2"></a>cluster_mixture &lt;-<span class="st"> </span><span class="kw">sits_som_evaluate_cluster</span>(som_cluster)</span>
<span id="cb52-3"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb52-3"></a><span class="co"># plot the mixture information.</span></span>
<span id="cb52-4"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#cb52-4"></a><span class="kw">plot</span>(cluster_mixture)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-44-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">4.4</span> Conclusion</h2>
<p>Machine learning methods are now established as a useful technique for remote sensing image analysis. Despite the well-known fact that the quality of the training data is a key factor in the accuracy of the resulting maps, the literature on methods for detecting and removing class noise in SITS training sets is limited. To contribute to solving this challenge, this paper proposed a new technique. The proposed method uses the SOM neural network to group similar samples in a 2D map for dimensionality reduction. The method identifies both mislabeled samples and outliers that are flagged to further investigation. The results demonstrate the positive impact on the overall classification accuracy. Although the class noise removal adds an extra cost to the entire classification process, we believe that it is essential to improve the accuracy of classified maps using SITS analysis mainly for large areas.</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="working-with-time-series.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machine-learning-for-data-cubes-using-the-sits-package.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["sitsbook.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
