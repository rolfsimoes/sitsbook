<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />
  <meta name="generator" content="bookdown 0.21.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Rolf Simoes" />
<meta name="author" content="Gilberto Camara" />
<meta name="author" content="Felipe Souza" />
<meta name="author" content="Lorena Santos" />
<meta name="author" content="Pedro R. Andrade" />
<meta name="author" content="Alexandre Carvalho" />
<meta name="author" content="Karine Ferreira" />
<meta name="author" content="Gilberto Queiroz" />


<meta name="date" content="2021-03-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="acessing-time-series-information-in-sits.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SITS Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#workflow"><i class="fa fa-check"></i><b>1.1</b> Workflow</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#handling-data-cubes-in-sits"><i class="fa fa-check"></i><b>1.2</b> Handling Data Cubes in <strong>sits</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><i class="fa fa-check"></i><b>1.2.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#using-stac-to-access-image-data-cubes"><i class="fa fa-check"></i><b>1.2.2</b> Using STAC to Access Image Data Cubes</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#defining-a-data-cube-using-files"><i class="fa fa-check"></i><b>1.2.3</b> Defining a data cube using files</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#handling-satellite-image-time-series-in-sits"><i class="fa fa-check"></i><b>1.3</b> Handling satellite image time series in <strong>sits</strong></a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#data-structure"><i class="fa fa-check"></i><b>1.3.1</b> Data structure</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#obtaining-time-series-data"><i class="fa fa-check"></i><b>1.3.2</b> Obtaining time series data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#filtering-techniques"><i class="fa fa-check"></i><b>1.4</b> Filtering techniques</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#clustering-for-sample-quality-control-using-self-organizing-maps"><i class="fa fa-check"></i><b>1.5</b> Clustering for sample quality control using self-organizing maps</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#classification-using-machine-learning"><i class="fa fa-check"></i><b>1.6</b> Classification using machine learning</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#validation-techniques"><i class="fa fa-check"></i><b>1.7</b> Validation techniques</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#cube-classification"><i class="fa fa-check"></i><b>1.8</b> Cube classification</a><ul>
<li class="chapter" data-level="1.8.1" data-path="introduction.html"><a href="introduction.html#steps-for-cube-classification"><i class="fa fa-check"></i><b>1.8.1</b> Steps for cube classification</a></li>
<li class="chapter" data-level="1.8.2" data-path="introduction.html"><a href="introduction.html#adjustments-for-improved-performance"><i class="fa fa-check"></i><b>1.8.2</b> Adjustments for improved performance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#smoothing-and-labelling-of-raster-data-after-classification"><i class="fa fa-check"></i><b>1.9</b> Smoothing and Labelling of raster data after classification</a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#final-remarks"><i class="fa fa-check"></i><b>1.10</b> Final remarks</a></li>
<li class="chapter" data-level="1.11" data-path="introduction.html"><a href="introduction.html#acknowledgements"><i class="fa fa-check"></i><b>1.11</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html"><i class="fa fa-check"></i><b>2</b> Acessing time series information in SITS</a><ul>
<li class="chapter" data-level="2.1" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#data-structures-for-satellite-time-series"><i class="fa fa-check"></i><b>2.1</b> Data structures for satellite time series</a></li>
<li class="chapter" data-level="2.2" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#utilities-for-handling-time-series"><i class="fa fa-check"></i><b>2.2</b> Utilities for handling time series</a></li>
<li class="chapter" data-level="2.3" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#time-series-visualisation"><i class="fa fa-check"></i><b>2.3</b> Time series visualisation</a></li>
<li class="chapter" data-level="2.4" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#obtaining-time-series-data-from-data-cubes"><i class="fa fa-check"></i><b>2.4</b> Obtaining time series data from data cubes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html"><i class="fa fa-check"></i><b>3</b> Satellite Image Time Series Filtering with SITS</a><ul>
<li class="chapter" data-level="3.1" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#filtering-techniques-in-sits"><i class="fa fa-check"></i><b>3.1</b> Filtering techniques in SITS</a></li>
<li class="chapter" data-level="3.2" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#common-interface-to-sits-filter-functions"><i class="fa fa-check"></i><b>3.2</b> Common interface to SITS filter functions</a><ul>
<li class="chapter" data-level="3.2.1" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#savitzkygolay-filter"><i class="fa fa-check"></i><b>3.2.1</b> Savitzky–Golay filter</a></li>
<li class="chapter" data-level="3.2.2" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#whittaker-filter"><i class="fa fa-check"></i><b>3.2.2</b> Whittaker filter</a></li>
<li class="chapter" data-level="3.2.3" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#envelope-filter"><i class="fa fa-check"></i><b>3.2.3</b> Envelope filter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html"><i class="fa fa-check"></i><b>4</b> Time Series Clustering to Improve the Quality of Training Samples</a><ul>
<li class="chapter" data-level="4.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.1</b> Clustering for sample quality control</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#hierachical-clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.2</b> Hierachical clustering for Sample Quality Control</a><ul>
<li class="chapter" data-level="4.2.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#creating-a-dendogram"><i class="fa fa-check"></i><b>4.2.1</b> Creating a dendogram</a></li>
<li class="chapter" data-level="4.2.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-a-dendrogram-to-evaluate-sample-quality"><i class="fa fa-check"></i><b>4.2.2</b> Using a dendrogram to evaluate sample quality</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-self-organizing-maps-for-sample-quality"><i class="fa fa-check"></i><b>4.3</b> Using Self-organizing Maps for Sample Quality</a><ul>
<li class="chapter" data-level="4.3.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#introduction-to-self-organizing-maps"><i class="fa fa-check"></i><b>4.3.1</b> Introduction to Self-organizing Maps</a></li>
<li class="chapter" data-level="4.3.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-som-for-removing-class-noise"><i class="fa fa-check"></i><b>4.3.2</b> Using SOM for removing class noise</a></li>
<li class="chapter" data-level="4.3.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#comparing-global-accuracy-of-original-and-clean-samples"><i class="fa fa-check"></i><b>4.3.3</b> Comparing Global Accuracy of Original and Clean Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html"><i class="fa fa-check"></i><b>5</b> Machine Learning for Data Cubes using the SITS package</a><ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-classification"><i class="fa fa-check"></i><b>5.1</b> Machine learning classification</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#data-used-in-the-machine-learning-examples"><i class="fa fa-check"></i><b>5.2</b> Data used in the machine learning examples</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#visualizing-samples"><i class="fa fa-check"></i><b>5.3</b> Visualizing Samples</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#common-interface-to-machine-learning-and-deeplearning-models"><i class="fa fa-check"></i><b>5.4</b> Common interface to machine learning and deeplearning models</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#random-forests"><i class="fa fa-check"></i><b>5.5</b> Random forests</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#support-vector-machines"><i class="fa fa-check"></i><b>5.6</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#extreme-gradient-boosting"><i class="fa fa-check"></i><b>5.7</b> Extreme Gradient Boosting</a></li>
<li class="chapter" data-level="5.8" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#deep-learning-using-multi-layer-perceptrons"><i class="fa fa-check"></i><b>5.8</b> Deep learning using multi-layer perceptrons</a></li>
<li class="chapter" data-level="5.9" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#d-convolutional-neural-networks"><i class="fa fa-check"></i><b>5.9</b> 1D Convolutional Neural Networks</a></li>
<li class="chapter" data-level="5.10" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#residual-1d-cnn-networks-resnet"><i class="fa fa-check"></i><b>5.10</b> Residual 1D CNN Networks (ResNet)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><i class="fa fa-check"></i><b>6</b> Classification of Images in Data Cubes using Satellite Image Time Series</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis-1"><i class="fa fa-check"></i><b>6.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#defining-a-data-cube-using-files-organised-as-raster-bricks"><i class="fa fa-check"></i><b>6.2</b> Defining a data cube using files organised as raster bricks</a></li>
<li class="chapter" data-level="6.3" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#classification-using-machine-learning-1"><i class="fa fa-check"></i><b>6.3</b> Classification using machine learning</a></li>
<li class="chapter" data-level="6.4" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#cube-classification-1"><i class="fa fa-check"></i><b>6.4</b> Cube classification</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#steps-for-cube-classification-1"><i class="fa fa-check"></i><b>6.4.1</b> Steps for cube classification</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#adjustments-for-improved-performance-1"><i class="fa fa-check"></i><b>6.4.2</b> Adjustments for improved performance</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#final-remarks-1"><i class="fa fa-check"></i><b>6.5</b> Final remarks</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><i class="fa fa-check"></i><b>7</b> Post classification smoothing using Bayesian techniques in SITS</a><ul>
<li class="chapter" data-level="7.1" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#overview-of-bayesian-estimattion"><i class="fa fa-check"></i><b>7.2</b> Overview of Bayesian estimattion</a><ul>
<li class="chapter" data-level="7.2.1" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#smmothing-using-bayes-rule"><i class="fa fa-check"></i><b>7.2.1</b> Smmothing using Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#use-of-bayesian-smoothing-in-sits"><i class="fa fa-check"></i><b>7.3</b> Use of Bayesian smoothing in SITS</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html"><i class="fa fa-check"></i><b>8</b> Validation and accuracy measurements in SITS</a><ul>
<li class="chapter" data-level="8.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#validation-techniques-1"><i class="fa fa-check"></i><b>8.1</b> Validation techniques</a></li>
<li class="chapter" data-level="8.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#comparing-different-validation-methods"><i class="fa fa-check"></i><b>8.2</b> Comparing different validation methods</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>sits</strong>: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<hr />
<p>Using time series derived from big Earth Observation data sets is one of the leading research trends in Land Use Science and Remote Sensing. One of the more promising uses of satellite time series is its application to classify land use and land cover since our growing demand for natural resources has caused significant environmental impacts. Here, we present an open-source <em>R</em> package for satellite image time series analysis called <strong>sits</strong>. The package support use of machine learning techniques for classification image time series obtained from data cubes. Methods available include linear and quadratic discrimination analysis, support vector machines, random forests, boosting, deep learning, and convolution neural.</p>
<hr />
<p>Earth observation satellites provide a common and consistent set of information about the planet’s land and oceans. Recently, most space agencies have adopted open data policies, making unprecedented amounts of satellite data available for research and operational use. This data deluge has brought about a significant challenge: <em>How to design and build technologies that allow the Earth observation community to analyze big data sets?</em></p>
<p>The approach taken in the current work is to develop data analysis methods that work with satellite image time series, obtained by taking calibrated and comparable measures of the same location in Earth at different times. These measures can be obtained by a single sensor (e.g., MODIS) or by combining various sensors (e.g., Landsat 8 and Sentinel-2). If acquired by frequent revisits, these data sets’ temporal resolution can capture significant land use changes.</p>
<p>Time series of remote sensing data show that land cover can occur not only progressively and gradually, but they may also show discontinuities with abrupt changes <span class="citation">(Lambin, Geist, and Lepers 2003)</span>. Analyses of multiyear time series of land surface attributes, their fine-scale spatial pattern, and their seasonal evolution lead to a broader view of land-cover change. Satellite image time series have already been used in applications such as mapping for detecting forest disturbance <span class="citation">(Kennedy, Yang, and Cohen 2010)</span>, ecology dynamics <span class="citation">(Pasquarella et al. 2016)</span>, agricultural intensification <span class="citation">(Galford et al. 2008)</span>, and its impacts on deforestation <span class="citation">(Arvor et al. 2012)</span>. Algorithms for processing image time series include BFAST for detecting breaks <span class="citation">(Verbesselt et al. 2010)</span>, TIMESAT for modeling and measuring phenological attributes <span class="citation">(Jönsson and Eklundh 2004)</span>, and methods based on Dynamic Time Warping (DTW) for land use and land cover classification <span class="citation">(Petitjean, Inglada, and Gancarski 2012)</span><span class="citation">(Maus et al. 2016)</span>.</p>
<p>In this work, we present <strong>sits</strong>, an open-source R package for satellite image time series analysis. It provides support on how to use machine learning techniques with image time series. These methods include linear and quadratic discrimination analysis, support vector machines, random forests, and neural networks. One important contribution of the package is to support the complete cycle of data analysis for time series classification, including data acquisition, visualization, filtering, clustering, classification, validation, and post-classification adjustments.</p>
<p>Most studies using satellite image time series for land cover classification use a  approach. For multiyear studies, researchers first derive best-fit yearly composites and then classify each composite image. To review these methods for land use and land cover classification using time series, see <span class="citation">(Gomez, White, and Wulder 2016)</span>. As an alternative to  methods, the <strong>sits</strong> package provides support for the classification of time series, preserving the full temporal resolution of the input data, using a  approach. <strong>sits</strong> uses all data in the image time series to create larger dimensional spaces for machine learning. The idea is to have as many temporal attributes as possible, increasing the classification space’s dimension. Each temporal instance of a time series is taken as an independent dimension in the classifier’s feature space. To the authors’ best knowledge, the classification techniques for image time series included in the package are not previously available in other R or python packages. Furthermore, the package provides filtering, clustering, and post-processing methods that have not been published in the literature.</p>
<div id="workflow" class="section level2">
<h2><span class="header-section-number">1.1</span> Workflow</h2>
<p>The main aim of <strong>sits</strong> is to support land cover and land change classification of image data cubes using machine learning methods. The basic workflow is:</p>
<ol style="list-style-type: decimal">
<li>Create a data cube using image collections available in the cloud or local machines.</li>
<li>Extract time series from the data cube, which is used as training data.</li>
<li>Perform quality control and filtering on the samples.</li>
<li>Train a machine learning model using the extracted samples.</li>
<li>Classify the data cube using the trained model.</li>
<li>Post-process the classified images.</li>
</ol>
</div>
<div id="handling-data-cubes-in-sits" class="section level2">
<h2><span class="header-section-number">1.2</span> Handling Data Cubes in <strong>sits</strong></h2>
<div id="image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Image data cubes as the basis for big Earth observation data analysis</h3>
<p>In broad terms, the cloud computing model is one where large satellite-generated data sets are archived on cloud services, providing computing facilities to process them. By using cloud services, users can share big Earth observation databases and minimize data download. Investment in infrastructure is minimized, and sharing of data and software increases. However, data available in the cloud is best organised for analysis by creating data cubes.</p>
<p>Generalizing <span class="citation">Appel and Pebesma (2019)</span>, we consider that a data cube is a four-dimensional structure with dimensions x (longitude or easting), y (latitude or northing), time, and bands. Its spatial dimensions refer to a single spatial reference system (SRS). Cells of a data cube have a constant spatial size (concerning the cube’s SRS). A set of intervals specifies the temporal dimension. For every combination of dimensions, a cell has a single value. Data cubes are particularly amenable for machine learning techniques; their data can be transformed into arrays in memory, fed to training and classification algorithms. Given the widespread availability of large data sets of Earth observation data, there is a growing interest in organising large data sets into “data cubes”.</p>
</div>
<div id="using-stac-to-access-image-data-cubes" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Using STAC to Access Image Data Cubes</h3>
<p>One of the distinguishing features of <strong>sits</strong> is that it has been designed to work with big satellite image data sets which reside on the cloud and with data cubes. Many <em>R</em> packages working with remote sensing images require data to be accessible in a local computer. However, with the coming of age of big Earth observation data, it is not always practical to transfer large data sets. Users have to rely on web services to provide access to these data sets. In this context, <strong>sits</strong> is based on access to data cubes using the information provided by STAC (Spatio-temporal Access Catalogue).</p>
<p>Currently, <strong>sits</strong> supports data cubes available in the following cloud services:</p>
<ol style="list-style-type: decimal">
<li>Sentinel-2/2A level 2A images in Amazon Web Services (AWS);</li>
<li>Collections of Sentinel, Landsat, and CBERS images in the Brazil Data Cube (BDC);</li>
<li>Collections available in Digital Earth Africa;</li>
<li>Data cubes produced by the <a href="https://github.com/appelmar/gdalcubes">gdalcubes package</a>;</li>
<li>Local image collections.</li>
</ol>
<p>In order, to access different STAC cloud services supported by <strong>sits</strong>, the <a href="http://github.com/brazil-data-cube/rstac">rstac</a> package is used. This package is developed under the Brazil Data Cube project and provides features for consuming STAC services. Besides, the <a href="https://github.com/cloudyr/aws.s3">aws.s3</a> package is used to access data in AWS.</p>
<p>The use of different providers in the <strong>sits</strong> package is done with as few change as possible for users. The user can define a data cube by selecting a cloud service collection and determining a space-time extent. The code below shows the definition of a data cube using AWS Sentinel-2/2A images to exemplify how it is used.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="introduction.html#cb1-1"></a>s2_cube &lt;-<span class="st"> </span><span class="kw">sits_cube</span>(</span>
<span id="cb1-2"><a href="introduction.html#cb1-2"></a>    <span class="dt">source        =</span> <span class="st">&quot;AWS&quot;</span>,</span>
<span id="cb1-3"><a href="introduction.html#cb1-3"></a>    <span class="dt">name          =</span> <span class="st">&quot;T20LKP_2018_2019&quot;</span>,</span>
<span id="cb1-4"><a href="introduction.html#cb1-4"></a>    <span class="dt">collection    =</span> <span class="st">&quot;sentinel-s2-l2a&quot;</span>,</span>
<span id="cb1-5"><a href="introduction.html#cb1-5"></a>    <span class="dt">bands         =</span> <span class="kw">c</span>(<span class="st">&quot;B08&quot;</span>, <span class="st">&quot;SCL&quot;</span>),</span>
<span id="cb1-6"><a href="introduction.html#cb1-6"></a>    <span class="dt">tiles         =</span> <span class="st">&quot;20LKP&quot;</span>,</span>
<span id="cb1-7"><a href="introduction.html#cb1-7"></a>    <span class="dt">start_date    =</span> <span class="kw">as.Date</span>(<span class="st">&quot;2018-07-18&quot;</span>),</span>
<span id="cb1-8"><a href="introduction.html#cb1-8"></a>    <span class="dt">end_date      =</span> <span class="kw">as.Date</span>(<span class="st">&quot;2018-08-18&quot;</span>),</span>
<span id="cb1-9"><a href="introduction.html#cb1-9"></a>    <span class="dt">s2_resolution =</span> <span class="dv">60</span></span>
<span id="cb1-10"><a href="introduction.html#cb1-10"></a>)</span></code></pre></div>
<p>In the above example, the user has selected the “Sentinel-2 Level 2” collection in the AWS cloud services. The data cube’s geographical area is defined by the tile “20LKP” and the temporal extent by a start and end date. Access to other cloud services works in similar ways.</p>
<p>Users can derive data cubes from ARD data that have pre-defined temporal resolutions. For example, a user may want to define the best Sentinel-2 pixel in a one-month period, as shown below. This can be done in <strong>sits</strong> by the <code>sits_regularize</code>, which calls the “gdalcubes” package.
For details in gdalcubes, please see <a href="https://github.com/appelmar/gdalcubes">https://github.com/appelmar/gdalcubes</a>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="introduction.html#cb2-1"></a>gc_cube &lt;-<span class="st"> </span><span class="kw">sits_regularize</span>(</span>
<span id="cb2-2"><a href="introduction.html#cb2-2"></a>          <span class="dt">cube       =</span> s2_cube,</span>
<span id="cb2-3"><a href="introduction.html#cb2-3"></a>          <span class="dt">name       =</span> <span class="st">&quot;T20LKP_2018_2019_1M&quot;</span>,</span>
<span id="cb2-4"><a href="introduction.html#cb2-4"></a>          <span class="dt">dir_images =</span> <span class="kw">tempdir</span>(),</span>
<span id="cb2-5"><a href="introduction.html#cb2-5"></a>          <span class="dt">period     =</span> <span class="st">&quot;P1M&quot;</span>,</span>
<span id="cb2-6"><a href="introduction.html#cb2-6"></a>          <span class="dt">agg_method =</span> <span class="st">&quot;median&quot;</span>,</span>
<span id="cb2-7"><a href="introduction.html#cb2-7"></a>          <span class="dt">resampling =</span> <span class="st">&quot;bilinear&quot;</span>,</span>
<span id="cb2-8"><a href="introduction.html#cb2-8"></a>          <span class="dt">cloud_mask =</span> <span class="ot">TRUE</span></span>
<span id="cb2-9"><a href="introduction.html#cb2-9"></a>)</span></code></pre></div>
</div>
<div id="defining-a-data-cube-using-files" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Defining a data cube using files</h3>
<p>To define a data cube using plain files (without STAC information), all image files should have the same spatial resolution and same projection. Each file contains a single image band for a single date. Since raster files in popular formats (e.g., GeoTiff and JPEG 2000) do not include time information, each file’s name needs to include the date and band information. Timeline and bands are deduced from filenames. For example, <code>CBERS-4_AWFI_B13_2018-02-02.tif</code> is a valid name. The user has to provide parsing information to allow <strong>sits</strong> to extract the band and the date. In the example above, the parsing info is <code>c("X1", "X2", "band", "date")</code> and the delimiter is <code>"_"</code>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="introduction.html#cb3-1"></a><span class="kw">library</span>(sits)</span>
<span id="cb3-2"><a href="introduction.html#cb3-2"></a><span class="co"># Create a cube based on a stack of CBERS data</span></span>
<span id="cb3-3"><a href="introduction.html#cb3-3"></a>data_dir &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;extdata/raster/cbers&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;sits&quot;</span>)</span>
<span id="cb3-4"><a href="introduction.html#cb3-4"></a></span>
<span id="cb3-5"><a href="introduction.html#cb3-5"></a><span class="co"># files are named using the convention </span></span>
<span id="cb3-6"><a href="introduction.html#cb3-6"></a><span class="co"># &quot;CBERS-4_AWFI_B13_2018-02-02.tif&quot;</span></span>
<span id="cb3-7"><a href="introduction.html#cb3-7"></a>cbers_cube &lt;-<span class="st"> </span><span class="kw">sits_cube</span>(</span>
<span id="cb3-8"><a href="introduction.html#cb3-8"></a>      <span class="dt">source =</span> <span class="st">&quot;LOCAL&quot;</span>,</span>
<span id="cb3-9"><a href="introduction.html#cb3-9"></a>      <span class="dt">name =</span> <span class="st">&quot;022024&quot;</span>,</span>
<span id="cb3-10"><a href="introduction.html#cb3-10"></a>      <span class="dt">satellite =</span> <span class="st">&quot;CBERS-4&quot;</span>,</span>
<span id="cb3-11"><a href="introduction.html#cb3-11"></a>      <span class="dt">sensor =</span> <span class="st">&quot;AWFI&quot;</span>,</span>
<span id="cb3-12"><a href="introduction.html#cb3-12"></a>      <span class="dt">data_dir =</span> data_dir,</span>
<span id="cb3-13"><a href="introduction.html#cb3-13"></a>      <span class="dt">delim =</span> <span class="st">&quot;_&quot;</span>,</span>
<span id="cb3-14"><a href="introduction.html#cb3-14"></a>      <span class="dt">parse_info =</span> <span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;band&quot;</span>, <span class="st">&quot;date&quot;</span>)</span>
<span id="cb3-15"><a href="introduction.html#cb3-15"></a>)</span></code></pre></div>
<pre><code>#&gt; Loading required namespace: terra</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="introduction.html#cb5-1"></a><span class="co"># print the timeline of the cube</span></span>
<span id="cb5-2"><a href="introduction.html#cb5-2"></a><span class="kw">sits_timeline</span>(cbers_cube)</span></code></pre></div>
<pre><code>#&gt;  [1] &quot;2018-02-02&quot; &quot;2018-02-18&quot; &quot;2018-03-06&quot; &quot;2018-03-22&quot; &quot;2018-04-07&quot;
#&gt;  [6] &quot;2018-04-23&quot; &quot;2018-05-09&quot; &quot;2018-05-25&quot; &quot;2018-06-10&quot; &quot;2018-06-26&quot;
#&gt; [11] &quot;2018-07-12&quot; &quot;2018-07-28&quot; &quot;2018-08-13&quot; &quot;2018-08-29&quot;</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="introduction.html#cb7-1"></a><span class="co"># print the bounding box of the cube</span></span>
<span id="cb7-2"><a href="introduction.html#cb7-2"></a><span class="kw">sits_bbox</span>(cbers_cube)</span></code></pre></div>
<pre><code>#&gt;    xmin    xmax    ymin    ymax 
#&gt; 5794837 5798037 9773148 9776348</code></pre>
</div>
</div>
<div id="handling-satellite-image-time-series-in-sits" class="section level2">
<h2><span class="header-section-number">1.3</span> Handling satellite image time series in <strong>sits</strong></h2>
<div id="data-structure" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Data structure</h3>
<p>Training a machine learning model in <strong>sits</strong> requires a set of time series describing properties in spatio-temporal locations of interest. This set consists of samples provided by experts that take <em>in-situ</em> field observations or recognize land classes using high-resolution images for land use classification. The package can also be used for any type of classification, provided that the timeline and bands of the time series (used for training) match that of the data cubes.</p>
<p>The package uses a <code>sits tibble</code> to organize time series data with associated spatial information for handling time series. A <code>tibble</code> is a generalization of a <code>data.frame</code>, the usual way in <em>R</em> to organize data in tables. Tibbles is part of the <code>tidyverse</code>, a collection of R packages designed to work together in data manipulation <span class="citation">(Wickham and Grolemund 2017)</span>. As an example of how the <strong>sits</strong> tibble works, the following code shows the first three lines of a tibble containing <span class="math inline">\(1,218\)</span> labeled samples of land cover in the Mato Grosso state of Brazil, with four classes: “Forest”, “Cerrado”, “Pasture”, “Soybean-Corn”.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="introduction.html#cb9-1"></a><span class="co"># data set of samples</span></span>
<span id="cb9-2"><a href="introduction.html#cb9-2"></a><span class="kw">data</span>(<span class="st">&quot;samples_modis_4bands&quot;</span>)</span>
<span id="cb9-3"><a href="introduction.html#cb9-3"></a>samples_modis_4bands[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,]</span></code></pre></div>
<pre><code>#&gt; # A tibble: 3 x 7
#&gt;   longitude latitude start_date end_date   label   cube    time_series      
#&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;list&gt;           
#&gt; 1     -55.2   -10.8  2013-09-14 2014-08-29 Pasture MOD13Q1 &lt;tibble [23 × 5]&gt;
#&gt; 2     -57.8    -9.76 2006-09-14 2007-08-29 Pasture MOD13Q1 &lt;tibble [23 × 5]&gt;
#&gt; 3     -51.9   -13.4  2014-09-14 2015-08-29 Pasture MOD13Q1 &lt;tibble [23 × 5]&gt;</code></pre>
<p>A <code>sits tibble</code> contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The spatial location is given in longitude and latitude coordinates for the “WGS84” ellipsoid. For example, the first sample has been labeled “Pasture” at location (<span class="math inline">\(-55.1852\)</span>, <span class="math inline">\(-10.8378\)</span>) and is valid for the period (2013-09-14, 2014-08-29).</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="introduction.html#cb11-1"></a><span class="co"># print the first time series records of the first sample</span></span>
<span id="cb11-2"><a href="introduction.html#cb11-2"></a><span class="kw">sits_time_series</span>(samples_modis_4bands[<span class="dv">1</span>,])[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,]</span></code></pre></div>
<pre><code>#&gt; # A tibble: 3 x 5
#&gt;   Index       NDVI   EVI   NIR   MIR
#&gt;   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1 2013-09-14 0.388 0.253 0.316 0.307
#&gt; 2 2013-09-30 0.491 0.277 0.275 0.170
#&gt; 3 2013-10-16 0.527 0.318 0.286 0.205</code></pre>
</div>
<div id="obtaining-time-series-data" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Obtaining time series data</h3>
<p>To get a time series in <strong>sits</strong>, first is must necessarily create a data cube. Users can request one or more time series points from a data cube using <code>sits_get_data()</code>. This function provides a general means of access to image time series. Given data cue, the user provides the latitude and longitude of the desired location, the bands, and the start date and end date of the time series. If the start and end dates are not provided, it retrieves all the available periods. The result is a tibble that can be visualized using <code>plot()</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="introduction.html#cb13-1"></a><span class="kw">library</span>(sits)</span>
<span id="cb13-2"><a href="introduction.html#cb13-2"></a>data_dir &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;extdata/raster/mod13q1&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;sits&quot;</span>)</span>
<span id="cb13-3"><a href="introduction.html#cb13-3"></a>modis_cube &lt;-<span class="st"> </span><span class="kw">sits_cube</span>(</span>
<span id="cb13-4"><a href="introduction.html#cb13-4"></a>    <span class="dt">source =</span> <span class="st">&quot;LOCAL&quot;</span>,</span>
<span id="cb13-5"><a href="introduction.html#cb13-5"></a>    <span class="dt">name =</span> <span class="st">&quot;sinop-2014&quot;</span>,</span>
<span id="cb13-6"><a href="introduction.html#cb13-6"></a>    <span class="dt">satellite =</span> <span class="st">&quot;TERRA&quot;</span>,</span>
<span id="cb13-7"><a href="introduction.html#cb13-7"></a>    <span class="dt">sensor =</span> <span class="st">&quot;MODIS&quot;</span>,</span>
<span id="cb13-8"><a href="introduction.html#cb13-8"></a>    <span class="dt">data_dir =</span> data_dir,</span>
<span id="cb13-9"><a href="introduction.html#cb13-9"></a>    <span class="dt">delim =</span> <span class="st">&quot;_&quot;</span>,</span>
<span id="cb13-10"><a href="introduction.html#cb13-10"></a>    <span class="dt">parse_info =</span> <span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;band&quot;</span>, <span class="st">&quot;date&quot;</span>)</span>
<span id="cb13-11"><a href="introduction.html#cb13-11"></a>)</span>
<span id="cb13-12"><a href="introduction.html#cb13-12"></a></span>
<span id="cb13-13"><a href="introduction.html#cb13-13"></a><span class="co"># obtain a set of locations defined by a CSV file</span></span>
<span id="cb13-14"><a href="introduction.html#cb13-14"></a>csv_raster_file &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;extdata/samples/samples_sinop_crop.csv&quot;</span>,</span>
<span id="cb13-15"><a href="introduction.html#cb13-15"></a>                               <span class="dt">package =</span> <span class="st">&quot;sits&quot;</span>)</span>
<span id="cb13-16"><a href="introduction.html#cb13-16"></a></span>
<span id="cb13-17"><a href="introduction.html#cb13-17"></a><span class="co"># retrieve the points from the data cube</span></span>
<span id="cb13-18"><a href="introduction.html#cb13-18"></a>points &lt;-<span class="st"> </span><span class="kw">sits_get_data</span>(modis_cube, <span class="dt">file =</span> csv_raster_file)</span></code></pre></div>
<pre><code>#&gt; Loading required namespace: terra
#&gt; Loading required namespace: terra</code></pre>
<pre><code>#&gt; All points have been retrieved</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="introduction.html#cb16-1"></a><span class="co"># plot the first point</span></span>
<span id="cb16-2"><a href="introduction.html#cb16-2"></a><span class="kw">plot</span>(points[<span class="dv">1</span>,])</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-8"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-8-1.png" alt="A one year time series of MOD13Q1 data for bands NDVI and EVI" width="70%" />
<p class="caption">
Figure 1.1: A one year time series of MOD13Q1 data for bands NDVI and EVI
</p>
</div>
</div>
</div>
<div id="filtering-techniques" class="section level2">
<h2><span class="header-section-number">1.4</span> Filtering techniques</h2>
<p>The literature on satellite image time series has several filtering applications to correct or smooth vegetation index data. The following filters are available in <strong>sits</strong>. They are described in more detail in the vignette “Satellite Image Time Series Filtering with SITS”:</p>
<ul>
<li>Savitzky–Golay filter (<code>sits_sgolay</code>)</li>
<li>Whittaker filter (<code>sits_whittaker</code>)</li>
<li>Envelope filter (<code>sits_envelope</code>)</li>
</ul>
<p>The <strong>sits</strong> package uses a common interface to all filter functions with the <code>sits_filter</code>. The function has two parameters: the <code>dataset</code> to be filtered and the <code>filter</code> function applied. To aid in data visualization, all filtered bands have a suffix that is appended, as shown in the examples below. Here we show an example using the Whittaker smoother, proposed in the literature <span class="citation">(Atzberger and Eilers 2011)</span> as arguably the most appropriate one to use for satellite image time series. The Whittaker smoother attempts to fit a curve representing the raw data but is penalized if subsequent points vary too much <span class="citation">(Atzberger and Eilers 2011)</span>. It balances between the residual to the original data and the “smoothness” of the fitted curve. It uses the parameter <code>lambda</code> to control the degree of smoothing.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="introduction.html#cb17-1"></a><span class="co"># Take a NDVI time series, apply Whittaker filter and plot the series</span></span>
<span id="cb17-2"><a href="introduction.html#cb17-2"></a>point_ndvi &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, <span class="dt">bands =</span> <span class="st">&quot;NDVI&quot;</span>)</span>
<span id="cb17-3"><a href="introduction.html#cb17-3"></a>point_whit &lt;-<span class="st"> </span><span class="kw">sits_whittaker</span>(point_ndvi, <span class="dt">lambda =</span> <span class="fl">5.0</span>)</span>
<span id="cb17-4"><a href="introduction.html#cb17-4"></a></span>
<span id="cb17-5"><a href="introduction.html#cb17-5"></a><span class="co"># merge with original data and plot the original and the filtered data</span></span>
<span id="cb17-6"><a href="introduction.html#cb17-6"></a>point_whit <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb17-7"><a href="introduction.html#cb17-7"></a><span class="st">  </span><span class="kw">sits_merge</span>(point_ndvi) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb17-8"><a href="introduction.html#cb17-8"></a><span class="st">  </span><span class="kw">plot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-9"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-9-1.png" alt="Whittaker smoother filter applied on one-year NDVI time series. The example uses default $\lambda=3$ parameter." width="70%" />
<p class="caption">
Figure 1.2: Whittaker smoother filter applied on one-year NDVI time series. The example uses default <span class="math inline">\(\lambda=3\)</span> parameter.
</p>
</div>
</div>
<div id="clustering-for-sample-quality-control-using-self-organizing-maps" class="section level2">
<h2><span class="header-section-number">1.5</span> Clustering for sample quality control using self-organizing maps</h2>
<p>One of the key challenges of machine learning classification models is assessing the training data sets’ quality. It helps apply pre-processing methods to improve the samples’ quality and remove those that might have been wrongly labeled or have low discriminatory power. Good samples lead to good classification maps. <strong>sits</strong> provides support for two clustering methods to test sample quality: (a) Agglomerative Hierarchical Clustering (AHC); (b) Self-organizing Maps (SOM). Full details of the cluster methods used in <strong>sits</strong> are available in the vignette <a href="https://github.com/e-sensing/sits-docs/blob/master/doc/clustering.pdf">‘Clustering of Satellite Image Time Series with SITS’</a>.</p>
</div>
<div id="classification-using-machine-learning" class="section level2">
<h2><span class="header-section-number">1.6</span> Classification using machine learning</h2>
<p>There has been much recent interest in using classifiers such as support vector machines <span class="citation">(Mountrakis, Im, and Ogole 2011)</span> and random forests <span class="citation">(Belgiu and Dragut 2016)</span> for remote sensing images. Most often, researchers use a  approach. The dimension of the decision space is limited to the number of spectral bands or their transformations. Sometimes, the decision space is extended with temporal attributes. To do this, researchers filter the raw data to get smoother time series <span class="citation">(Brown et al. 2013; Kastens et al. 2017)</span>. Using software such as TIMESAT <span class="citation">(Jönsson and Eklundh 2004)</span>, they derive a small set of phenological parameters from vegetation indexes, like the beginning, peak, and length of the growing season <span class="citation">(Estel et al. 2015; Pelletier et al. 2016)</span>.</p>
<p>In a recent review of machine learning methods to classify remote sensing data <span class="citation">(Maxwell, Warner, and Fang 2018)</span>, the authors note that many factors influence these classifiers’ performance, including the size and quality of the training dataset dimension of the feature space and the choice of the parameters. We support both  and  approaches. Therefore, the <strong>sits</strong> package provides functionality to explore the full depth of satellite image time series data.</p>
<p>When used in  approach, <strong>sits</strong> treats time series as a feature vector. To be consistent, the procedure aligns all-time series from different years by its time proximity considering a given cropping schedule. Once aligned, the feature vector is formed by all pixel “bands”. The idea is to have as many temporal attributes as possible, increasing the classification space’s dimension. In this scenario, statistical learning models are the natural candidates to deal with high-dimensional data: learning to distinguish all land cover and land use classes from trusted samples exemplars (the training data) to infer classes of a larger data set.</p>
<p>The <strong>sits</strong> package provides a common interface to all machine learning models, using the <code>sits_train</code> function. This function takes two parameters: the input <code>data samples</code> and the ML method (<code>ml_method</code>), as shown below. After the model is estimated, it can classify individual time series or full data cubes using the <code>sits_classify</code> function. In the examples that follow, we show how to apply each method to classify a single time series. Then, we discuss how to organize full data cubes.</p>
<p>When a dataset of time series organised as a tibble is taken as input to the classifier, the result is the same tibble with one additional column (“predicted”), which contains the information on what labels are have been assigned for each interval. The following example illustrates how to train a dataset and classify an individual time series. First, we use the <code>sits_train</code> function with two parameters: the training dataset (described above) and the chosen machine learning model (in this case, a random forest classifier). The trained model is then used to classify a time series from Mato Grosso Brazilian state, using <code>sits_classify</code>. The results can be shown in text format using the function <code>sits_show_prediction</code> or graphically using the <code>plot</code> function.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="introduction.html#cb18-1"></a><span class="co">#select the data for classification</span></span>
<span id="cb18-2"><a href="introduction.html#cb18-2"></a></span>
<span id="cb18-3"><a href="introduction.html#cb18-3"></a><span class="co"># Train a machine learning model using Random Forest</span></span>
<span id="cb18-4"><a href="introduction.html#cb18-4"></a>rfor_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(<span class="dt">data =</span> samples_modis_4bands, </span>
<span id="cb18-5"><a href="introduction.html#cb18-5"></a>                    <span class="dt">ml_method =</span> <span class="kw">sits_rfor</span>(<span class="dt">num_trees =</span> <span class="dv">1000</span>))</span>
<span id="cb18-6"><a href="introduction.html#cb18-6"></a></span>
<span id="cb18-7"><a href="introduction.html#cb18-7"></a><span class="co"># get a point to be classified</span></span>
<span id="cb18-8"><a href="introduction.html#cb18-8"></a>point_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb18-9"><a href="introduction.html#cb18-9"></a>                            <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb18-10"><a href="introduction.html#cb18-10"></a></span>
<span id="cb18-11"><a href="introduction.html#cb18-11"></a><span class="co"># Classify using random forest model and plot the result</span></span>
<span id="cb18-12"><a href="introduction.html#cb18-12"></a>class.tb &lt;-<span class="st"> </span><span class="kw">sits_classify</span>(point_4bands, rfor_model)</span>
<span id="cb18-13"><a href="introduction.html#cb18-13"></a><span class="co"># show the results of the prediction</span></span>
<span id="cb18-14"><a href="introduction.html#cb18-14"></a><span class="kw">sits_show_prediction</span>(class.tb)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 17 x 3
#&gt;    from       to         class   
#&gt;    &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   
#&gt;  1 2000-09-13 2001-08-29 Forest  
#&gt;  2 2001-09-14 2002-08-29 Forest  
#&gt;  3 2002-09-14 2003-08-29 Forest  
#&gt;  4 2003-09-14 2004-08-28 Pasture 
#&gt;  5 2004-09-13 2005-08-29 Pasture 
#&gt;  6 2005-09-14 2006-08-29 Pasture 
#&gt;  7 2006-09-14 2007-08-29 Pasture 
#&gt;  8 2007-09-14 2008-08-28 Pasture 
#&gt;  9 2008-09-13 2009-08-29 Pasture 
#&gt; 10 2009-09-14 2010-08-29 Soy_Corn
#&gt; 11 2010-09-14 2011-08-29 Soy_Corn
#&gt; 12 2011-09-14 2012-08-28 Soy_Corn
#&gt; 13 2012-09-13 2013-08-29 Soy_Corn
#&gt; 14 2013-09-14 2014-08-29 Soy_Corn
#&gt; 15 2014-09-14 2015-08-29 Soy_Corn
#&gt; 16 2015-09-14 2016-08-28 Soy_Corn
#&gt; 17 2016-09-13 2017-08-29 Soy_Corn</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="introduction.html#cb20-1"></a><span class="co"># plot the results of the prediction</span></span>
<span id="cb20-2"><a href="introduction.html#cb20-2"></a><span class="kw">plot</span>(class.tb)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-10"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-10-1.png" alt="Random forest classification of a $16$ years time series. The location (latitude, longitude) shown at the top of the graph is in geographic coordinate system (WGS84 {\it datum})." width="70%" />
<p class="caption">
Figure 1.3: Random forest classification of a <span class="math inline">\(16\)</span> years time series. The location (latitude, longitude) shown at the top of the graph is in geographic coordinate system (WGS84 {}).
</p>
</div>
<p>The following methods are available in <strong>sits</strong> for training machine learning models:</p>
<ul>
<li>Linear discriminant analysis (<code>sits_lda</code>)</li>
<li>Quadratic discriminant analysis (<code>sits_qda</code>)</li>
<li>Multinomial logit and its variants ‘lasso’ and ‘ridge’ (<code>sits_mlr</code>)</li>
<li>Support vector machines (<code>sits_svm</code>)</li>
<li>Random forests (<code>sits_rfor</code>)</li>
<li>Extreme gradient boosting (<code>sits_xgboost</code>)</li>
<li>Deep learning (DL) using multi-layer perceptrons (<code>sits_deeplearning</code>)</li>
<li>DL with 1D convolutional neural networks (<code>sits_CNN</code>),</li>
<li>DL combining 1D CNN and multi-layer perceptron networks (<code>sits_tempCNN</code>)</li>
<li>DL using 1D version of ResNet (<code>sits_ResNet</code>).</li>
<li>DL using a combination of long-short term memory (LSTM) and 1D CNN (<code>sits_LSTM_FCN</code>)</li>
</ul>
<p>For more details on each method, please see the vignette <a href="">“Machine Learning for Data Cubes using the sits package”</a></p>
</div>
<div id="validation-techniques" class="section level2">
<h2><span class="header-section-number">1.7</span> Validation techniques</h2>
<p>Validation is a process undertaken on models to estimate some errors associated with them. Hence, it has been used widely in different scientific disciplines. Here, we are interested in assessing the prediction error associated with some models. For this purpose, we concentrate on the <em>cross-validation</em> approach, probably the most used validation technique <span class="citation">(Hastie, Tibshirani, and J. 2009)</span>.</p>
<p>To be sure, cross-validation estimates the expected prediction error. It uses part of the available samples to fit the classification model and a different part to test it. In the so-called <em>k-fold</em> validation, we split the data into <span class="math inline">\(k\)</span> partitions with approximately the same size and proceed by fitting the model and testing it <span class="math inline">\(k\)</span> times. At each step, we take one distinct partition for the test and the remaining <span class="math inline">\({k-1}\)</span> for training the model and calculate its prediction error for classifying the test partition. A simple average gives us an estimation of the expected prediction error.</p>
<p>A natural question that arises is: <em>how good is this estimation?</em> According to <span class="citation">Hastie, Tibshirani, and J. (2009)</span>, there is a bias-variance trade-off in the choice of <span class="math inline">\(k\)</span>. If <span class="math inline">\(k\)</span> is set to the number of samples, we obtain the so-called <em>leave-one-out</em> validation. The estimator gives a low bias for the true expected error but produces a high variance expectation. This can be computationally expensive as it requires the same number of a fitting process as the number of samples. On the other hand, if we choose <span class="math inline">\({k=2}\)</span>, we get a high biased expected prediction error estimation that overestimates the true prediction error but has a low variance. The recommended choices of <span class="math inline">\(k\)</span> are <span class="math inline">\(5\)</span> or <span class="math inline">\(10\)</span> <span class="citation">(Hastie, Tibshirani, and J. 2009)</span>, which somewhat overestimates the true prediction error.</p>
<p><code>sits_kfold_validate()</code> gives support the k-fold validation in <strong>sits</strong>. The following code gives an example of how to proceed <em>k-fold cross-validation</em> in the package. It performs a five-fold validation using the SVM classification model as a default classifier. We can see in the output text the corresponding confusion matrix and the accuracy statistics (overall and by class).</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="introduction.html#cb21-1"></a><span class="co"># perform a five fold validation for the &quot;cerrado_2classes&quot; data set</span></span>
<span id="cb21-2"><a href="introduction.html#cb21-2"></a><span class="co"># Random Forest machine learning method using default parameters</span></span>
<span id="cb21-3"><a href="introduction.html#cb21-3"></a>acc &lt;-<span class="st"> </span><span class="kw">sits_kfold_validate</span>(cerrado_2classes, </span>
<span id="cb21-4"><a href="introduction.html#cb21-4"></a>                           <span class="dt">folds =</span> <span class="dv">5</span>, </span>
<span id="cb21-5"><a href="introduction.html#cb21-5"></a>                           <span class="dt">ml_method =</span> <span class="kw">sits_rfor</span>(<span class="dt">num_trees =</span> <span class="dv">1000</span>))</span></code></pre></div>
</div>
<div id="cube-classification" class="section level2">
<h2><span class="header-section-number">1.8</span> Cube classification</h2>
<p>The continuous observation of the Earth’s surface provided by orbital sensors is unprecedented in history. Just for the sake of illustration, a unique tile from MOD13Q1 product, a square of <span class="math inline">\(4800\)</span> pixels provided every 16 days since February 2000, takes around <span class="math inline">\(18\)</span>GB of uncompressed data to store only one band or vegetation index. This data deluge puts the field into a big data era. It imposes challenges to design and build technologies that allow the Earth observation community to analyze those data sets <span class="citation">(Câmara et al. 2017)</span>.</p>
<p>To classify a data cube, use the function <code>sits_classify()</code> as described below. This function works with data cubes built with the classification algorithms. It allows users to choose how many processes will run the task in parallel and the size of each data chunk consumed at each iteration. This strategy enables <strong>sits</strong> to work on average desktop computers without depleting all computational resources. The code below illustrates how to classify a small raster brick image that accompanies the package.</p>
<div id="steps-for-cube-classification" class="section level3">
<h3><span class="header-section-number">1.8.1</span> Steps for cube classification</h3>
<p>Once a data cube that has associated files is defined, the steps for classification are:</p>
<ol style="list-style-type: decimal">
<li>Select a set of training samples;</li>
<li>Train a machine learning model;</li>
<li>Classify the data cubes using the model, producing a data cube with class probabilities;</li>
<li>Label the cube with probabilities, including data smoothing if desired.</li>
</ol>
</div>
<div id="adjustments-for-improved-performance" class="section level3">
<h3><span class="header-section-number">1.8.2</span> Adjustments for improved performance</h3>
<p>To reduce processing time, it is necessary to adjust <code>sits_classify()</code> according to the server’s capabilities. The package tries to keep memory use to a minimum, performing garbage collection to free memory as often as possible. Nevertheless, there is an inevitable trade-off between computing time, memory use, and I/O operations. The best trade-off must be determined by the user, considering disk read speed, number of cores in the server, and CPU performance.</p>
<p>The first parameter is <code>memsize</code>. It controls the size of the main memory (in GBytes) to be used for classification. The user must specify how much free memory will be available. The second factor controlling the performance of raster classification is <code>multicores</code>. Once a block of data is read from the disk into the main memory, it is split into different cores, as specified by the user. In general, the more cores are assigned to classification, the faster the result will be. However, there are overheads in switching time, especially when the server has other processes running.</p>
<p>Based on recent experience, the classification of a Sentinel-2 tile at 20-meter resolution (5490*5490 pixels), with six bands and 66-time instances covering one year of data, using SVM with a training data set of about 10,000 samples, takes about 3 hours using 20 cores and a memory size of 60 GB, in a server with 2.4GHz Xeon CPU and 96 GB of memory to produce the yearly classification maps.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="introduction.html#cb22-1"></a><span class="co"># Retrieve the set of samples for the Mato Grosso region </span></span>
<span id="cb22-2"><a href="introduction.html#cb22-2"></a><span class="co"># Select the data for classification</span></span>
<span id="cb22-3"><a href="introduction.html#cb22-3"></a>samples_2bands  &lt;-<span class="st"> </span><span class="kw">sits_select</span>(samples_modis_4bands, </span>
<span id="cb22-4"><a href="introduction.html#cb22-4"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span>
<span id="cb22-5"><a href="introduction.html#cb22-5"></a></span>
<span id="cb22-6"><a href="introduction.html#cb22-6"></a><span class="co"># build a machine learning model for this area</span></span>
<span id="cb22-7"><a href="introduction.html#cb22-7"></a>svm_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_2bands, <span class="kw">sits_svm</span>())</span>
<span id="cb22-8"><a href="introduction.html#cb22-8"></a></span>
<span id="cb22-9"><a href="introduction.html#cb22-9"></a><span class="co"># create a data cube to be classified</span></span>
<span id="cb22-10"><a href="introduction.html#cb22-10"></a><span class="co"># Cube is composed of MOD13Q1 images from the Sinop region in Mato Grosso (Brazil)</span></span>
<span id="cb22-11"><a href="introduction.html#cb22-11"></a>data_dir &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;extdata/raster/mod13q1&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;sits&quot;</span>)</span>
<span id="cb22-12"><a href="introduction.html#cb22-12"></a>sinop &lt;-<span class="st"> </span><span class="kw">sits_cube</span>(</span>
<span id="cb22-13"><a href="introduction.html#cb22-13"></a>    <span class="dt">source =</span> <span class="st">&quot;LOCAL&quot;</span>,</span>
<span id="cb22-14"><a href="introduction.html#cb22-14"></a>    <span class="dt">name =</span> <span class="st">&quot;sinop-2014&quot;</span>,</span>
<span id="cb22-15"><a href="introduction.html#cb22-15"></a>    <span class="dt">satellite =</span> <span class="st">&quot;TERRA&quot;</span>,</span>
<span id="cb22-16"><a href="introduction.html#cb22-16"></a>    <span class="dt">sensor =</span> <span class="st">&quot;MODIS&quot;</span>,</span>
<span id="cb22-17"><a href="introduction.html#cb22-17"></a>    <span class="dt">data_dir =</span> data_dir,</span>
<span id="cb22-18"><a href="introduction.html#cb22-18"></a>    <span class="dt">delim =</span> <span class="st">&quot;_&quot;</span>,</span>
<span id="cb22-19"><a href="introduction.html#cb22-19"></a>    <span class="dt">parse_info =</span> <span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;band&quot;</span>, <span class="st">&quot;date&quot;</span>)</span>
<span id="cb22-20"><a href="introduction.html#cb22-20"></a>)</span>
<span id="cb22-21"><a href="introduction.html#cb22-21"></a><span class="co"># Classify the raster cube, generating a probability file</span></span>
<span id="cb22-22"><a href="introduction.html#cb22-22"></a>probs_cube &lt;-<span class="st"> </span><span class="kw">sits_classify</span>(sinop, </span>
<span id="cb22-23"><a href="introduction.html#cb22-23"></a>                            <span class="dt">ml_model =</span> svm_model, </span>
<span id="cb22-24"><a href="introduction.html#cb22-24"></a>                            <span class="dt">output_dir =</span> <span class="kw">tempdir</span>(),</span>
<span id="cb22-25"><a href="introduction.html#cb22-25"></a>                            <span class="dt">memsize =</span> <span class="dv">16</span>,</span>
<span id="cb22-26"><a href="introduction.html#cb22-26"></a>                            <span class="dt">multicores =</span> <span class="dv">4</span>,</span>
<span id="cb22-27"><a href="introduction.html#cb22-27"></a>                            <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>#&gt; Loading required namespace: terra</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="introduction.html#cb24-1"></a><span class="co"># plot the probabilities cubes</span></span>
<span id="cb24-2"><a href="introduction.html#cb24-2"></a><span class="kw">plot</span>(probs_cube)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-12-1.png" alt="Class probabilities for each pixel" width="70%" />
<p class="caption">
Figure 1.4: Class probabilities for each pixel
</p>
</div>
</div>
</div>
<div id="smoothing-and-labelling-of-raster-data-after-classification" class="section level2">
<h2><span class="header-section-number">1.9</span> Smoothing and Labelling of raster data after classification</h2>
<p>Post-processing is a desirable step in any classification process. Most statistical classifiers use training samples derived from “pure” pixels. Users have selected as representative of the desired output classes. However, images contain many mixed pixels irrespective of the resolution. Also, there is a considerable degree of data variability in each class. These effects lead to outliers whose chance of misclassification is significant. To offset these problems, most post-processing methods use the “smoothness assumption” <span class="citation">(Schindler 2012)</span>: nearby pixels tend to have the same label. To put this assumption in practice, smoothing methods use the neighborhood information to remove outliers and enhance consistency in the resulting product.</p>
<p>Smoothing methods are an essential complement to machine learning algorithms for image classification. Since these methods are primarily pixel-based, they complement them with post-processing smoothing to include spatial information in the result. For each pixel, machine learning and other statistical algorithms provide the pixel’s probabilities belonging to each of the classes. As a first step in obtaining a result, each pixel is assigned to the class whose probability is higher. After this step, smoothing methods use class probabilities to detect and correct outliers or misclassified pixels. <strong>sits</strong> uses a Bayesian smoothing method, which provides the means to incorporate prior knowledge in data analysis. Please see the vignette “Post classification smoothing using Bayesian techniques in SITS” for more details on the smoothing procedure.</p>
<p>Doing post-processing using Bayesian smoothing in <strong>sits</strong> is straightforward. The result of the <code>sits_classify</code> function applied to a data cube is a set of more probability images, one per requested classification interval. The next step is to use the <code>sits_smooth</code> function. By default, this function selects the most likely class for each pixel, considering a Bayesian estimator that considers the neighbors. The following example takes the previously produced classification output and applies a Bayesian smoothing.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="introduction.html#cb25-1"></a><span class="co"># smooth the result with a bayesian filter</span></span>
<span id="cb25-2"><a href="introduction.html#cb25-2"></a>sinop_bayes &lt;-<span class="st"> </span><span class="kw">sits_smooth</span>(probs_cube, <span class="dt">output_dir =</span> <span class="kw">tempdir</span>())</span>
<span id="cb25-3"><a href="introduction.html#cb25-3"></a><span class="co"># label the resulting image</span></span>
<span id="cb25-4"><a href="introduction.html#cb25-4"></a>label_bayes &lt;-<span class="st"> </span><span class="kw">sits_label_classification</span>(sinop_bayes, <span class="dt">output_dir =</span> <span class="kw">tempdir</span>())</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-14-1.png" alt="Classified image post-processed with Bayesian smoothing" width="90%" height="90%" />
<p class="caption">
Figure 1.5: Classified image post-processed with Bayesian smoothing
</p>
</div>
</div>
<div id="final-remarks" class="section level2">
<h2><span class="header-section-number">1.10</span> Final remarks</h2>
<p>Current approaches to image time series analysis still use a limited number of attributes. A common approach is deriving a small set of phenological parameters from vegetation indices, like the beginning, peak, and length of growing season <span class="citation">(Brown et al. 2013)</span>, <span class="citation">(Kastens et al. 2017)</span>, <span class="citation">(Estel et al. 2015)</span>, <span class="citation">(Pelletier et al. 2016)</span>. These phenological parameters are then fed in specialized classifiers such as TIMESAT <span class="citation">(Jönsson and Eklundh 2004)</span>. These approaches do not use the power of advanced statistical learning techniques to work on high-dimensional spaces with big training data sets <span class="citation">(James et al. 2013)</span>.</p>
<p>Package <strong>sits</strong> can use the full depth of satellite image time series to create larger dimensional spaces. We tested different methods of extracting attributes from time series data, including those reported by <span class="citation">Pelletier et al. (2016)</span> and <span class="citation">Kastens et al. (2017)</span>. We conclude that part of the information in the raw time series is lost after filtering. Thus, the method we developed uses all the data available in the time series samples. The idea is to have as many temporal attributes as possible, increasing the classification space’s dimension. Our experiments found that modern statistical models such as support vector machines and random forests perform better in high-dimensional spaces than in lower-dimensional ones.</p>
</div>
<div id="acknowledgements" class="section level2">
<h2><span class="header-section-number">1.11</span> Acknowledgements</h2>
<p>The authors would like to thank all the researchers that provided data samples used in the examples: Alexandre Coutinho, Julio Esquerdo, and Joao Antunes (Brazilian Agricultural Research Agency, Brazil) who provided ground samples for “soybean-fallow”, “fallow-cotton”, “soybean-cotton”, “soybean-corn”, “soybean-millet”, “soybean-sunflower”, and “pasture” classes; Rodrigo Bergotti (National Institute for Space Research, Brazil) who provided samples for “cerrado” and “forest” classes; and Damien Arvor (Rennes University, France) who provided ground samples for “soybean-fallow” class.</p>
<p>This work was partially funded by the São Paulo Research Foundation (FAPESP) through the eScience Program grant 2014/08398-6. We thank the Coordination for the Improvement of Higher Education Personnel (CAPES) and National Council for Scientific and Technological Development (CNPq) grants 312151/2014-4 (GC) and 140684/2016-6 (RS). We thank Ricardo Cartaxo and Lúbia Vinhas, who provided insight and expertise to support this paper.</p>
<p>This work has also been supported by the International Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety under Grant Agreement 17-III-084-Global-A-RESTORE+ (``RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil’’).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="acessing-time-series-information-in-sits.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["sitsbook.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
