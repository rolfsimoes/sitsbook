<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Post classification smoothing | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</title>
  <meta name="description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Post classification smoothing | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Post classification smoothing | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  
  <meta name="twitter:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Rolf Simoes" />
<meta name="author" content="Gilberto Camara" />
<meta name="author" content="Felipe Souza" />
<meta name="author" content="Lorena Santos" />
<meta name="author" content="Pedro R. Andrade" />
<meta name="author" content="Charlotte Peletier" />
<meta name="author" content="Alexandre Carvalho" />
<meta name="author" content="Karine Ferreira" />
<meta name="author" content="Gilberto Queiroz" />
<meta name="author" content="Victor Maus" />


<meta name="date" content="2021-09-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"/>
<link rel="next" href="validation-and-accuracy-measurements-in-sits.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SITS Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#main-reference-for-sits"><i class="fa fa-check"></i>Main reference for sits</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#publications-using-sits"><i class="fa fa-check"></i>Publications using sits</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reproducible-papers-used-in-building-sits-functions"><i class="fa fa-check"></i>Reproducible papers used in building sits functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="part"><span><b>I Overview</b></span></li>
<li class="chapter" data-level="1" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html"><i class="fa fa-check"></i><b>1</b> A taste of sits</a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#creating-a-data-cube"><i class="fa fa-check"></i><b>1.1</b> Creating a Data Cube</a></li>
<li class="chapter" data-level="1.2" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#the-time-series-table"><i class="fa fa-check"></i><b>1.2</b> The time series table</a></li>
<li class="chapter" data-level="1.3" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#training-a-machine-learning-model"><i class="fa fa-check"></i><b>1.3</b> Training a machine learning model</a></li>
<li class="chapter" data-level="1.4" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#data-cube-classification"><i class="fa fa-check"></i><b>1.4</b> Data cube classification</a></li>
<li class="chapter" data-level="1.5" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#spatial-smoothing"><i class="fa fa-check"></i><b>1.5</b> Spatial smoothing</a></li>
<li class="chapter" data-level="1.6" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#labelling-a-probability-data-cube"><i class="fa fa-check"></i><b>1.6</b> Labelling a probability data cube</a></li>
<li class="chapter" data-level="1.7" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#how-the-sits-api-works"><i class="fa fa-check"></i><b>1.7</b> How the sits API works</a></li>
<li class="chapter" data-level="1.8" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#final-remarks"><i class="fa fa-check"></i><b>1.8</b> Final remarks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html"><i class="fa fa-check"></i><b>2</b> Earth observation data cubes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="2.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#analysis-ready-data-image-collections"><i class="fa fa-check"></i><b>2.2</b> Analysis-ready data image collections</a></li>
<li class="chapter" data-level="2.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-and-image-collections-in-sits"><i class="fa fa-check"></i><b>2.3</b> Accessing Data Cubes and Image Collections in SITS</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-in-amazon-web-services"><i class="fa fa-check"></i><b>2.3.1</b> Accessing data cubes in Amazon Web Services</a></li>
<li class="chapter" data-level="2.3.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-the-brazil-data-cube"><i class="fa fa-check"></i><b>2.3.2</b> Accessing the Brazil Data Cube</a></li>
<li class="chapter" data-level="2.3.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#defining-a-data-cube-using-files"><i class="fa fa-check"></i><b>2.3.3</b> Defining a data cube using files</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#regularizing-data-cubes"><i class="fa fa-check"></i><b>2.4</b> Regularizing data cubes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="working-with-time-series.html"><a href="working-with-time-series.html"><i class="fa fa-check"></i><b>3</b> Working with time series</a>
<ul>
<li class="chapter" data-level="3.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#data-structures-for-satellite-time-series"><i class="fa fa-check"></i><b>3.1</b> Data structures for satellite time series</a></li>
<li class="chapter" data-level="3.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#utilities-for-handling-time-series"><i class="fa fa-check"></i><b>3.2</b> Utilities for handling time series</a></li>
<li class="chapter" data-level="3.3" data-path="working-with-time-series.html"><a href="working-with-time-series.html#time-series-visualisation"><i class="fa fa-check"></i><b>3.3</b> Time series visualisation</a></li>
<li class="chapter" data-level="3.4" data-path="working-with-time-series.html"><a href="working-with-time-series.html#obtaining-time-series-data-from-data-cubes"><i class="fa fa-check"></i><b>3.4</b> Obtaining time series data from data cubes</a></li>
<li class="chapter" data-level="3.5" data-path="working-with-time-series.html"><a href="working-with-time-series.html#filtering-techniques-for-time-series"><i class="fa fa-check"></i><b>3.5</b> Filtering techniques for time series</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#savitzkygolay-filter"><i class="fa fa-check"></i><b>3.5.1</b> Savitzky–Golay filter</a></li>
<li class="chapter" data-level="3.5.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#whittaker-filter"><i class="fa fa-check"></i><b>3.5.2</b> Whittaker filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Clustering</b></span></li>
<li class="chapter" data-level="4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html"><i class="fa fa-check"></i><b>4</b> Time Series Clustering to Improve the Quality of Training Samples</a>
<ul>
<li class="chapter" data-level="4.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#clustering-for-sample-quality-control-overview"><i class="fa fa-check"></i><b>4.1</b> Clustering for sample quality control: overview</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#hierachical-clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.2</b> Hierachical clustering for sample quality control</a></li>
<li class="chapter" data-level="4.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-self-organizing-maps-for-sample-quality-control"><i class="fa fa-check"></i><b>4.3</b> Using self-organizing maps for sample quality control</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#som-based-quality-assessment-part-1-creating-the-som-map"><i class="fa fa-check"></i><b>4.3.1</b> SOM-based quality assessment part 1: creating the SOM map</a></li>
<li class="chapter" data-level="4.3.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#som-based-quality-assessment-part-2-assessing-confusion-between-labels"><i class="fa fa-check"></i><b>4.3.2</b> SOM-based quality assessment part 2: assessing confusion between labels</a></li>
<li class="chapter" data-level="4.3.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#som-based-quality-assessment-part-3-using-probabilities-to-detect-noisy-samples"><i class="fa fa-check"></i><b>4.3.3</b> SOM-based quality assessment part 3: using probabilities to detect noisy samples</a></li>
<li class="chapter" data-level="4.3.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#comparing-original-and-clean-samples"><i class="fa fa-check"></i><b>4.3.4</b> Comparing Original and Clean Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>III Classification</b></span></li>
<li class="chapter" data-level="5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html"><i class="fa fa-check"></i><b>5</b> Machine Learning for Data Cubes using the SITS package</a>
<ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-classification"><i class="fa fa-check"></i><b>5.1</b> Machine learning classification</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#visualizing-samples"><i class="fa fa-check"></i><b>5.2</b> Visualizing Samples</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#common-interface-to-machine-learning-and-deeplearning-models"><i class="fa fa-check"></i><b>5.3</b> Common interface to machine learning and deeplearning models</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#random-forests"><i class="fa fa-check"></i><b>5.4</b> Random forests</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#support-vector-machines"><i class="fa fa-check"></i><b>5.5</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#extreme-gradient-boosting"><i class="fa fa-check"></i><b>5.6</b> Extreme Gradient Boosting</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#deep-learning-using-multi-layer-perceptrons"><i class="fa fa-check"></i><b>5.7</b> Deep learning using multi-layer perceptrons</a></li>
<li class="chapter" data-level="5.8" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#temporal-convolutional-neural-network-tempcnn"><i class="fa fa-check"></i><b>5.8</b> Temporal Convolutional Neural Network (TempCNN)</a></li>
<li class="chapter" data-level="5.9" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#residual-1d-cnn-networks-resnet"><i class="fa fa-check"></i><b>5.9</b> Residual 1D CNN Networks (ResNet)</a></li>
<li class="chapter" data-level="5.10" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#considerations-on-model-choice"><i class="fa fa-check"></i><b>5.10</b> Considerations on model choice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><i class="fa fa-check"></i><b>6</b> Classification of Images in Data Cubes using Satellite Image Time Series</a>
<ul>
<li class="chapter" data-level="6.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#data-cube-classification-1"><i class="fa fa-check"></i><b>6.1</b> Data cube classification</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#processing-time-estimates"><i class="fa fa-check"></i><b>6.2</b> Processing time estimates</a></li>
</ul></li>
<li class="part"><span><b>IV Post classification</b></span></li>
<li class="chapter" data-level="7" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html"><i class="fa fa-check"></i><b>7</b> Post classification smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bayesian-smoothing"><i class="fa fa-check"></i><b>7.2</b> Bayesian smoothing</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#derivation-of-bayesian-parameters-for-spatiotemporal-smoothing"><i class="fa fa-check"></i><b>7.2.1</b> Derivation of bayesian parameters for spatiotemporal smoothing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#use-of-bayesian-smoothing-in-sits"><i class="fa fa-check"></i><b>7.3</b> Use of Bayesian smoothing in SITS</a></li>
<li class="chapter" data-level="7.4" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bilateral-smoothing"><i class="fa fa-check"></i><b>7.4</b> Bilateral smoothing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html"><i class="fa fa-check"></i><b>8</b> Validation and accuracy measurements in SITS</a>
<ul>
<li class="chapter" data-level="8.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#validation-techniques"><i class="fa fa-check"></i><b>8.1</b> Validation techniques</a></li>
<li class="chapter" data-level="8.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#comparing-different-machine-learning-methods-using-k-fold-validation"><i class="fa fa-check"></i><b>8.2</b> Comparing different machine learning methods using k-fold validation</a></li>
<li class="chapter" data-level="8.3" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#accuracy-assessment"><i class="fa fa-check"></i><b>8.3</b> Accuracy assessment</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#time-series"><i class="fa fa-check"></i><b>8.3.1</b> Time series</a></li>
<li class="chapter" data-level="8.3.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#classified-images"><i class="fa fa-check"></i><b>8.3.2</b> Classified images</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>9</b> Case studies</a></li>
<li class="chapter" data-level="10" data-path="design-and-extensibility-considerations.html"><a href="design-and-extensibility-considerations.html"><i class="fa fa-check"></i><b>10</b> Design and extensibility considerations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="design-and-extensibility-considerations.html"><a href="design-and-extensibility-considerations.html#design-decisions"><i class="fa fa-check"></i><b>10.1</b> Design decisions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>sits</strong>: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="post-classification-smoothing" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Post classification smoothing</h1>
<hr />
<p>This chapter describes the methods available for spatial smoothing of the results of machine learning classifications.</p>
<hr />
<div id="introduction" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction</h2>
<p>Smoothing methods are an important complement to machine learning algorithms for image classification. Since these methods are mostly pixel-based, it is useful to complement them with post-processing smoothing to include spatial information in the result. For each pixel, machine learning and other statistical algorithms provide the probabilities of that pixel belonging to each of the classes. As a first step in obtaining a result, each pixel is assigned to the class whose probability is higher. After this step, smoothing methods use class probabilities to detect and correct outliers or misclassified pixels.</p>
<p>Image classification post-processing has been defined as “a refinement of the labelling in a classified image in order to enhance its classification accuracy” <span class="citation">[53]</span>. In remote sensing image analysis, these procedures are used to combine pixel-based classification methods with a spatial post-processing method to remove outliers and misclassified pixels. For pixel-based classifiers, post-processing methods enable the inclusion of spatial information in the final results.</p>
<p>Post-processing is a desirable step in any classification process. Most statistical classifiers use training samples derived from “pure” pixels, that have been selected by users as representative of the desired output classes. However, images contain many mixed pixels irrespective of the resolution. Also, there is a considerable degree of data variability in each class. These effects lead to outliers whose chance of misclassification is significant. To offset these problems, most post-processing methods use the “smoothness assumption” <span class="citation">[54]</span>: nearby pixels tend to have the same label. To put this assumption in practice, smoothing methods use the neighbourhood information to remove outliers and enhance consistency in the resulting product.</p>
<p>The following spatial smoothing methods are available in <strong>sits</strong>: bayesian smoothing, gaussian smoothing and bilinear smoothing. These methods are called using the <code>sits_smooth</code> function, as shown in the examples below.</p>
</div>
<div id="bayesian-smoothing" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Bayesian smoothing</h2>
<p>Bayesian inference can be thought of as way of coherently updating our uncertainty in the light of new evidence. It allows the inclusion of expert knowledge on the derivation of probabilities. In a Bayesian context, probability is taken as a subjective belief. The observation of the class probabilities of each pixel is taken as our initial belief on what the actual class of the pixel is. We then use Bayes’ rule to consider how much the class probabilities of the neighbouring pixels affect our original belief. In the case of continuous probability distributions, Bayesian inference is expressed by the rule:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \pi(x|\theta)\pi(\theta)
\]</span></p>
<p>Bayesian inference involves the estimation of an unknown parameter <span class="math inline">\(\theta\)</span>, which is the random variable that describe what we are trying to measure. In the case of smoothing of image classification, <span class="math inline">\(\theta\)</span> is the class probability for a given pixel. We model our initial belief about this value by a probability distribution, <span class="math inline">\(\pi(\theta)\)</span>, called the  distribution. It represents what we know about <span class="math inline">\(\theta\)</span>  observing the data. The distribution <span class="math inline">\(\pi(x|\theta)\)</span>, called the , is estimated based on the observed data. It represents the added information provided by our observations. The  distribution <span class="math inline">\(\pi(\theta|x)\)</span> is our improved belief of <span class="math inline">\(\theta\)</span>  seeing the data. Bayes’s rule states that the  probability is proportional to the product of the  and the  probability.</p>
<div id="derivation-of-bayesian-parameters-for-spatiotemporal-smoothing" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Derivation of bayesian parameters for spatiotemporal smoothing</h3>
<p>In our post-classification smoothing model, we consider the output of a machine learning algorithm that provides the probabilities of each pixel in the image to belong to target classes. More formally, consider a set of <span class="math inline">\(K\)</span> classes that are candidates for labelling each pixel. Let <span class="math inline">\(p_{i,t,k}\)</span> be the probability of pixel <span class="math inline">\(i\)</span> belonging to class <span class="math inline">\(k\)</span>, <span class="math inline">\(k = 1, \dots, K\)</span> at a time <span class="math inline">\(t\)</span>, <span class="math inline">\(t=1,\dots{},T\)</span>. We have
<span class="math display">\[
\sum_{k=1}^K p_{i,t,k} = 1, p_{i,t,k} &gt; 0
\]</span>
We label a pixel <span class="math inline">\(p_i\)</span> as being of class <span class="math inline">\(k\)</span> if
<span class="math display">\[
    p_{i,t,k} &gt; p_{i,t,m}, \forall m = 1, \dots, K, m \neq k
\]</span></p>
<p>For each pixel <span class="math inline">\(i\)</span>, we take the odds of the classification for class <span class="math inline">\(k\)</span>, expressed as
<span class="math display">\[
    O_{i,t,k} = p_{i,t,k} / (1-p_{i,t,k})
\]</span>
where <span class="math inline">\(p_{i,t,k}\)</span> is the probability of class <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>. We have more confidence in pixels with higher odds since their class assignment is stronger. There are situations, such as border pixels or mixed ones, where the odds of different classes are similar in magnitude. We take them as cases of low confidence in the classification result. To assess and correct these cases, Bayesian smoothing methods borrow strength from the neighbors and reduces the variance of the estimated class for each pixel.</p>
<p>We further make the transformation
<span class="math display">\[
    x_{i,t,k} = \log [O_{i,t,k}]
\]</span>
which measures the <em>logit</em> (log of the odds) associated to classifying the pixel <span class="math inline">\(i\)</span> as being of class <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>. The support of <span class="math inline">\(x_{i,t,k}\)</span> is <span class="math inline">\(\mathbb{R}\)</span>. We can express the pixel data as a <span class="math inline">\(K\)</span>-dimensional multivariate logit vector</p>
<p><span class="math display">\[
\mathbf{x}_{i,t}=(x_{i,t,k_{0}},x_{i,t,k_{1}},\dots{},x_{i,t,k_{K}})
\]</span></p>
<p>For each pixel, the random variable that describes the class probability <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span> is denoted by <span class="math inline">\(\theta_{i,t,k}\)</span>. This formulation allows uses to use the class covariance matrix in our formulations. We can express Bayes’ rule for all combinations of pixel and classes for a time interval as</p>
<p><span class="math display">\[
\pi(\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}) \propto \pi(\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t})\pi(\boldsymbol\theta_{i,t}).    
\]</span></p>
<p>We assume the conditional distribution <span class="math inline">\(\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t}\)</span> follows a multivariate normal distribution</p>
<p><span class="math display">\[
    [\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t}]\sim\mathcal{N}_{K}(\boldsymbol\theta_{i,t},\boldsymbol\Sigma_{i,t}),
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol\theta_{i,t}\)</span> is the mean parameter vector for the pixel <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span> is a known <span class="math inline">\(k\times{}k\)</span> covariance matrix that we will use as a parameter to control the level of smoothness effect. We will discuss later on how to estimate <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span>. To model our uncertainty about the parameter <span class="math inline">\(\boldsymbol\theta_{i,t}\)</span>, we will assume it also follows a multivariate normal distribution with hyper-parameters <span class="math inline">\(\mathbf{m}_{i,t}\)</span> for the mean vector, and <span class="math inline">\(\mathbf{S}_{i,t}\)</span> for the covariance matrix.</p>
<p><span class="math display">\[
    [\boldsymbol\theta_{i,t}]\sim\mathcal{N}_{K}(\mathbf{m}_{i,t}, \mathbf{S}_{i,t}).
\]</span></p>
<p>The above equation defines our prior distribution. The hyper-parameters <span class="math inline">\(\mathbf{m}_{i,t}\)</span> and <span class="math inline">\(\mathbf{S}_{i,t}\)</span> are obtained by considering the neighboring pixels of pixel <span class="math inline">\(i\)</span>. The neighborhood can be defined as any graph scheme (e.g. a given Chebyshev distance on the time-space lattice) and can include the referencing pixel <span class="math inline">\(i\)</span> as a neighbor. Also, it can make no reference to time steps other than <span class="math inline">\(t\)</span> defining a space-only neighborhood. More formally, let</p>
<p><span class="math display">\[
    \mathbf{V}_{i,t}=\{\mathbf{x}_{i_{j},t_{j}}\}_{j=1}^{N}, 
\]</span>
denote the <span class="math inline">\(N\)</span> logit vectors of a spatiotemporal neighborhood <span class="math inline">\(N\)</span> of pixel <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>. Then the prior mean is calculated by</p>
<p><span class="math display">\[
    \mathbf{m}_{i,t}=\operatorname{E}[\mathbf{V}_{i,t}],
\]</span></p>
<p>and the prior covariance matrix by</p>
<p><span class="math display">\[
    \mathbf{S}_{i,t}=\operatorname{E}\left[
      \left(\mathbf{V}_{i,t}-\mathbf{m}_{i,t}\right)
      \left(\mathbf{V}_{i,t}-\mathbf{m}_{i,t}\right)^\intercal
    \right].
\]</span></p>
<p>Since the likelihood and prior are multivariate normal distributions, the posterior will also be a multivariate normal distribution, whose updated parameters can be derived by applying the density functions associated to the above equations. The posterior distribution is given by</p>
<p><span class="math display">\[
    [\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}]\sim\mathcal{N}_{K}\left(
    (\mathbf{S}_{i,t}^{-1} + \boldsymbol\Sigma^{-1})^{-1}( \mathbf{S}_{i,t}^{-1}\mathbf{m}_{i,t} + \boldsymbol\Sigma^{-1} \mathbf{x}_{i,t}),
    (\mathbf{S}_{i,t}^{-1} + \boldsymbol\Sigma^{-1})^{-1}
    \right).
\]</span></p>
<p>The <span class="math inline">\(\boldsymbol\theta_{i,t}\)</span> parameter model is our initial belief about a pixel vector using the neighborhood information in the prior distribution. It represents what we know about the probable value of <span class="math inline">\(\mathbf{x}_{i,t}\)</span> (and hence, about the class probabilities as the logit function is a monotonically increasing function)  observing it. The  function <span class="math inline">\(P[\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t}]\)</span> represents the added information provided by our observation of <span class="math inline">\(\mathbf{x}_{i,t}\)</span>. The  probability density function <span class="math inline">\(P[\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}]\)</span> is our improved belief of the pixel vector  seeing <span class="math inline">\(\mathbf{x}_{i,t}\)</span>.</p>
<p>At this point, we are able to infer a point estimator <span class="math inline">\(\hat{\boldsymbol\theta}_{i,t}\)</span> for the <span class="math inline">\(\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}\)</span> parameter. For the multivariate normal distribution, the posterior mean minimises not only the quadratic loss but the absolute and zero-one loss functions. It can be taken from the updated mean parameter of the posterior distribution which, after some algebra, can be expressed as</p>
<p><span class="math display">\[
    \hat{\boldsymbol{\theta}}_{i,t}=\operatorname{E}[\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}]=\boldsymbol\Sigma_{i,t}\left(\boldsymbol\Sigma_{i,t}+\mathbf{S}_{i,t}\right)^{-1}\mathbf{m}_{i,t} +
    \mathbf{S}_{i,t}\left(\boldsymbol\Sigma_{i,t}+\mathbf{S}_{i,t}\right)^{-1}\mathbf{x}_{i,t}.
\]</span></p>
<p>The estimator value for the logit vector <span class="math inline">\(\hat{\boldsymbol\theta}_{i,t}\)</span> is a weighted combination of the original logit vector <span class="math inline">\(\mathbf{x}_{i,t}\)</span> and the neighborhood mean vector <span class="math inline">\(\mathbf{m}_{i,t}\)</span>. The weights are given by the covariance matrix <span class="math inline">\(\mathbf{S}_{i,t}\)</span> of the prior distribution and the covariance matrix of the conditional distribution. The matrix <span class="math inline">\(\mathbf{S}_{i,t}\)</span> is calculated considering the spatiotemporal neighbors and the matrix <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span> corresponds to the smoothing factor provided as prior belief by the user.</p>
<p>When the values of local class covariance <span class="math inline">\(\mathbf{S}_{i,t}\)</span> are relative to the conditional covariance <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span>, our confidence on the influence of the neighbors is low, and the smoothing algorithm gives more weight to the original pixel value <span class="math inline">\(x_{i,k}\)</span>. When the local class covariance <span class="math inline">\(\mathbf{S}_{i,t}\)</span> decreases relative to the smoothness factor <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span>, then our confidence on the influence of the neighborhood increases. The smoothing procedure will be most relevant in situations where the original classification odds ratio is low, showing a low level of separability between classes. In these cases, the updated values of the classes will be influenced by the local class variances.</p>
<p>In practice, <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span> is a user-controlled covariance matrix parameter that will be set by users based on their knowledge of the region to be classified. In the simplest case, users can associate the conditional covariance <span class="math inline">\(\boldsymbol\Sigma_{i,t}\)</span> to a diagonal matrix, using only one hyperparameter <span class="math inline">\(\sigma^2_k\)</span> to set the level of smoothness. Higher values of <span class="math inline">\(\sigma^2_k\)</span> will cause the assignment of the local mean to the pixel updated probability. In our case, after some classification tests, we decided to <span class="math inline">\(\sigma^2_k=20\)</span> by default for all <span class="math inline">\(k\)</span>.</p>
</div>
</div>
<div id="use-of-bayesian-smoothing-in-sits" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Use of Bayesian smoothing in SITS</h2>
<p>Doing post-processing using Bayesian smoothing in SITS is straightforward. The result of the <code>sits_classify</code> function applied to a data cube is set of probability images, one per class. The next step is to apply the <code>sits_smooth</code> function. By default, this function selects the most likely class for each pixel considering only the probabilities of each class for each pixel. To allow for Bayesian smoothing, it suffices to include the <code>type = bayesian</code> parameter (which is also the default). If desired, the <code>smoothness</code> parameter (associated to the hyperparameter <span class="math inline">\(\sigma^2_k\)</span> described above) can control the degree of smoothness. If so desired, the <code>smoothness</code> parameter can also be expressed as a matrix</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="post-classification-smoothing.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the data for the Mato Grosso state</span></span>
<span id="cb81-2"><a href="post-classification-smoothing.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;samples_modis_4bands&quot;</span>)</span>
<span id="cb81-3"><a href="post-classification-smoothing.html#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="post-classification-smoothing.html#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="co"># select the bands &quot;ndvi&quot;, &quot;evi&quot;</span></span>
<span id="cb81-5"><a href="post-classification-smoothing.html#cb81-5" aria-hidden="true" tabindex="-1"></a>samples_2bands <span class="ot">&lt;-</span> <span class="fu">sits_select</span>(samples_modis_4bands, <span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span>
<span id="cb81-6"><a href="post-classification-smoothing.html#cb81-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-7"><a href="post-classification-smoothing.html#cb81-7" aria-hidden="true" tabindex="-1"></a><span class="co">#select a rfor model</span></span>
<span id="cb81-8"><a href="post-classification-smoothing.html#cb81-8" aria-hidden="true" tabindex="-1"></a>rfor_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(samples_2bands, <span class="at">ml_method =</span> <span class="fu">sits_rfor</span>(<span class="at">verbose =</span> <span class="dv">0</span>))</span>
<span id="cb81-9"><a href="post-classification-smoothing.html#cb81-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-10"><a href="post-classification-smoothing.html#cb81-10" aria-hidden="true" tabindex="-1"></a>data_dir <span class="ot">&lt;-</span> <span class="fu">system.file</span>(<span class="st">&quot;extdata/sinop&quot;</span>, <span class="at">package =</span> <span class="st">&quot;sitsdata&quot;</span>)</span>
<span id="cb81-11"><a href="post-classification-smoothing.html#cb81-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-12"><a href="post-classification-smoothing.html#cb81-12" aria-hidden="true" tabindex="-1"></a><span class="co"># create a raster metadata file based on the information about the files</span></span>
<span id="cb81-13"><a href="post-classification-smoothing.html#cb81-13" aria-hidden="true" tabindex="-1"></a>raster_cube <span class="ot">&lt;-</span> <span class="fu">sits_cube</span>(<span class="at">source =</span> <span class="st">&quot;LOCAL&quot;</span>,</span>
<span id="cb81-14"><a href="post-classification-smoothing.html#cb81-14" aria-hidden="true" tabindex="-1"></a>                       <span class="at">origin     =</span> <span class="st">&quot;BDC&quot;</span>,</span>
<span id="cb81-15"><a href="post-classification-smoothing.html#cb81-15" aria-hidden="true" tabindex="-1"></a>                       <span class="at">collection =</span> <span class="st">&quot;MOD13Q1-6&quot;</span>,</span>
<span id="cb81-16"><a href="post-classification-smoothing.html#cb81-16" aria-hidden="true" tabindex="-1"></a>                       <span class="at">name =</span> <span class="st">&quot;Sinop&quot;</span>,</span>
<span id="cb81-17"><a href="post-classification-smoothing.html#cb81-17" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data_dir =</span> data_dir,</span>
<span id="cb81-18"><a href="post-classification-smoothing.html#cb81-18" aria-hidden="true" tabindex="-1"></a>                       <span class="at">parse_info =</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;tile&quot;</span>, <span class="st">&quot;band&quot;</span>, <span class="st">&quot;date&quot;</span>)</span>
<span id="cb81-19"><a href="post-classification-smoothing.html#cb81-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb81-20"><a href="post-classification-smoothing.html#cb81-20" aria-hidden="true" tabindex="-1"></a><span class="co"># classify the raster image and generate a probability file</span></span>
<span id="cb81-21"><a href="post-classification-smoothing.html#cb81-21" aria-hidden="true" tabindex="-1"></a>raster_probs <span class="ot">&lt;-</span> <span class="fu">sits_classify</span>(raster_cube, <span class="at">ml_model =</span> rfor_model, </span>
<span id="cb81-22"><a href="post-classification-smoothing.html#cb81-22" aria-hidden="true" tabindex="-1"></a>                              <span class="at">memsize =</span> <span class="dv">1</span>, <span class="at">multicores =</span> <span class="dv">1</span>, </span>
<span id="cb81-23"><a href="post-classification-smoothing.html#cb81-23" aria-hidden="true" tabindex="-1"></a>                              <span class="at">output_dir =</span> <span class="fu">tempdir</span>())</span>
<span id="cb81-24"><a href="post-classification-smoothing.html#cb81-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-25"><a href="post-classification-smoothing.html#cb81-25" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(raster_probs)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt; downsample set to c(1,1,1)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-68"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-68-1.png" alt="Probability values for classified image" width="70%" />
<p class="caption">
Figure 7.1: Probability values for classified image
</p>
</div>
<p>The plots show the class probabilities, which can then be smoothed by a bayesian smoother.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="post-classification-smoothing.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># smooth the result with a bayesian filter</span></span>
<span id="cb83-2"><a href="post-classification-smoothing.html#cb83-2" aria-hidden="true" tabindex="-1"></a>raster_probs_bayes <span class="ot">&lt;-</span> <span class="fu">sits_smooth</span>(raster_probs, </span>
<span id="cb83-3"><a href="post-classification-smoothing.html#cb83-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">type =</span> <span class="st">&quot;bayes&quot;</span>,</span>
<span id="cb83-4"><a href="post-classification-smoothing.html#cb83-4" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">output_dir =</span> <span class="fu">tempdir</span>())</span>
<span id="cb83-5"><a href="post-classification-smoothing.html#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the result</span></span>
<span id="cb83-6"><a href="post-classification-smoothing.html#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(raster_probs_bayes)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt; downsample set to c(1,1,1)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-69"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-69-1.png" alt="Probability values smoothed by bayesian method" width="70%" />
<p class="caption">
Figure 7.2: Probability values smoothed by bayesian method
</p>
</div>
<p>The bayesian smoothing has removed some of local variability associated to misclassified pixels which are different from their neighbors. The impact of smoothing is best appreciated comparing the labelled map produced without smoothing to the one that follows the procedure, as shown below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-70"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-70-1.png" alt="Classified image without smoothing" width="90%" />
<p class="caption">
Figure 7.3: Classified image without smoothing
</p>
</div>
<p>The resulting labelled map shows a number of likely misclassified pixels which can be removed using the bayesian smoother.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-71"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-71-1.png" alt="Classified image with Bayesian smoothing" width="90%" />
<p class="caption">
Figure 7.4: Classified image with Bayesian smoothing
</p>
</div>
<p>Comparing the two plots, it is apparent that the smoothing procedure has reduced a lot of the noise in the original classification and produced a more homogeneous result.</p>
</div>
<div id="bilateral-smoothing" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Bilateral smoothing</h2>
<p>One of the problems with post-classification smoothing is that we would like to remove noisy pixels (e.g., a pixel with high probability of being labeled “Forest” in the midst of pixels likely to be labeled “Cerrado”), but would like to preserve the edges between areas. Because of its design, bilateral filter has proven to be a useful method for post-classification processing since it preserves edges while removing noisy pixels <span class="citation">[54]</span>.</p>
<p>Bilateral smoothing combines proximity (combining pixels which are close) and similarity (comparing the values of the pixels) <span class="citation">[55]</span>. If most of the pixels in a neighborhood have similar values, it is easy to identify outliers and noisy pixels. In contrast, there is a strong difference between the values of pixels in a if neighborhood, it is possible that the pixel is located in a class boundary. Bilateral filtering combines domain filtering with range filtering. In domain filtering, the weights used to combine pixels decrease with distance. In range filtering, the weights are computed considering value similarity.</p>
<p>The combination of domain and range filtering is mathematically expressed as:</p>
<p><span class="math display">\[
S(x_i) = \frac{1}{W_{i}} \sum_{x_k \in \theta} I(x_k)\mathcal{N}_{\tau}(\|I(x_k) - I(x_i)\|)\mathcal{N}_{\sigma}(\|x_k - x_i\|),
\]</span>
where</p>
<ul>
<li><span class="math inline">\(S(x_i)\)</span> is the smoothed value of pixel <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(I\)</span> is the original probability image to be filtered;</li>
<li><span class="math inline">\(I(x_i)\)</span> is the value of pixel <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(\theta\)</span> is the neighborhood centered in <span class="math inline">\(x_i\)</span>;</li>
<li><span class="math inline">\(x_k\)</span> is a pixel <span class="math inline">\(k\)</span> which belongs to neighborhood <span class="math inline">\(\theta\)</span>;</li>
<li><span class="math inline">\(I(x_k)\)</span> is the value of a pixel <span class="math inline">\(k\)</span> in the neighborhood of pixel <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(\|I(x_k) - I(x_i)\|\)</span> is the absolute difference between the values of the pixel <span class="math inline">\(k\)</span> and pixel <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(\|x_k - x_i\|\)</span> is the distance between pixel <span class="math inline">\(k\)</span> and pixel <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(\mathcal{N}_{\tau}\)</span> is the Gaussian range kernel for smoothing differences in intensities;</li>
<li><span class="math inline">\(\mathcal{N}_{\sigma}\)</span>is the Gaussian spatial kernel for smoothing differences based on proximity.</li>
<li><span class="math inline">\(\tau\)</span> is the variance of the Gaussian range kernel;</li>
<li><span class="math inline">\(\sigma\)</span> is the variance of the Gaussian spatial kernel.</li>
</ul>
<p>The normalization term to be applied to compute the smoothed values of pixel <span class="math inline">\(i\)</span> is defined as</p>
<p><span class="math display">\[
W_{i} = \sum_{x_k \in \theta}{\mathcal{N}_{\tau}(\|I(x_k) - I(x_i)\|)\mathcal{N}_{\sigma}(\|x_k - x_i\|)}
\]</span></p>
<p>For every pixel, the method takes a considers two factors: the distance between the pixel and its neighbors, and the difference in value between them. Each of the values contributes according to a Gaussian kernel. These factors are calculated independently. Big difference between pixel values reduce the influence of the neighbor in the smoothed pixel. Big distance between pixels also reduce the impact of neighbors. The achieve a satisfactory result, we need to balance the <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span>. As a general rule, the values of <span class="math inline">\(\tau\)</span> should range from 0.05 to 0.50, while the values of <span class="math inline">\(\sigma\)</span> should vary between 4 and 16<span class="citation">[56]</span>. The default values adopted in <em>sits</em> are <code>tau = 0.1</code> and <code>sigma = 8</code>. As the best values of <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\sigma\)</span> depend on the variance of the noisy pixels, users are encouraged to experiment and find parameter values that best fit their requirements.</p>
<p>The following example shows the behavior of the bilateral smoother.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="post-classification-smoothing.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># smooth the result with a bilateral filter</span></span>
<span id="cb85-2"><a href="post-classification-smoothing.html#cb85-2" aria-hidden="true" tabindex="-1"></a>raster_probs_bil <span class="ot">&lt;-</span> <span class="fu">sits_smooth</span>(raster_probs, </span>
<span id="cb85-3"><a href="post-classification-smoothing.html#cb85-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">type =</span> <span class="st">&quot;bilateral&quot;</span>,</span>
<span id="cb85-4"><a href="post-classification-smoothing.html#cb85-4" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">sigma =</span> <span class="dv">8</span>,</span>
<span id="cb85-5"><a href="post-classification-smoothing.html#cb85-5" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">tau =</span> <span class="fl">0.1</span>,</span>
<span id="cb85-6"><a href="post-classification-smoothing.html#cb85-6" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">output_dir =</span> <span class="fu">tempdir</span>())</span>
<span id="cb85-7"><a href="post-classification-smoothing.html#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the result</span></span>
<span id="cb85-8"><a href="post-classification-smoothing.html#cb85-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(raster_probs_bil)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt; downsample set to c(1,1,1)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-72"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-72-1.png" alt="Probability values for classified image smoothed by bilateral filter" width="70%" />
<p class="caption">
Figure 7.5: Probability values for classified image smoothed by bilateral filter
</p>
</div>
<p>The impact on the classified image can be seen in the following example.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-73"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-73-1.png" alt="Classified image with bilateral smoothing" width="90%" />
<p class="caption">
Figure 7.6: Classified image with bilateral smoothing
</p>
</div>
<p>Bayesian smoothing tends to produce more homogeneous labeled images than bilateral smoothing. However, some spatial details and some edges are better preserved by the bilateral method. Choosing between the methods depends on user needs and requirements. In any case, as stated by <span class="citation">[54]</span>, smoothing improves the quality of classified images and thus should be applied in most situations.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="validation-and-accuracy-measurements-in-sits.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["sitsbook.pdf"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
