<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</title>
  <meta name="description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="generator" content="bookdown 0.21.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  
  <meta name="twitter:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Rolf Simoes" />
<meta name="author" content="Gilberto Camara" />
<meta name="author" content="Felipe Souza" />
<meta name="author" content="Lorena Santos" />
<meta name="author" content="Pedro R. Andrade" />
<meta name="author" content="Alexandre Carvalho" />
<meta name="author" content="Karine Ferreira" />
<meta name="author" content="Gilberto Queiroz" />
<meta name="author" content="Victor Maus" />


<meta name="date" content="2021-04-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="time-series-clustering-to-improve-the-quality-of-training-samples.html"/>
<link rel="next" href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SITS Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#publications-using-sits"><i class="fa fa-check"></i>Publications using sits</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reproducible-papers-used-in-building-sits-functions"><i class="fa fa-check"></i>Reproducible papers used in building sits functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a><ul>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html#docker-images"><i class="fa fa-check"></i>Docker images</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="part"><span><b>I Overview</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#workflow-and-api"><i class="fa fa-check"></i><b>1.1</b> Workflow and API</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#handling-data-cubes-in-sits"><i class="fa fa-check"></i><b>1.2</b> Handling Data Cubes in sits</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#handling-satellite-image-time-series-in-sits"><i class="fa fa-check"></i><b>1.3</b> Handling satellite image time series in <strong>sits</strong></a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#data-structure"><i class="fa fa-check"></i><b>1.3.1</b> Data structure</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#obtaining-time-series-data"><i class="fa fa-check"></i><b>1.3.2</b> Obtaining time series data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sample-quality-control-using-clustering"><i class="fa fa-check"></i><b>1.4</b> Sample quality control using clustering</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#classification-using-machine-learning"><i class="fa fa-check"></i><b>1.5</b> Classification using machine learning</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#cube-classification"><i class="fa fa-check"></i><b>1.6</b> Cube classification</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#smoothing-and-labelling-of-raster-data-after-classification"><i class="fa fa-check"></i><b>1.7</b> Smoothing and Labelling of raster data after classification</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#validation-techniques"><i class="fa fa-check"></i><b>1.8</b> Validation techniques</a></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#final-remarks"><i class="fa fa-check"></i><b>1.9</b> Final remarks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html"><i class="fa fa-check"></i><b>2</b> Earth observation data cubes</a><ul>
<li class="chapter" data-level="2.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="2.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#using-stac-to-access-image-collection"><i class="fa fa-check"></i><b>2.2</b> Using STAC to Access Image Collection</a></li>
<li class="chapter" data-level="2.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-in-amazon-web-services"><i class="fa fa-check"></i><b>2.3</b> Accessing data cubes in Amazon Web Services</a></li>
<li class="chapter" data-level="2.4" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-the-brazil-data-cube"><i class="fa fa-check"></i><b>2.4</b> Accessing the Brazil Data Cube</a></li>
<li class="chapter" data-level="2.5" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#defining-a-data-cube-using-files"><i class="fa fa-check"></i><b>2.5</b> Defining a data cube using files</a></li>
<li class="chapter" data-level="2.6" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#regularizing-data-cubes"><i class="fa fa-check"></i><b>2.6</b> Regularizing data cubes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="working-with-time-series.html"><a href="working-with-time-series.html"><i class="fa fa-check"></i><b>3</b> Working with time series</a><ul>
<li class="chapter" data-level="3.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#data-structures-for-satellite-time-series"><i class="fa fa-check"></i><b>3.1</b> Data structures for satellite time series</a></li>
<li class="chapter" data-level="3.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#utilities-for-handling-time-series"><i class="fa fa-check"></i><b>3.2</b> Utilities for handling time series</a></li>
<li class="chapter" data-level="3.3" data-path="working-with-time-series.html"><a href="working-with-time-series.html#time-series-visualisation"><i class="fa fa-check"></i><b>3.3</b> Time series visualisation</a></li>
<li class="chapter" data-level="3.4" data-path="working-with-time-series.html"><a href="working-with-time-series.html#obtaining-time-series-data-from-data-cubes"><i class="fa fa-check"></i><b>3.4</b> Obtaining time series data from data cubes</a></li>
<li class="chapter" data-level="3.5" data-path="working-with-time-series.html"><a href="working-with-time-series.html#filtering-techniques-for-time-series"><i class="fa fa-check"></i><b>3.5</b> Filtering techniques for time series</a><ul>
<li class="chapter" data-level="3.5.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#savitzkygolay-filter"><i class="fa fa-check"></i><b>3.5.1</b> Savitzky–Golay filter</a></li>
<li class="chapter" data-level="3.5.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#whittaker-filter"><i class="fa fa-check"></i><b>3.5.2</b> Whittaker filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Clustering</b></span></li>
<li class="chapter" data-level="4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html"><i class="fa fa-check"></i><b>4</b> Time Series Clustering to Improve the Quality of Training Samples</a><ul>
<li class="chapter" data-level="4.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.1</b> Clustering for sample quality control</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#hierachical-clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.2</b> Hierachical clustering for Sample Quality Control</a><ul>
<li class="chapter" data-level="4.2.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#creating-a-dendogram"><i class="fa fa-check"></i><b>4.2.1</b> Creating a dendogram</a></li>
<li class="chapter" data-level="4.2.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-a-dendrogram-to-evaluate-sample-quality"><i class="fa fa-check"></i><b>4.2.2</b> Using a dendrogram to evaluate sample quality</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-self-organizing-maps-for-sample-quality"><i class="fa fa-check"></i><b>4.3</b> Using Self-organizing Maps for Sample Quality</a><ul>
<li class="chapter" data-level="4.3.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#introduction-to-self-organizing-maps"><i class="fa fa-check"></i><b>4.3.1</b> Introduction to Self-organizing Maps</a></li>
<li class="chapter" data-level="4.3.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-som-for-removing-class-noise"><i class="fa fa-check"></i><b>4.3.2</b> Using SOM for removing class noise</a></li>
<li class="chapter" data-level="4.3.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#comparing-global-accuracy-of-original-and-clean-samples"><i class="fa fa-check"></i><b>4.3.3</b> Comparing Global Accuracy of Original and Clean Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>III Classification</b></span></li>
<li class="chapter" data-level="5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html"><i class="fa fa-check"></i><b>5</b> Machine Learning for Data Cubes using the SITS package</a><ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-classification"><i class="fa fa-check"></i><b>5.1</b> Machine learning classification</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#visualizing-samples"><i class="fa fa-check"></i><b>5.2</b> Visualizing Samples</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#common-interface-to-machine-learning-and-deeplearning-models"><i class="fa fa-check"></i><b>5.3</b> Common interface to machine learning and deeplearning models</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#random-forests"><i class="fa fa-check"></i><b>5.4</b> Random forests</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#support-vector-machines"><i class="fa fa-check"></i><b>5.5</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#extreme-gradient-boosting"><i class="fa fa-check"></i><b>5.6</b> Extreme Gradient Boosting</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#deep-learning-using-multi-layer-perceptrons"><i class="fa fa-check"></i><b>5.7</b> Deep learning using multi-layer perceptrons</a></li>
<li class="chapter" data-level="5.8" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#combined-1d-cnn-and-multi-layer-perceptron-networks"><i class="fa fa-check"></i><b>5.8</b> Combined 1D CNN and multi-layer perceptron networks</a></li>
<li class="chapter" data-level="5.9" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#d-convolutional-neural-networks"><i class="fa fa-check"></i><b>5.9</b> 1D Convolutional Neural Networks</a></li>
<li class="chapter" data-level="5.10" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#residual-1d-cnn-networks-resnet"><i class="fa fa-check"></i><b>5.10</b> Residual 1D CNN Networks (ResNet)</a></li>
<li class="chapter" data-level="5.11" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#lstm-convolutional-networks-for-time-series-classification"><i class="fa fa-check"></i><b>5.11</b> LSTM Convolutional Networks for Time Series Classification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><i class="fa fa-check"></i><b>6</b> Classification of Images in Data Cubes using Satellite Image Time Series</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis-1"><i class="fa fa-check"></i><b>6.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#defining-a-data-cube-using-files-organised-as-raster-bricks"><i class="fa fa-check"></i><b>6.2</b> Defining a data cube using files organised as raster bricks</a></li>
<li class="chapter" data-level="6.3" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#classification-using-machine-learning-1"><i class="fa fa-check"></i><b>6.3</b> Classification using machine learning</a></li>
<li class="chapter" data-level="6.4" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#cube-classification-1"><i class="fa fa-check"></i><b>6.4</b> Cube classification</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#steps-for-cube-classification"><i class="fa fa-check"></i><b>6.4.1</b> Steps for cube classification</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#adjustments-for-improved-performance"><i class="fa fa-check"></i><b>6.4.2</b> Adjustments for improved performance</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#final-remarks-1"><i class="fa fa-check"></i><b>6.5</b> Final remarks</a></li>
</ul></li>
<li class="part"><span><b>IV Post classification</b></span></li>
<li class="chapter" data-level="7" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html"><i class="fa fa-check"></i><b>7</b> Post classification smoothing</a><ul>
<li class="chapter" data-level="7.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bayesian-smoothing"><i class="fa fa-check"></i><b>7.2</b> Bayesian smoothing</a><ul>
<li class="chapter" data-level="7.2.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#derivation-of-bayesian-parameters-for-spatiotemporal-smoothing"><i class="fa fa-check"></i><b>7.2.1</b> Derivation of bayesian parameters for spatiotemporal smoothing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#use-of-bayesian-smoothing-in-sits"><i class="fa fa-check"></i><b>7.3</b> Use of Bayesian smoothing in SITS</a></li>
<li class="chapter" data-level="7.4" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bilateral-smoothing"><i class="fa fa-check"></i><b>7.4</b> Bilateral smoothing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html"><i class="fa fa-check"></i><b>8</b> Validation and accuracy measurements in SITS</a><ul>
<li class="chapter" data-level="8.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#validation-techniques-1"><i class="fa fa-check"></i><b>8.1</b> Validation techniques</a></li>
<li class="chapter" data-level="8.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#comparing-different-machine-learning-methods-using-k-fold-validation"><i class="fa fa-check"></i><b>8.2</b> Comparing different machine learning methods using k-fold validation</a></li>
<li class="chapter" data-level="8.3" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#accuracy-assessment"><i class="fa fa-check"></i><b>8.3</b> Accuracy assessment</a><ul>
<li class="chapter" data-level="8.3.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#time-series"><i class="fa fa-check"></i><b>8.3.1</b> Time series</a></li>
<li class="chapter" data-level="8.3.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#classified-images"><i class="fa fa-check"></i><b>8.3.2</b> Classified images</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>sits</strong>: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-for-data-cubes-using-the-sits-package" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Machine Learning for Data Cubes using the SITS package</h1>
<!-- [![Example Available](https://img.shields.io/badge/example-available-green)](https://www.kaggle.com/brazildatacube/sits-classification-r) -->
<hr />
<p>This chapter presents the machine learning techniques available in SITS. The main use for machine learning in SITS is for classification of land use and land cover. These machine learning methods available in SITS include linear and quadratic discrimination analysis, support vector machines, random forests, deep learning and neural networks.</p>
<hr />
<div id="machine-learning-classification" class="section level2">
<h2><span class="header-section-number">5.1</span> Machine learning classification</h2>
<p><a href="https://www.kaggle.com/brazildatacube/sits-classification-r" target="_blank"><img src="https://img.shields.io/badge/example-available-green"/></a></p>
<p>There has been much recent interest in using classifiers such as support vector machines <span class="citation">(Mountrakis, Im, and Ogole 2011)</span> and random forests <span class="citation">(Belgiu and Dragut 2016)</span> for remote sensing images. Most often, researchers use a  approach. The dimension of the decision space is limited to the number of spectral bands or their transformations. Sometimes, the decision space is extended with temporal attributes. To do this, researchers filter the raw data to get smoother time series <span class="citation">(Brown et al. 2013; Kastens et al. 2017)</span>. Using software such as TIMESAT <span class="citation">(Jonsson and Eklundh 2004)</span>, they derive a small set of phenological parameters from vegetation indexes, like the beginning, peak, and length of the growing season <span class="citation">(Estel et al. 2015; Pelletier et al. 2016)</span>.</p>
<p>Most studies using satellite image time series for land cover classification use a  approach. For multiyear studies, most researchers first derive best-fit yearly composites and then classify each composite image <span class="citation">(Gomez, White, and Wulder 2016)</span>. As an alternative, the <em>sits</em> package provides support for the classification of time series, preserving the full temporal resolution of the input data, using a  approach. The idea is to have as many temporal attributes as possible, increasing the classification space’s dimension. Each temporal instance of a time series is taken as an independent dimension in the classifier’s feature space. To the authors’ best knowledge, the classification techniques for image time series included in the package are not previously available in other R or python packages. Furthermore, the package provides filtering, clustering, and post-processing methods that have not been published in the literature.</p>
<p>Current approaches to image time series analysis still use a limited number of attributes. A common approach is deriving a small set of phenological parameters from vegetation indices, like the beginning, peak, and length of growing season <span class="citation">(Brown et al. 2013)</span>, <span class="citation">(Kastens et al. 2017)</span>, <span class="citation">(Estel et al. 2015)</span>, <span class="citation">(Pelletier et al. 2016)</span>. These phenological parameters are then fed in specialized classifiers such as TIMESAT <span class="citation">(Jonsson and Eklundh 2004)</span>. These approaches do not use the power of advanced statistical learning techniques to work on high-dimensional spaces with big training data sets <span class="citation">(James et al. 2013)</span>. Package *sits** uses the full depth of satellite image time series to create larger dimensional spaces, an approach we consider to be more appropriate to use with machine learning.</p>
<p><code>sits</code> has support for a variety of machine learning techniques: linear discriminant analysis, quadratic discriminant analysis, multinomial logistic regression, random forests, boosting, support vector machines, and deep learning. The deep learning methods include multi-layer perceptrons, 1D convolution neural networks and mixed approaches such as TempCNN <span class="citation">(Pelletier, Webb, and Petitjean 2019)</span> . In a recent review of machine learning methods to classify remote sensing data <span class="citation">(Maxwell, Warner, and Fang 2018)</span>, the authors note that many factors influence the performance of these classifiers, including the size and quality of the training dataset, the dimension of the feature space, and the choice of the parameters. We support both  and  approaches. Therefore, the <code>sits</code> package provides functionality to explore the full depth of satellite image time series data.</p>
<p>When used in  approach, <code>sits</code> treats time series as a feature vector. To be consistent, the procedure aligns all time series from different years by its time proximity considering an given cropping schedule. Once aligned, the feature vector is formed by all pixel “bands”. The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. In this scenario, statistical learning models are the natural candidates to deal with high-dimensional data: learning to distinguish all land cover and land use classes from trusted samples exemplars (the training data) to infer classes of a larger data set.</p>
<p>SITS provides support for the classification of both individual time series as well as data cubes. The following machine learning methods are available in SITS:</p>
<ul>
<li>Linear discriminant analysis (<code>sits_lda</code>)</li>
<li>Quadratic discriminant analysis (<code>sits_qda</code>)</li>
<li>Multinomial logit and its variants ‘lasso’ and ‘ridge’ (<code>sits_mlr</code>)</li>
<li>Support vector machines (<code>sits_svm</code>)</li>
<li>Random forests (<code>sits_rfor</code>)</li>
<li>Extreme gradient boosting (<code>sits_xgboost</code>)</li>
<li>Deep learning (DL) using multi-layer perceptrons (<code>sits_deeplearning</code>)</li>
<li>DL with 1D convolutional neural networks (<code>sits_FCN</code>)</li>
<li>DL using Deep Residual Networks (<code>sits_ResNet</code>)</li>
<li>DL combining 1D CNN and multi-layer perceptron networks (<code>sits_TempCNN</code>)</li>
<li>DL using a combination of long-short term memory (LSTM) and 1D CNN (<code>sits_LSTM-FCN</code>)</li>
</ul>
<p>For the machine learning examples, we use the data set “samples_matogrosso_mod13q1”, containing a sits tibble with time series samples from Brazilian Mato Grosso State (Amazon and Cerrado biomes), obtained from the MODIS MOD13Q1 product. The tibble with 1,892 samples and 9 classes (“Cerrado”, “Fallow_Cotton”, “Forest”, “Millet_Cotton”, “Pasture”,“Soy_Corn”, “Soy_Cotton”, “Soy_Fallow”, “Soy_Millet”). Each time series comprehends 12 months (23 data points) with 6 bands (“NDVI”, “EVI”, “BLUE”, “RED”, “NIR”, “MIR”) . The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” <span class="citation">(Picoli et al. 2018)</span>, and is available in the R package “sitsdata”, which is downloadable from the website associated to the “e-sensing” project.</p>
</div>
<div id="visualizing-samples" class="section level2">
<h2><span class="header-section-number">5.2</span> Visualizing Samples</h2>
<p>One useful way of describing and understanding the samples is by plotting them. A direct way of doing so is using the <code>plot</code> function, as discussed in <a href="https://e-sensing.github.io/sitsbook/acessing-time-series-information-in-sits.html">Chapter 3</a>. In the plot, the thick red line is the median value for each time instance and the yellow lines are the first and third interquartile ranges.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-1"></a><span class="kw">data</span>(<span class="st">&quot;samples_matogrosso_mod13q1&quot;</span>)</span>
<span id="cb53-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-2"></a><span class="co"># Select a subset of the samples to be plotted</span></span>
<span id="cb53-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-3"></a><span class="co"># Retrieve the set of samples for the Mato Grosso region </span></span>
<span id="cb53-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-4"></a>samples_matogrosso_mod13q1 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb53-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-5"></a><span class="st">    </span><span class="kw">sits_select</span>(<span class="dt">bands =</span> <span class="st">&quot;NDVI&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb53-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-6"></a><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">filter</span>(label <span class="op">==</span><span class="st"> &quot;Forest&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb53-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb53-7"></a><span class="st">    </span><span class="kw">plot</span>()</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-46-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>An alternative to visualise the samples is to estimate a statistical approximation to an idealized pattern based on a generalised additive model (GAM). A GAM is a linear model in which the linear predictor depends linearly on a smooth function of the predictor variables
<span class="math display">\[
y = \beta_{i} + f(x) + \epsilon, \epsilon \sim N(0, \sigma^2).
\]</span>
The function <code>sits_patterns</code> uses a GAM to predict a smooth, idealized approximation to the time series associated to the each label, for all bands. This function is based on the R package <code>dtwSat</code><span class="citation">(Maus et al. 2019)</span>, which implements the TWDTW time series matching method described in <span class="citation">Maus et al. (2016)</span>. The resulting patterns can be viewed using <code>plot</code>.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb54-1"></a><span class="co"># Select a subset of the samples to be plotted</span></span>
<span id="cb54-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb54-2"></a>samples_matogrosso_mod13q1 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb54-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb54-3"></a><span class="st">    </span><span class="kw">sits_patterns</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb54-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb54-4"></a><span class="st">    </span><span class="kw">plot</span>()</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-47-1.png" width="70%" style="display: block; margin: auto;" />
The resulting plots provide some insights over the time series behaviour of each class. While the response of the “Forest” class is quite distinctive, there are similarities between the double-cropping classes (“Soy-Corn”, “Soy-Millet”, “Soy-Sunflower” and “Soy-Corn”) and between the “Cerrado” and “Pasture” classes. This could suggest that additional information, more bands, or higher-resolution data could be considered to provide a better basis for time series samples that can better distinguish the intended classes. Despite these limitations, the best machine learning algorithms can provide good performance even in the above case.</p>
</div>
<div id="common-interface-to-machine-learning-and-deeplearning-models" class="section level2">
<h2><span class="header-section-number">5.3</span> Common interface to machine learning and deeplearning models</h2>
<p>The SITS package provides a common interface to all machine learning models, using the <code>sits_train</code> function. this function takes two parameters: the input data samples and the ML method (<code>ml_method</code>), as shown below. After the model is estimated, it can be used to classify individual time series or full data cubes using the <code>sits_classify</code> function. In the examples that follow, we show how to apply each method for the classification of a single time series. Then, in <a href="https://e-sensing.github.io/sitsbook/classification-of-images-in-data-cubes-using-satellite-image-time-series.html">Chapter 6</a> we disscuss how to classify data cubes.</p>
<p>When a dataset of time series organised as a SITS tibble is taken as input to the classifier, the result is the same tibble with one additional column (“predicted”), which contains the information on what labels are have been assigned for each interval. The results can be shown in text format using the function <code>sits_show_prediction</code> or graphically using <code>plot</code>.</p>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">5.4</span> Random forests</h2>
<p>The Random forest uses the idea of <em>decision trees</em> as its base model. It combines many decision trees via <em>bootstrap</em> procedure and <em>stochastic feature selection</em>, developing a population of somewhat uncorrelated base models. The final classification model is obtained by a majority voting schema. This procedure decreases the classification variance, improving prediction of individual decision trees.</p>
<p>Random forest training process is essentially nondeterministic. It starts by growing trees through repeatedly random sampling-with-replacement the observations set. At each growing tree, the random forest considers only a fraction of the original attributes to decide where to split a node, according to a <em>purity criterion</em>. This criterion is used to identify relevant features and to perform variable selection. This decreases the correlation among trees and improves the prediction performance. Two often-used impurity criteria are the <em>Gini</em> index and the <em>permutation</em> measure. The Gini index considers the contribution of each variable which improves the spliting criteria for building tress. Permutation increases the importance of variables that have a positive effect on the prediction accuracy. The splitting process continues until the tree reaches some given minimum nodes size or a minimum impurity index value.</p>
<p>One of the advantages of the random forest model is that the classification performance is mostly dependent on the number of decision trees to grow and of the “importance” parameter, which controls the purity variable importance measures. SITS provides a <code>sits_rfor</code> function which is a front-end to the <code>randomForest</code> package<span class="citation">(Wright et al. 2017)</span>; its main parameters is <code>num_trees</code> (number of trees to grow).</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-1"></a><span class="co"># Retrieve the set of samples (provided by EMBRAPA) from the </span></span>
<span id="cb55-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-2"></a><span class="co"># Mato Grosso region for train the Random Forest model.</span></span>
<span id="cb55-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-3"></a>rfor_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(<span class="dt">data =</span> samples_matogrosso_mod13q1, </span>
<span id="cb55-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-4"></a>                         <span class="dt">ml_method =</span> <span class="kw">sits_rfor</span>(<span class="dt">num_trees =</span> <span class="dv">1000</span>))</span>
<span id="cb55-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-5"></a><span class="co"># retrieve a point to be classified</span></span>
<span id="cb55-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-6"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb55-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-7"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb55-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-8"></a><span class="co"># Classify using Random Forest model and plot the result</span></span>
<span id="cb55-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-9"></a>point_class &lt;-<span class="st"> </span><span class="kw">sits_classify</span>(point_mt_4bands, rfor_model)</span>
<span id="cb55-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb55-10"></a><span class="kw">plot</span>(point_class, <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/eval-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb56-1"></a><span class="co"># show the results of the prediction</span></span>
<span id="cb56-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb56-2"></a><span class="kw">sits_show_prediction</span>(point_class)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt; # A tibble: 17 x 3
#&gt;    from       to         class   
#&gt;    &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   
#&gt;  1 2000-09-13 2001-08-29 Forest  
#&gt;  2 2001-09-14 2002-08-29 Forest  
#&gt;  3 2002-09-14 2003-08-29 Forest  
#&gt;  4 2003-09-14 2004-08-28 Pasture 
#&gt;  5 2004-09-13 2005-08-29 Pasture 
#&gt;  6 2005-09-14 2006-08-29 Pasture 
#&gt;  7 2006-09-14 2007-08-29 Pasture 
#&gt;  8 2007-09-14 2008-08-28 Pasture 
#&gt;  9 2008-09-13 2009-08-29 Pasture 
#&gt; 10 2009-09-14 2010-08-29 Soy_Corn
#&gt; 11 2010-09-14 2011-08-29 Soy_Corn
#&gt; 12 2011-09-14 2012-08-28 Soy_Corn
#&gt; 13 2012-09-13 2013-08-29 Soy_Corn
#&gt; 14 2013-09-14 2014-08-29 Soy_Corn
#&gt; 15 2014-09-14 2015-08-29 Soy_Corn
#&gt; 16 2015-09-14 2016-08-28 Soy_Corn
#&gt; 17 2016-09-13 2017-08-29 Soy_Corn</code></pre>
<p>Random forest classifier are robust to outliers and able to deal with irrelevant inputs <span class="citation">(Hastie, Tibshirani, and J 2009)</span>. However, despite being robust, random forest tend to overemphasize some variables and thus rarely turn out to be the classifier with the smallest error. One reason is that the performance of random forest tends to stabilise after a part of the trees are grown <span class="citation">(Hastie, Tibshirani, and J 2009)</span>. Random forest classifiers can be quite useful to provide a baseline to compare with more sophisticated methods.</p>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">5.5</span> Support Vector Machines</h2>
<p>Given a multidimensional data set, the Support Vector Machine (SVM) method finds an optimal separation hyperplane that minimizes misclassifications <span class="citation">(Cortes and Vapnik 1995)</span>. Hyperplanes are linear <span class="math inline">\({(p-1)}\)</span>-dimensional boundaries and define linear partitions in the feature space. The solution for the hyperplane coefficients depends only on those samples that violates the maximum margin criteria, the so-called <em>support vectors</em>. All other points far away from the hyperplane does not exert any influence on the hyperplane coefficients which let SVM less sensitive to outliers.</p>
<p>For data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. In this manner, the new classification model, despite having a linear boundary on the enlarged feature space, generally translates its hyperplane to a nonlinear boundaries in the original attribute space. The use of kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space an hence can improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been widely applied to classify remote sensing data <span class="citation">(Mountrakis, Im, and Ogole 2011)</span>.</p>
<p>In <code>sits</code>, SVM is implemented as a wrapper of <code>e1071</code> R package that uses the <code>LIBSVM</code> implementation <span class="citation">(Chang and Lin 2011)</span>, <code>sits</code> adopts the <em>one-against-one</em> method for multiclass classification. For a <span class="math inline">\(q\)</span> class problem, this method creates <span class="math inline">\({q(q-1)/2}\)</span> SVM binary models, one for each class pair combination and tests any unknown input vectors throughout all those models. The overall result is computed by a voting scheme. Considering that SVM is not a robust to outliers as Random Forest, we apply a whittaker filter to smoothen the data by a small factor.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-1"></a><span class="co"># Filter the data slightly to reduce noise without reducing variability</span></span>
<span id="cb58-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-2"></a>samples_filtered &lt;-<span class="st"> </span><span class="kw">sits_whittaker</span>(samples_matogrosso_mod13q1, <span class="dt">lambda =</span> <span class="fl">0.5</span>,</span>
<span id="cb58-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-3"></a>                                   <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb58-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-4"></a><span class="co"># Train a machine learning model for the mato grosso dataset using SVM</span></span>
<span id="cb58-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-5"></a><span class="co"># The parameters are those of the &quot;e1071:svm&quot; method</span></span>
<span id="cb58-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-6"></a>svm_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb58-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-7"></a>                        <span class="dt">ml_method =</span> <span class="kw">sits_svm</span>(<span class="dt">kernel =</span> <span class="st">&quot;radial&quot;</span>,</span>
<span id="cb58-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-8"></a>                                             <span class="dt">cost =</span> <span class="dv">100</span>))</span>
<span id="cb58-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-9"></a><span class="co"># Classify using SVM model and plot the result</span></span>
<span id="cb58-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-10"></a>class &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb58-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-11"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb58-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-12"></a><span class="st">    </span><span class="kw">sits_classify</span>(svm_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb58-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb58-13"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-49-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The result is mostly consistent of what one could expect by visualising the time series. The area started out as a forest in 2000, it was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2008 onwards. However, the result shows some inconsistencies. First, since the training dataset does not contain a samples of deforested areas, places where forest is removed will tend to be classified as “Cerrado”, which is the nearest kind of vegetation cover where trees and grasslands are mixed. This misinterpretation needs to be corrected in post-processing by applying a time-dependent rule (see the main SITS vignette and the post-processing methods vignette). Also, the classification for year 2009 is “Soy-Millet”, which is different from the “Soy-Corn” label assigned from the other years from 2008 to 2017. To test if this result is inconsistent, one could apply spatial post-processing techniques, as discussed in the main SITS vignette and in the post-processing one.</p>
<p>One of the drawbacks of using the <code>sits_svm</code> method is its sensitivity to its parameters. Using a linear or a polynomial kernel fails to produce good results. If one varies the parameter <code>cost</code> (cost of contraints violation) from 100 to 1, the results can be strinkgly different. Such sensitity to the input parameters points to a limitation when using the SVM method for classifying time series.</p>
</div>
<div id="extreme-gradient-boosting" class="section level2">
<h2><span class="header-section-number">5.6</span> Extreme Gradient Boosting</h2>
<p>Boosting techniques are based on the idea of starting from a weak predictor and then improving performance sequentially by fitting better model at each iteration. It starts by fitting a simple classifier to the training data. Then it uses the residuals of the regression to build a better prediction. Typically, the base classifier is a regression tree. Although both random forests and boosting use trees for classification, there is an important difference. In the random forest classifier, the same random logic for tree selections is applied at every step. Boosting trees are built to improve on previous result, by applying finer divisions that improve the performance <span class="citation">(Hastie, Tibshirani, and J 2009)</span>. The performance of random forests generally increases with the number of trees until it becomes stable; however, the number of trees grown by boosting techniques cannot be too large, at the risk of overfitting the model.</p>
<p>Gradient boosting is a variant of boosting methods where the cost function is minimized by agradient descent algorithm. Extreme gradient boosting <span class="citation">(<span class="citeproc-not-found" data-reference-id="Chen2016"><strong>???</strong></span>)</span>, called “XGBoost”, improves by using an efficient approximation to the gradient loss function. The algorithm is fast and accurate. XGBoost is considered one of the best statistical learning algorithms available and has won many competitions; it is generally considered to be better than SVM and random forests. However, actual performance is controlled by the quality of the training dataset.</p>
<p>In SITS, the XGBoost method is implemented by the <code>sits_xbgoost()</code> function, which is based on “XGBoost” R package and has five parameters that require tuning. The learning rate <code>eta</code> varies from 0 to 1, but should be kept small (default is 0.3) to avoid overfitting. The minimim loss value <code>gamma</code> specifies the minimum reduction required to make a split. Its default is 0, but increasing it makes the algorithm more conservative. The maximum depth of a tree <code>max_depth</code> controls how deep tress are to be built. In principle, it should not be largem since higher depth trees lead to overfitting (default is 6.0). The <code>subsample</code> parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The <code>nrounds</code> parameters controls the maximum number of boosting interactions; its default is 100, which has proven to be sufficient in the SITS. In order to follow the convergence of the algorithm, users can turn the <code>verbose</code> parameter on.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-1"></a><span class="co"># Train a machine learning model for the mato grosso dataset using XGBOOST</span></span>
<span id="cb59-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-2"></a><span class="co"># The parameters are those of the &quot;xgboost&quot; package</span></span>
<span id="cb59-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-3"></a>xgb_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_filtered, <span class="kw">sits_xgboost</span>(<span class="dt">verbose =</span> <span class="dv">0</span>))</span>
<span id="cb59-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-4"></a><span class="co"># Classify using SVM model and plot the result</span></span>
<span id="cb59-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-5"></a>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb59-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-6"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.50</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb59-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-7"></a><span class="st">    </span><span class="kw">sits_classify</span>(xgb_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb59-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb59-8"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-50-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In general, the results from the extreme gradient boosting model are similar to the Random Forest model. However, for each specific study, users need to perform validation. See the function <code>sits_kfold_validate</code> for more details.</p>
</div>
<div id="deep-learning-using-multi-layer-perceptrons" class="section level2">
<h2><span class="header-section-number">5.7</span> Deep learning using multi-layer perceptrons</h2>
<p>Using the <code>keras</code> package <span class="citation">(Chollet and Allaire 2018)</span> as a backend, SITS supports the following =deep learning techniques, as described in this section and the next ones. The first method is that of feedforward neural networks, or multi-layer perceptron (MLPs). These are the quintessential deep learning models. The goal of a multilayer perceptrons is to approximate some function <span class="math inline">\(f\)</span>. For example, for a classifier <span class="math inline">\(y =f(x)\)</span> maps an input <span class="math inline">\(x\)</span> to a category <span class="math inline">\(y\)</span>. A feedforward network defines a mapping <span class="math inline">\(y = f(x;\theta)\)</span> and learns the value of the parameters <span class="math inline">\(\theta\)</span> that result in the best function approximation. These models are called feedforward because information flows through the function being evaluated from <span class="math inline">\(x\)</span>, through the intermediate computations used to define <span class="math inline">\(f\)</span>, and finally to the output <span class="math inline">\(y\)</span>. There are no feedback connections in which outputs of the model are fed back into itself <span class="citation">(Goodfellow, Bengio, and Courville 2016)</span>.</p>
<p>Specifying a MLP requires some work on customization, which requires some amount of trial-and-error by the user, since there is no proven model for classification of satellite image time series. The most important decision is the number of layers in the model. Initial tests indicate that 3 to 5 layers are enough to produce good results. The choice of the number of layers depends on the inherent separability of the data set to be classified. For data sets where the classes have different signatures, a shallow model (with 3 layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. The user should be aware that some models with many hidden layers may take a long time to train and may not be able to converge. The suggestion is to start with 3 layers and test different options of number of neurons per layer, before increasing the number of layers.</p>
<p>Three other important parameters for an MLP are: (a) the activation function; (b) the optimization method; (c) the dropout rate. The activation function the activation function of a node defines the output of that node given an input or set of inputs. Following standard practices <span class="citation">(Goodfellow, Bengio, and Courville 2016)</span>, we recommend the use of the “relu” and “elu” functions. The optimization method is a crucial choice, and the most common choices are gradient descent algorithm. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function <span class="citation">(Ruder 2016)</span>. Based on experience with image time series, we recommend that users start by using the default method provided by <code>sits</code>, which is the <code>optimizer_adam</code> method. Please refer to the <code>keras</code> package documentation for more information.</p>
<p>The dropout rates have a huge impact on the performance of MLP classifiers. Dropout is a technique for randomly dropping units from the neural network during training <span class="citation">(Srivastava et al. 2014)</span>. By randomly discarding some neurons, dropout reduces overfitting. It is a counter-intuitive idea that works well. Since the purpose of a cascade of neural nets is to improve learning as more data is acquired, discarding some of these neurons may seem a waste of resources. In fact, as experience has shown <span class="citation">(Goodfellow, Bengio, and Courville 2016)</span>, this procedures prevents an early convergence of the optimization to a local minimum. We suggest that users experiment with different dropout rates.</p>
<p>In the following example, we classify the same data set using an example of the <code>deep learning</code> method. The parameters for the MLP have been chosen based on the proposal by <span class="citation">(Wang, Yan, and Oates 2017)</span> to take multilayer perceptrons as the baseline for time series classifications : (a) Four layers with 512 neurons each, specified by the parameter <code>layers</code>; (b) Using the ‘elu’ activation function; (c) dropout rates of 10%, 20%, 20%, and 30% for the layers; (d) the “optimizer_adam” as optimizer (default value); (e) a number of training steps (<code>epochs</code>) of 150; (f) a <code>batch_size</code> of 64, which indicates how many time series are used for input at a given steps; (g) a validation percentage of 20%, which means 20% of the samples will be randomly set side for validation. In practice, users may want to increase the number of epochs and the number of layers. In our experience, if the training dataset is of good quality, using 3 to 5 layers is a reasonable compromise. Further increase on the number of layers will not improve the model. To simplify the vignette generation, the <code>verbose</code> option has been turned off. The default value is on. After the model has been generated, we plot its training history.</p>
<p>In this and in the following examples of using deep learning classifiers, both the training samples and the point to be classified are filtered with <code>sits_whittaker</code> with a small smoothing parameter (lambda = 0.5). In our experiments, we found that deep learning classifiers are not as robust to noise as Random Forest or XGBoost. Thus, the right amount of smoothing appears to improve their detection power.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-1"></a><span class="co"># train a machine learning model for the Mato Grosso data using an MLP model</span></span>
<span id="cb60-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-2"></a></span>
<span id="cb60-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-3"></a>mlp_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_filtered, </span>
<span id="cb60-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-4"></a>                        <span class="kw">sits_deeplearning</span>(</span>
<span id="cb60-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-5"></a>                        <span class="dt">layers           =</span> <span class="kw">c</span>(<span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb60-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-6"></a>                        <span class="dt">activation       =</span> <span class="st">&quot;elu&quot;</span>,</span>
<span id="cb60-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-7"></a>                        <span class="dt">dropout_rates    =</span> <span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.20</span>, <span class="fl">0.25</span>, <span class="fl">0.30</span>, <span class="fl">0.35</span>, <span class="fl">0.40</span>),</span>
<span id="cb60-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-8"></a>                        <span class="dt">epochs           =</span> <span class="dv">100</span>,</span>
<span id="cb60-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-9"></a>                        <span class="dt">batch_size       =</span> <span class="dv">64</span>,</span>
<span id="cb60-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-10"></a>                        <span class="dt">verbose          =</span> <span class="dv">0</span>,</span>
<span id="cb60-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-11"></a>                        <span class="dt">validation_split =</span> <span class="fl">0.1</span>) )</span>
<span id="cb60-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-12"></a></span>
<span id="cb60-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-13"></a><span class="co"># show training evolution</span></span>
<span id="cb60-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb60-14"></a><span class="kw">plot</span>(mlp_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb61-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb61-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb61-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-4"></a>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb61-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-5"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb61-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-6"></a><span class="st">    </span><span class="kw">sits_classify</span>(mlp_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb61-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb61-7"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-52-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Compared to the Random Forest and XGBoost models, the deep learning model captures more subtle changes. For example, the transition from Forest to Pasture as estimated by the model is not abrupt, but takes more than one year. In the year 2004, the time series corresponds to a degraded forest. Since there are no samples for “Forest Degradation”, the model assigns this series to class that is neither “Forest” nor “Pasture”. This indicates that users should include samples of “Forest Degradation” to improve classification. Moreover, while the RandomForest and XGBoost models consider that the agricultural production in the area started only in 2010, the MLP model indicates an earlier starting date of 2008. Although the model mixes the “Soy_Corn” and “Soy_Millet”, the distinction between the classes is quite subtle, and thus indicates the need to improve the number of samples. Thus, in a first and coarse appreciation, the MLP model shows an increased sensitivity to the data variation than the previous models.</p>
</div>
<div id="combined-1d-cnn-and-multi-layer-perceptron-networks" class="section level2">
<h2><span class="header-section-number">5.8</span> Combined 1D CNN and multi-layer perceptron networks</h2>
<p>Convolutional neural networks (CNN) are a variety of deep learning methods where a convolution filter (sliding window) is applied to the input data. In the case of time series, a 1D CNN works by applying a moving window to the series. Using convolution filters is a way to incorporate temporal autocorrelation information in the classification. The result of the convolution is another time series. <span class="citation">Rußwurm and Korner (2017)</span> states that the use of 1D-CNN for time series classification improves on the use of multi-layer perceptrons, since the classifier is able to represent temporal relationships. Also, 1D-CNNs with a suitable convolution window make the classifier more robust to moderate noise, e.g. intermittent presence of clouds.</p>
<p>The combination of 1D CNNs and multi-layer perceptron models for satellite image time series classification is proposed in <span class="citation">Pelletier, Webb, and Petitjean (2019)</span>. The so-called “tempCNN” architecture consists of a number of 1D-CNN layers, similar to the fullCNN model discussed above, whose output is fed into a set of multi-layer perceptrons. The original tempCNN architecture is composed of three 1D convolutional layers (each with 64 units), one dense layer of 256 units and a final softmax layer for classification (see figure). The kernel size of the convolution filters is set to 5. The authors use a combination of different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalisation. In the tempCNN paper <span class="citation">(Pelletier, Webb, and Petitjean 2019)</span>, the authors compare favourably the tempCNN model with the Recurrent Neural Network proposed by <span class="citation">Russwurm and Korner (2018)</span> for land use classification. The figure below shows the architecture of the tempCNN model.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-53"></span>
<img src="images/tempcnn.png" alt="Structure of tempCNN architecture (source: Pelletier et al.(2019))" width="80%" height="80%" />
<p class="caption">
Figure 5.1: Structure of tempCNN architecture (source: Pelletier et al.(2019))
</p>
</div>
<p>The function <code>sits_tempCNN</code> implements the model. The code has been derived from the Python source provided by the authors (<a href="https://github.com/charlotte-pel/temporalCNN" class="uri">https://github.com/charlotte-pel/temporalCNN</a>). The parameter <code>cnn_layers</code> controls the number of 1D-CNN layers and the size of the filters applied at each layer; the parameter <code>cnn_kernels</code> indicates the size of the convolution kernels. Activation, regularisation for all 1D-CNN layers are set, respectively, by the <code>cnn_activation</code>, <code>cnn_L2_rate</code>. The dropout rates for each 1D-CNN layer are controlled individually by the parameter <code>cnn_dropout_rates</code>. The parameters <code>mlp_layers</code> and <code>mlp_dropout_rates</code> allow the user to set the number and size of the desired MLP layers, as well as their dropout_rates. The activation of the MLP layers is controlled by <code>mlp_activation</code>. By default, the function uses the ADAM optimizer, but any of the optimizers available in the <code>keras</code> package can be used. The <code>validation_split</code> controls the size of the test set, relative to the full data set. We recommend to set aside at least 20% of the samples for validation. Based on our experiments, in the example below we propose a different setup of parameters than <span class="citation">Pelletier, Webb, and Petitjean (2019)</span>. The parameters are: (a) filters of size 3, which aim for a local convolution that reduces noise but do not decrease natural variability; (b) smaller dropout rates ranging from 10% to 35%; (d) an increased number of perceptron layers of size 512, also with smaller dropout rates.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-1"></a><span class="co"># train a machine learning model using tempCNN</span></span>
<span id="cb62-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-2"></a>tCNN_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_filtered, </span>
<span id="cb62-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-3"></a>                       <span class="kw">sits_TempCNN</span>(</span>
<span id="cb62-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-4"></a>                          <span class="dt">cnn_layers           =</span> <span class="kw">c</span>(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">64</span>),</span>
<span id="cb62-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-5"></a>                          <span class="dt">cnn_kernels          =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb62-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-6"></a>                          <span class="dt">cnn_activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb62-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-7"></a>                          <span class="dt">cnn_L2_rate          =</span> <span class="fl">1e-06</span>,</span>
<span id="cb62-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-8"></a>                          <span class="dt">cnn_dropout_rates    =</span> <span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.50</span>, <span class="fl">0.50</span>),</span>
<span id="cb62-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-9"></a>                          <span class="dt">mlp_layers           =</span> <span class="kw">c</span>(<span class="dv">256</span>),</span>
<span id="cb62-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-10"></a>                          <span class="dt">mlp_activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb62-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-11"></a>                          <span class="dt">mlp_dropout_rates    =</span> <span class="kw">c</span>(<span class="fl">0.50</span>),</span>
<span id="cb62-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-12"></a>                          <span class="dt">epochs               =</span> <span class="dv">100</span>,</span>
<span id="cb62-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-13"></a>                          <span class="dt">batch_size           =</span> <span class="dv">64</span>,</span>
<span id="cb62-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-14"></a>                          <span class="dt">validation_split     =</span> <span class="fl">0.2</span>,</span>
<span id="cb62-15"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-15"></a>                          <span class="dt">verbose              =</span> <span class="dv">0</span>) )</span>
<span id="cb62-16"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-16"></a></span>
<span id="cb62-17"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-17"></a><span class="co"># show training evolution</span></span>
<span id="cb62-18"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb62-18"></a><span class="kw">plot</span>(tCNN_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the TempCNN model</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb63-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb63-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb63-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-4"></a>class &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb63-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-5"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb63-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-6"></a><span class="st">    </span><span class="kw">sits_classify</span>(tCNN_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb63-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-7"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-55-1.png" width="70%" style="display: block; margin: auto;" />
While the result of the TempCNN model using default parameters is similar to that of the MLP model, it has has the potential to better explore the time series data than the the MLP model. Given that it has more parameters, it requires more effort to calibrate them. Users interested in working with this models are encouraged to compare different settings to define what is the best configuration for their problems.</p>
</div>
<div id="d-convolutional-neural-networks" class="section level2">
<h2><span class="header-section-number">5.9</span> 1D Convolutional Neural Networks</h2>
<p>Convolutional neural networks (CNN) are a variety of deep learning methods where a convolution filter (sliding window) is applied to the input data. In the case of time series, a 1D CNN works by applying a moving window to the series. Using convolution filters is a way to incorporate temporal autocorrelation information in the classification. The result of the convolution is another time series. <span class="citation">Rußwurm and Korner (2017)</span> states that the use of 1D-CNN for time series classification improves on the use of multi-layer perceptrons, since the classifier is able to represent temporal relationships. Also, 1D-CNNs with a suitable convolution window make the classifier more robust to moderate noise, e.g. intermittent presence of clouds.</p>
<p>SITS includes four different variations of 1D-CNN, described in what follows. The first one is a “full Convolutional Neural Network”<span class="citation">(Wang, Yan, and Oates 2017)</span>, implemented in the <code>sits_FCN</code> function. FullCNNs are cascading networks, where the size of the input data is kept constant during the convolution. After the nvolutions have been applied, the model includes a global average pooling layer which reduces the number of parameters, and highlights which parts of the input time series contribute the most to the classification <span class="citation">(Fawaz et al. 2019)</span>. The fullCNN architecture proposed in <span class="citation">Wang, Yan, and Oates (2017)</span> has three convolutional layers. Each layer performs a convolution in its input and uses batch normalization for avoiding premature convergence to a local minimum. Batch normalisation is an alternative to dropout <span class="citation">(Ioffe and Szegedy 2015)</span>. The result is to a ReLU activation function. The result of the third convolutional block is averaged over the whole time dimension which corresponds to a global average pooling layer. Finally, a traditional softmax classifier is used to get the classification results (see figure below).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-56"></span>
<img src="images/fullcnn.png" alt="Structure of fullCNN architecture (source: Wang et al.(2017))" width="80%" height="80%" />
<p class="caption">
Figure 5.2: Structure of fullCNN architecture (source: Wang et al.(2017))
</p>
</div>
<p>The <code>sits_FCN</code> function uses the architecture proposed by Wang as its default, and allows the users to experiment with different settings. The <code>layers</code> parameter controls the number of layers and the number of filters in each layer. The <code>kernels</code> parameters controls the size of the convolution kernels for earh layer. In the example below, the first convolution uses 64 filters with a kernel size of 8, followed by a second convolution of 128 filters with a kernel size of 5, and a third and final convolutional layer with 64 filters, each one with a kernel size to 3.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-1"></a><span class="co"># train a machine learning model using deep learning</span></span>
<span id="cb64-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-2"></a>fcn_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_filtered, </span>
<span id="cb64-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-3"></a>                        <span class="kw">sits_FCN</span>(</span>
<span id="cb64-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-4"></a>                            <span class="dt">layers           =</span> <span class="kw">c</span>(<span class="dv">64</span>, <span class="dv">256</span>, <span class="dv">64</span>),</span>
<span id="cb64-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-5"></a>                            <span class="dt">kernels          =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">3</span>),</span>
<span id="cb64-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-6"></a>                            <span class="dt">activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb64-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-7"></a>                            <span class="dt">L2_rate          =</span> <span class="fl">1e-06</span>,</span>
<span id="cb64-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-8"></a>                            <span class="dt">epochs           =</span> <span class="dv">100</span>,</span>
<span id="cb64-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-9"></a>                            <span class="dt">batch_size       =</span> <span class="dv">128</span>,</span>
<span id="cb64-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-10"></a>                            <span class="dt">verbose          =</span> <span class="dv">0</span>) )</span>
<span id="cb64-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-11"></a><span class="co"># show training evolution</span></span>
<span id="cb64-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-12"></a><span class="kw">plot</span>(fcn_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the FCN model</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb65-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb65-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb65-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-5"></a><span class="st">  </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-6"></a><span class="st">    </span><span class="kw">sits_classify</span>(fcn_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-7"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-58-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="residual-1d-cnn-networks-resnet" class="section level2">
<h2><span class="header-section-number">5.10</span> Residual 1D CNN Networks (ResNet)</h2>
<p>The Residual Network (ResNet) is a variation of the fullCNN network proposed by <span class="citation">Wang, Yan, and Oates (2017)</span>. ResNet is composed of 11 layers (see figure below). ResNet is a deep network, by default divided in three blocks of three 1D CNN layers each. Each block corresponds to a fullCNN network archicture. The output of each block is combined with a shortcut that links its output to its input. The idea is avoid the so-called “vanishing gradient”, which occurs when a deep learning network is trained based gradient optimization methods<span class="citation">(Hochreiter 1998)</span>. As the networks get deeper, otimising them becomes more difficult. Including the input layer of the block at its end is a heuristic that has shown to be effective. In a recent review of time series classification methods using deep learning, Fawaz et al. state the RestNet and fullCNN have the best performance on the UCR/UEA time series test archive <span class="citation">(Fawaz et al. 2019)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-59"></span>
<img src="images/resnet.png" alt="Structure of ResNet architecture (source: Wang et al.(2017))" width="80%" height="80%" />
<p class="caption">
Figure 5.3: Structure of ResNet architecture (source: Wang et al.(2017))
</p>
</div>
<p>In SITS, the ResNet is implemented using the <code>sits_resnet</code> function. The default parameters are those proposed by <span class="citation">Wang, Yan, and Oates (2017)</span>, and we also benefited from the code provided by <span class="citation">Fawaz et al. (2019)</span> ( <a href="https://github.com/hfawaz/dl-4-tsc" class="uri">https://github.com/hfawaz/dl-4-tsc</a>). The first parameter is <code>blocks</code>, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128 filters. Users can control the number of blocks and filter size by changing this parameter. The parameter <code>kernels</code> controls the size the of kernels of the three layers inside each block. We have found out that it is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is “relu”, which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is the same as proposed in <span class="citation">Wang, Yan, and Oates (2017)</span> and <span class="citation">Fawaz et al. (2019)</span>.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-1"></a><span class="co"># train a machine learning model using ResNet</span></span>
<span id="cb66-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-2"></a>resnet_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_filtered, </span>
<span id="cb66-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-3"></a>                       <span class="kw">sits_ResNet</span>(</span>
<span id="cb66-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-4"></a>                          <span class="dt">blocks               =</span> <span class="kw">c</span>(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">128</span>),</span>
<span id="cb66-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-5"></a>                          <span class="dt">kernels              =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">3</span>),</span>
<span id="cb66-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-6"></a>                          <span class="dt">activation           =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb66-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-7"></a>                          <span class="dt">epochs               =</span> <span class="dv">150</span>,</span>
<span id="cb66-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-8"></a>                          <span class="dt">batch_size           =</span> <span class="dv">64</span>,</span>
<span id="cb66-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-9"></a>                          <span class="dt">validation_split     =</span> <span class="fl">0.2</span>,</span>
<span id="cb66-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-10"></a>                          <span class="dt">verbose              =</span> <span class="dv">0</span>) )</span>
<span id="cb66-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-11"></a><span class="co"># show training evolution</span></span>
<span id="cb66-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-12"></a><span class="kw">plot</span>(resnet_model)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-60-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Then, we classify a 16-year time series using the ResNet model.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb67-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb67-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb67-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb67-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-5"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb67-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-6"></a><span class="st">    </span><span class="kw">sits_classify</span>(resnet_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb67-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-7"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-61-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In a similar way as other deep learning models, ResNet finds an outlier in year 2004. As explained above, the time series for 2004 is likely to come from a situation where part of the natural forest had been removed, but not all of the area had been replaced by pasture. Since the training set does not include time series for degraded forests, the classifier assigns it to a crop class. Although incorrect, such labeling is useful to point out to missing classes in the training set. Also similarly to the MLP model, the RestNet model considers that the agricultural production in the area started in 2008.</p>
</div>
<div id="lstm-convolutional-networks-for-time-series-classification" class="section level2">
<h2><span class="header-section-number">5.11</span> LSTM Convolutional Networks for Time Series Classification</h2>
<p>Given the success of 1D-CNN networks for time series classification, there have been a number of variants proposed in the literature. One of these variants is the LSTM-CNN network <span class="citation">(Karim et al. 2018)</span>, where a fullCNN is combined with long short term memory (LSTM) recurrent neural network. LSTMs are an improved version of recurrent neural networks (RNN). An RNN is a neural network that includes a state vector, which is updated every time step. In this way, a RNN combines an input vector with information that is kept from all previous inputs. One can conceive of RNN as networks that have loops, allowing information to be passed from one step to to the next. In theory, a RNN would be able to handle long-term dependencies between elements of the input vectors. In practice, they are prone to the exhibit the “vanishing gradient” effect. As discussed above, in deep neural networks architectures with gradient descent optimization the gradient function can approach zero, thus impeding training to be done efficiently. LSTM improve on RNN architecture by including the additional feature of being able to regulate whether or not new information should be included on the cell state. LSTM unit also include a forget gate, which is able to discard the previous information stored in the cell state. Thus, a LSTM unit is able to remember values over arbitrary time intervals.</p>
<p><span class="citation">Karim et al. (2019)</span> consider that LSTM networks are “well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series”. The authors proposed a mixed LSTM-CNN architecture, composed of two paralel data streams: a 3-step CNN such as the one implemented in <code>sits_FCN</code> (see above) combined with a data stream consisting of an LSTM unit, as shown in the figure below. In <span class="citation">Karim et al. (2018)</span>, the authors argue the LSTM-CNN model is capable of a better performance in the UCR/UEA time series test set than architectures such as ResNet and 1DCNN.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-62"></span>
<img src="images/lstm-cnn.png" alt="LSTM Fully Convolutional Networks for Time Series Classification (source: Karim et al.(2019))" width="80%" height="80%" />
<p class="caption">
Figure 5.4: LSTM Fully Convolutional Networks for Time Series Classification (source: Karim et al.(2019))
</p>
</div>
<p>In the SITS package, the combined LSTM-CNN architecture is implemented by the <code>sits_LSTM_CNN</code> function. The default values are similar those proposed by <span class="citation">Karim et al. (2019)</span>. The parameter <code>lstm_units</code> controls the number of units in the LSTM cell at every time step of the network. <span class="citation">Karim et al. (2019)</span> proposes an LSTM with 8 units, each with a dropout rate of 80%, which are controlled by parameters <code>lstm_units</code> and <code>lstm_dropout</code>. In initial experiments, we got a better performance with an LSTM with 16 units. The CNN layers have filter sizes of {128, 256, 128} and kernel convolution sizes of {8, 5, 3}, controlled by the parameters <code>cnn_layers</code> and <code>cnn_kernels</code>. One should experiment with these parameters, and consider the simulations carried out by <span class="citation">Pelletier, Webb, and Petitjean (2019)</span> (see above), where the authors found that an FCN network of sizes {64, 64, 64} with kernels sizes of {5, 5, 5} had best performance in their case study. In this example, the estimated accuracy of the model was 94.7%.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-1"></a>lstm_fcn_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_filtered, </span>
<span id="cb68-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-2"></a>                       <span class="kw">sits_LSTM_FCN</span>(</span>
<span id="cb68-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-3"></a>                          <span class="dt">lstm_units =</span> <span class="dv">8</span>,</span>
<span id="cb68-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-4"></a>                          <span class="dt">lstm_dropout =</span> <span class="fl">0.80</span>,</span>
<span id="cb68-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-5"></a>                          <span class="dt">cnn_layers =</span> <span class="kw">c</span>(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb68-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-6"></a>                          <span class="dt">cnn_kernels =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">3</span>),</span>
<span id="cb68-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-7"></a>                          <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>,</span>
<span id="cb68-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-8"></a>                          <span class="dt">optimizer =</span> keras<span class="op">::</span><span class="kw">optimizer_adam</span>(<span class="dt">lr =</span> <span class="fl">0.001</span>),</span>
<span id="cb68-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-9"></a>                          <span class="dt">epochs =</span> <span class="dv">100</span>,</span>
<span id="cb68-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-10"></a>                          <span class="dt">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb68-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-11"></a>                          <span class="dt">validation_split =</span> <span class="fl">0.2</span>,</span>
<span id="cb68-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-12"></a>                          <span class="dt">verbose     =</span> <span class="dv">0</span>))</span>
<span id="cb68-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-13"></a></span>
<span id="cb68-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-14"></a><span class="co"># show training evolution</span></span>
<span id="cb68-15"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-15"></a><span class="kw">plot</span>(lstm_fcn_model)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-63-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb69-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb69-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb69-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-4"></a>class &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb69-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-5"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.5</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb69-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-6"></a><span class="st">    </span><span class="kw">sits_classify</span>(lstm_fcn_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb69-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-7"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-64-1.png" width="70%" style="display: block; margin: auto;" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="time-series-clustering-to-improve-the-quality-of-training-samples.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["sitsbook.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
