<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</title>
  <meta name="description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series" />
  
  <meta name="twitter:description" content="This book presents sits, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classification image time series obtained from data cubes." />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Rolf Simoes" />
<meta name="author" content="Gilberto Camara" />
<meta name="author" content="Felipe Souza" />
<meta name="author" content="Lorena Santos" />
<meta name="author" content="Pedro R. Andrade" />
<meta name="author" content="Charlotte Peletier" />
<meta name="author" content="Alexandre Carvalho" />
<meta name="author" content="Karine Ferreira" />
<meta name="author" content="Gilberto Queiroz" />
<meta name="author" content="Victor Maus" />


<meta name="date" content="2021-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="time-series-clustering-to-improve-the-quality-of-training-samples.html"/>
<link rel="next" href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SITS Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-this-book-is-for"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#main-reference-for-sits"><i class="fa fa-check"></i>Main reference for sits</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#publications-using-sits"><i class="fa fa-check"></i>Publications using sits</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reproducible-papers-used-in-building-sits-functions"><i class="fa fa-check"></i>Reproducible papers used in building sits functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a>
<ul>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html#docker-images"><i class="fa fa-check"></i>Docker images</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="part"><span><b>I Overview</b></span></li>
<li class="chapter" data-level="1" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html"><i class="fa fa-check"></i><b>1</b> A taste of sits</a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#creating-a-data-cube"><i class="fa fa-check"></i><b>1.1</b> Creating a Data Cube</a></li>
<li class="chapter" data-level="1.2" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#the-time-series-table"><i class="fa fa-check"></i><b>1.2</b> The time series table</a></li>
<li class="chapter" data-level="1.3" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#training-a-machine-learning-model"><i class="fa fa-check"></i><b>1.3</b> Training a machine learning model</a></li>
<li class="chapter" data-level="1.4" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#data-cube-classification"><i class="fa fa-check"></i><b>1.4</b> Data cube classification</a></li>
<li class="chapter" data-level="1.5" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#spatial-smoothing"><i class="fa fa-check"></i><b>1.5</b> Spatial smoothing</a></li>
<li class="chapter" data-level="1.6" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#labelling-a-probability-data-cube"><i class="fa fa-check"></i><b>1.6</b> Labelling a probability data cube</a></li>
<li class="chapter" data-level="1.7" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#how-the-sits-api-works"><i class="fa fa-check"></i><b>1.7</b> How the sits API works</a></li>
<li class="chapter" data-level="1.8" data-path="a-taste-of-sits.html"><a href="a-taste-of-sits.html#final-remarks"><i class="fa fa-check"></i><b>1.8</b> Final remarks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html"><i class="fa fa-check"></i><b>2</b> Earth observation data cubes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="2.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#analysis-ready-data-image-collections"><i class="fa fa-check"></i><b>2.2</b> Analysis-ready data image collections</a></li>
<li class="chapter" data-level="2.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-and-image-collections-in-sits"><i class="fa fa-check"></i><b>2.3</b> Accessing Data Cubes and Image Collections in SITS</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-data-cubes-in-amazon-web-services"><i class="fa fa-check"></i><b>2.3.1</b> Accessing data cubes in Amazon Web Services</a></li>
<li class="chapter" data-level="2.3.2" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#accessing-the-brazil-data-cube"><i class="fa fa-check"></i><b>2.3.2</b> Accessing the Brazil Data Cube</a></li>
<li class="chapter" data-level="2.3.3" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#defining-a-data-cube-using-files"><i class="fa fa-check"></i><b>2.3.3</b> Defining a data cube using files</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="earth-observation-data-cubes.html"><a href="earth-observation-data-cubes.html#regularizing-data-cubes"><i class="fa fa-check"></i><b>2.4</b> Regularizing data cubes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="working-with-time-series.html"><a href="working-with-time-series.html"><i class="fa fa-check"></i><b>3</b> Working with time series</a>
<ul>
<li class="chapter" data-level="3.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#data-structures-for-satellite-time-series"><i class="fa fa-check"></i><b>3.1</b> Data structures for satellite time series</a></li>
<li class="chapter" data-level="3.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#utilities-for-handling-time-series"><i class="fa fa-check"></i><b>3.2</b> Utilities for handling time series</a></li>
<li class="chapter" data-level="3.3" data-path="working-with-time-series.html"><a href="working-with-time-series.html#time-series-visualisation"><i class="fa fa-check"></i><b>3.3</b> Time series visualisation</a></li>
<li class="chapter" data-level="3.4" data-path="working-with-time-series.html"><a href="working-with-time-series.html#obtaining-time-series-data-from-data-cubes"><i class="fa fa-check"></i><b>3.4</b> Obtaining time series data from data cubes</a></li>
<li class="chapter" data-level="3.5" data-path="working-with-time-series.html"><a href="working-with-time-series.html#filtering-techniques-for-time-series"><i class="fa fa-check"></i><b>3.5</b> Filtering techniques for time series</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="working-with-time-series.html"><a href="working-with-time-series.html#savitzkygolay-filter"><i class="fa fa-check"></i><b>3.5.1</b> Savitzky–Golay filter</a></li>
<li class="chapter" data-level="3.5.2" data-path="working-with-time-series.html"><a href="working-with-time-series.html#whittaker-filter"><i class="fa fa-check"></i><b>3.5.2</b> Whittaker filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Clustering</b></span></li>
<li class="chapter" data-level="4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html"><i class="fa fa-check"></i><b>4</b> Time Series Clustering to Improve the Quality of Training Samples</a>
<ul>
<li class="chapter" data-level="4.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#clustering-for-sample-quality-control-overview"><i class="fa fa-check"></i><b>4.1</b> Clustering for sample quality control: overview</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#hierachical-clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.2</b> Hierachical clustering for sample quality control</a></li>
<li class="chapter" data-level="4.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-self-organizing-maps-for-sample-quality-control"><i class="fa fa-check"></i><b>4.3</b> Using self-organizing maps for sample quality control</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#som-based-quality-assessment-part-1-creating-the-som-map"><i class="fa fa-check"></i><b>4.3.1</b> SOM-based quality assessment part 1: creating the SOM map</a></li>
<li class="chapter" data-level="4.3.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#som-based-quality-assessment-part-2-assessing-confusion-between-labels"><i class="fa fa-check"></i><b>4.3.2</b> SOM-based quality assessment part 2: assessing confusion between labels</a></li>
<li class="chapter" data-level="4.3.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#som-based-quality-assessment-part-3-using-probabilities-to-detect-noisy-samples"><i class="fa fa-check"></i><b>4.3.3</b> SOM-based quality assessment part 3: using probabilities to detect noisy samples</a></li>
<li class="chapter" data-level="4.3.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#comparing-original-and-clean-samples"><i class="fa fa-check"></i><b>4.3.4</b> Comparing Original and Clean Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>III Classification</b></span></li>
<li class="chapter" data-level="5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html"><i class="fa fa-check"></i><b>5</b> Machine Learning for Data Cubes using the SITS package</a>
<ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-classification"><i class="fa fa-check"></i><b>5.1</b> Machine learning classification</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#visualizing-samples"><i class="fa fa-check"></i><b>5.2</b> Visualizing Samples</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#common-interface-to-machine-learning-and-deeplearning-models"><i class="fa fa-check"></i><b>5.3</b> Common interface to machine learning and deeplearning models</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#random-forests"><i class="fa fa-check"></i><b>5.4</b> Random forests</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#support-vector-machines"><i class="fa fa-check"></i><b>5.5</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#extreme-gradient-boosting"><i class="fa fa-check"></i><b>5.6</b> Extreme Gradient Boosting</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#deep-learning-using-multi-layer-perceptrons"><i class="fa fa-check"></i><b>5.7</b> Deep learning using multi-layer perceptrons</a></li>
<li class="chapter" data-level="5.8" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#temporal-convolutional-neural-network-tempcnn"><i class="fa fa-check"></i><b>5.8</b> Temporal Convolutional Neural Network (TempCNN)</a></li>
<li class="chapter" data-level="5.9" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#residual-1d-cnn-networks-resnet"><i class="fa fa-check"></i><b>5.9</b> Residual 1D CNN Networks (ResNet)</a></li>
<li class="chapter" data-level="5.10" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#considerations-on-model-choice"><i class="fa fa-check"></i><b>5.10</b> Considerations on model choice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><i class="fa fa-check"></i><b>6</b> Classification of Images in Data Cubes using Satellite Image Time Series</a>
<ul>
<li class="chapter" data-level="6.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#data-cube-classification-1"><i class="fa fa-check"></i><b>6.1</b> Data cube classification</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#processing-time-estimates"><i class="fa fa-check"></i><b>6.2</b> Processing time estimates</a></li>
</ul></li>
<li class="part"><span><b>IV Post classification</b></span></li>
<li class="chapter" data-level="7" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html"><i class="fa fa-check"></i><b>7</b> Post classification smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bayesian-smoothing"><i class="fa fa-check"></i><b>7.2</b> Bayesian smoothing</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#derivation-of-bayesian-parameters-for-spatiotemporal-smoothing"><i class="fa fa-check"></i><b>7.2.1</b> Derivation of bayesian parameters for spatiotemporal smoothing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#use-of-bayesian-smoothing-in-sits"><i class="fa fa-check"></i><b>7.3</b> Use of Bayesian smoothing in SITS</a></li>
<li class="chapter" data-level="7.4" data-path="post-classification-smoothing.html"><a href="post-classification-smoothing.html#bilateral-smoothing"><i class="fa fa-check"></i><b>7.4</b> Bilateral smoothing</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html"><i class="fa fa-check"></i><b>8</b> Validation and accuracy measurements in SITS</a>
<ul>
<li class="chapter" data-level="8.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#validation-techniques"><i class="fa fa-check"></i><b>8.1</b> Validation techniques</a></li>
<li class="chapter" data-level="8.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#comparing-different-machine-learning-methods-using-k-fold-validation"><i class="fa fa-check"></i><b>8.2</b> Comparing different machine learning methods using k-fold validation</a></li>
<li class="chapter" data-level="8.3" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#accuracy-assessment"><i class="fa fa-check"></i><b>8.3</b> Accuracy assessment</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#time-series"><i class="fa fa-check"></i><b>8.3.1</b> Time series</a></li>
<li class="chapter" data-level="8.3.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#classified-images"><i class="fa fa-check"></i><b>8.3.2</b> Classified images</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>9</b> Case studies</a></li>
<li class="chapter" data-level="10" data-path="design-and-extensibility-considerations.html"><a href="design-and-extensibility-considerations.html"><i class="fa fa-check"></i><b>10</b> Design and extensibility considerations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="design-and-extensibility-considerations.html"><a href="design-and-extensibility-considerations.html#design-decisions"><i class="fa fa-check"></i><b>10.1</b> Design decisions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>sits</strong>: Data Analysis and Machine Learning on Earth Observation Data Cubes with Satellite Image Time Series</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-for-data-cubes-using-the-sits-package" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Machine Learning for Data Cubes using the SITS package</h1>
<!-- [![Example Available](https://img.shields.io/badge/example-available-green)](https://www.kaggle.com/brazildatacube/sits-classification-r) -->
<hr />
<p>This chapter presents the machine learning (ML) techniques available in SITS. The main use for machine learning in SITS is for classification of land use and land cover. These machine learning methods available in SITS include linear and quadratic discrimination analysis, support vector machines, random forests, deep learning and neural networks.</p>
<hr />
<div id="machine-learning-classification" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Machine learning classification</h2>
<p><a href="https://www.kaggle.com/brazildatacube/sits-classification-r" target="_blank"><img src="https://img.shields.io/badge/example-available-green"/></a></p>
<p>The package provides support for the classification of time series, preserving the full temporal resolution of the input data. Instead of extracting metrics from time series segments, it uses all values of the time series. It supports two kinds of machine learning methods. The first group of methods does not explicitly consider spatial or temporal dimensions; these models treat time series as a vector in a high-dimensional feature space. From this class of models, <code>sits</code> includes random forests<span class="citation">[24]</span>, support vector machines<span class="citation">[25]</span>, extreme gradient boosting<span class="citation">[26]</span>, and multi-layer perceptrons<span class="citation">[27]</span>.</p>
<p>The second group of models comprises deep learning methods specifically designed to work with time series. Temporal relations between observed values in a time series are taken into account. Time series classification models for satellite data include 1D convolution neural networks (1D-CNN) <span class="citation">[28]</span>, recurrent neural networks (RNN)<span class="citation">[29]</span>, and~attention-based deep learning <span class="citation">[31]</span>. The <code>sits</code> package supports two 1D-CNN algorithms: TempCNN<span class="citation">[2]</span> and ResNet<span class="citation">[32]</span>. For models based on 1D-CNN, the order of the samples in the time series is relevant for the classifier. Each layer of the network applies a convolution filter to the output of the previous layer. This cascade of convolutions captures time series features in different time scales <span class="citation">[2]</span>.</p>
<p>Thus, the following machine learning methods are available in SITS:</p>
<ul>
<li>Support vector machines (<code>sits_svm()</code>)</li>
<li>Random forests (<code>sits_rfor()</code>)</li>
<li>Extreme gradient boosting (<code>sits_xgboost()</code>)</li>
<li>Deep learning (DL) using multi-layer perceptrons (<code>sits_mlp()</code>)</li>
<li>DL using Deep Residual Networks (<code>sits_ResNet()</code>)</li>
<li>DL with 1D convolutional neural networks (<code>sits_TempCNN()</code>)</li>
</ul>
<p>For the machine learning examples, we use the data set “samples_matogrosso_mod13q1,” containing a sits tibble with time series samples from Brazilian Mato Grosso State (Amazon and Cerrado biomes), obtained from the MODIS MOD13Q1 product. The tibble with 1,892 samples and 9 classes (“Cerrado,” “Fallow_Cotton,” “Forest,” “Millet_Cotton,” “Pasture,”“Soy_Corn,” “Soy_Cotton,” “Soy_Fallow,” “Soy_Millet”). Each time series comprehends 12 months (23 data points) with 6 bands (“NDVI,” “EVI,” “BLUE,” “RED,” “NIR,” “MIR”). The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” <span class="citation">[33]</span>, and is available in the R package “sitsdata,” which is downloadable from the website associated to the “e-sensing” project.</p>
<p>The following examples show how to train ML and apply it to classify a single time series; they should not be taken as indication of which method performs better. The most important factor for achieving a good result is the quality of the training data <span class="citation">[12]</span>. Given a good set of samples, users of <code>sits</code> should be able to get good results. For examples of ML for classifying large areas, please see <a href="https://e-sensing.github.io/sitsbook/classification-of-images-in-data-cubes-using-satellite-image-time-series.html">Chapter 6</a> and the papers by the authors <span class="citation">[36]</span>.</p>
</div>
<div id="visualizing-samples" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Visualizing Samples</h2>
<p>One useful way of describing and understanding the samples is by plotting them. A direct way of doing so is using the <code>plot</code> function, as discussed in <a href="https://e-sensing.github.io/sitsbook/acessing-time-series-information-in-sits.html">Chapter 3</a>. In the plot, the thick red line is the median value for each time instance and the yellow lines are the first and third interquartile ranges.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;samples_matogrosso_mod13q1&quot;</span>)</span>
<span id="cb67-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Select a subset of the samples to be plotted</span></span>
<span id="cb67-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the set of samples for the Mato Grosso region </span></span>
<span id="cb67-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-4" aria-hidden="true" tabindex="-1"></a>samples_matogrosso_mod13q1 <span class="sc">%&gt;%</span> </span>
<span id="cb67-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_select</span>(<span class="at">bands =</span> <span class="st">&quot;NDVI&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb67-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-6" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">filter</span>(label <span class="sc">==</span> <span class="st">&quot;Forest&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb67-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb67-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-50"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-50-1.png" alt="Visualisation of samples associated to class Forest in band NDVI" width="70%" />
<p class="caption">
Figure 5.1: Visualisation of samples associated to class Forest in band NDVI
</p>
</div>
<p>An alternative to visualise the samples is to estimate a statistical approximation to an idealized pattern based on a generalised additive model (GAM). A GAM is a linear model in which the linear predictor depends linearly on a smooth function of the predictor variables
<span class="math display">\[
y = \beta_{i} + f(x) + \epsilon, \epsilon \sim N(0, \sigma^2).
\]</span>
The function <code>sits_patterns()</code> uses a GAM to predict a smooth, idealized approximation to the time series associated to the each label, for all bands. This function is based on the R package <code>dtwSat</code><span class="citation">[37]</span>, which implements the TWDTW time series matching method described in <span class="citation">[16]</span>. The resulting patterns can be viewed using <code>plot</code>.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select a subset of the samples to be plotted</span></span>
<span id="cb68-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-2" aria-hidden="true" tabindex="-1"></a>samples_matogrosso_mod13q1 <span class="sc">%&gt;%</span> </span>
<span id="cb68-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_patterns</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb68-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-51"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-51-1.png" alt="Patterns for the samples for Mato Grosso" width="70%" />
<p class="caption">
Figure 5.2: Patterns for the samples for Mato Grosso
</p>
</div>
<p>The resulting plots provide some insights over the time series behaviour of each class. While the response of the “Forest” class is quite distinctive, there are similarities between the double-cropping classes (“Soy-Corn,” “Soy-Millet,” “Soy-Sunflower” and “Soy-Corn”) and between the “Cerrado” and “Pasture” classes. This could suggest that additional information, more bands, or higher-resolution data could be considered to provide a better basis for time series samples that can better distinguish the intended classes. Despite these limitations, the best machine learning algorithms can provide good performance even in the above case.</p>
</div>
<div id="common-interface-to-machine-learning-and-deeplearning-models" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Common interface to machine learning and deeplearning models</h2>
<p>The SITS package provides a common interface to all machine learning models, using the <code>sits_train()</code> function. This function takes two mandatory parameters: the input data samples and the ML method (<code>ml_method</code>), as shown below. After the model is estimated, it can be used to classify individual time series or full data cubes using <code>sits_classify()</code>. In the examples that follow, we show how to apply each method for the classification of a single time series. Then, in <a href="https://e-sensing.github.io/sitsbook/classification-of-images-in-data-cubes-using-satellite-image-time-series.html">Chapter 6</a> we disscuss how to classify data cubes.</p>
<p>Since <code>sits</code> is aimed at remote sensing users who are not machine learning experts, the~package provides a set of default values for all classification models. These settings have been chosen based on extensive testing by the authors. Nevertheless, users can control all parameters for each model. The package documentation describes in detail the tuning parameters for all models that are available in the respective functions. Thus, novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs.</p>
<p>When a dataset of time series organised as a SITS tibble is taken as input to the classifier, the result is the same tibble with one additional column (“predicted”), which contains the information on what labels are have been assigned for each interval. The results can be shown in text format using the function <code>sits_show_prediction()</code> or graphically using <code>plot()</code>.</p>
</div>
<div id="random-forests" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Random forests</h2>
<p>The random forest model uses the idea of <em>decision trees</em> as its base model, with many refinements. When building the decision trees, each time a split in a tree is considered, a random sample of
<code>m</code> features is chosen as split candidates from the full set of <code>n</code> features of the sample set<span class="citation">[38]</span>. Each of these features is then tested, the one maximizing the decrease in a purity measure is used to build the trees. This criterion is used to identify relevant features and to perform variable selection. This decreases the correlation among trees and improves the prediction performance. The classification performance depends on the number of trees in the forest as well as the number of features randomly selected at each node. SITS provides a <code>sits_rfor</code> function which is a front-end to the <code>randomForest</code> package<span class="citation">[39]</span>; its main parameter is <code>num_trees</code> (number of trees to grow). In practice, between 100 and 200 trees are sufficient to achieve a reasonable classification result.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the set of samples (provided by EMBRAPA) from the </span></span>
<span id="cb69-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Mato Grosso region for train the Random Forest model.</span></span>
<span id="cb69-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-3" aria-hidden="true" tabindex="-1"></a>rfor_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(<span class="at">data =</span> samples_matogrosso_mod13q1, </span>
<span id="cb69-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">ml_method =</span> <span class="fu">sits_rfor</span>(<span class="at">num_trees =</span> <span class="dv">200</span>))</span>
<span id="cb69-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="co"># retrieve a point to be classified</span></span>
<span id="cb69-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-6" aria-hidden="true" tabindex="-1"></a>point_mt_4bands <span class="ot">&lt;-</span> <span class="fu">sits_select</span>(point_mt_6bands, </span>
<span id="cb69-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb69-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify using Random Forest model and plot the result</span></span>
<span id="cb69-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-9" aria-hidden="true" tabindex="-1"></a>point_class <span class="ot">&lt;-</span> <span class="fu">sits_classify</span>(point_mt_4bands, rfor_model)</span>
<span id="cb69-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(point_class, <span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:eval"></span>
<img src="sitsbook_files/figure-html/eval-1.png" alt="Classification of time series using random forests" width="70%" />
<p class="caption">
Figure 5.3: Classification of time series using random forests
</p>
</div>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># show the results of the prediction</span></span>
<span id="cb70-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sits_show_prediction</span>(point_class)</span></code></pre></div>
<pre class="sourceCode"><code>#&gt; # A tibble: 17 × 3
#&gt;    from       to         class   
#&gt;    &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   
#&gt;  1 2000-09-13 2001-08-29 Forest  
#&gt;  2 2001-09-14 2002-08-29 Forest  
#&gt;  3 2002-09-14 2003-08-29 Forest  
#&gt;  4 2003-09-14 2004-08-28 Pasture 
#&gt;  5 2004-09-13 2005-08-29 Pasture 
#&gt;  6 2005-09-14 2006-08-29 Pasture 
#&gt;  7 2006-09-14 2007-08-29 Pasture 
#&gt;  8 2007-09-14 2008-08-28 Pasture 
#&gt;  9 2008-09-13 2009-08-29 Pasture 
#&gt; 10 2009-09-14 2010-08-29 Soy_Corn
#&gt; 11 2010-09-14 2011-08-29 Soy_Corn
#&gt; 12 2011-09-14 2012-08-28 Soy_Corn
#&gt; 13 2012-09-13 2013-08-29 Soy_Corn
#&gt; 14 2013-09-14 2014-08-29 Soy_Corn
#&gt; 15 2014-09-14 2015-08-29 Soy_Corn
#&gt; 16 2015-09-14 2016-08-28 Soy_Corn
#&gt; 17 2016-09-13 2017-08-29 Soy_Corn</code></pre>
<p>The result shows that the area started out as a forest in 2000, it was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards.They are consistent with expert evaluation of the process of land use change in this region of Amazonia.</p>
<p>Random forests are robust to outliers and able to deal with irrelevant inputs <span class="citation">[40]</span>. However, despite being robust, this approach tends to overemphasize some variables and thus rarely turns out to be the classifier with the smallest error. One reason is that the performance of random forests tends to stabilize after a part of the trees are grown <span class="citation">[40]</span>. In general, random forests can be useful to provide a baseline to compare with more sophisticated methods.</p>
</div>
<div id="support-vector-machines" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Support Vector Machines</h2>
<p>The support vector Machine (SVM) classifies is a generalization of a linear classifier which finds are an optimal separation hyperplane that minimizes misclassifications <span class="citation">[41]</span>. Since a set of samples with <span class="math inline">\(n\)</span> features defines an n-dimensional feature space, hyperplanes are linear <span class="math inline">\({(n-1)}\)</span>-dimensional boundaries that define linear partitions in that space. If the classes are linearly separable on the feature space, there will be an optimal solution defined by the <em>maximal margin hyperplane</em>, which is the separating hyperplane that is farthest from the training observations<span class="citation">[38]</span>. The maximal margin is computed as the the smallest distance from the observations to the hyperplane. However, in general the data is not linearly separable. In this case, the SVM classifier allows some samples to be on the wrong side of hyperplane. SVM attempts to minimize the number of wrong classified samples, based on an optimization parameter that defines the cost of misclassification.</p>
<p>The fact SVMs have misclassified samples actually turns out to be an advantage, since such behavior makes the result robust to outliers and focus on getting a correct result using most of the observations <span class="citation">[38]</span>. The solution for the hyperplane coefficients depends only on the samples that violates the maximum margin criteria, the so-called <em>support vectors</em>.</p>
<p>For data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. The new classification model, despite having a linear boundary on the enlarged feature space, generally translates its hyperplane to a nonlinear boundaries in the original attribute space. Kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space; thus, they improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been widely applied to classify remote sensing data <span class="citation">[25]</span>.</p>
<p>In <code>sits</code>, SVM is implemented as a wrapper of <code>e1071</code> R package that uses the <code>LIBSVM</code> implementation <span class="citation">[42]</span>, the <code>sits</code> package adopts the <em>one-against-one</em> method for multiclass classification. For a <span class="math inline">\(q\)</span> class problem, this method creates <span class="math inline">\({q(q-1)/2}\)</span> SVM binary models, one for each class pair combination and tests any unknown input vectors throughout all those models. The overall result is computed by a voting scheme.</p>
<p>The example below shows how to apply the SVM method for classification of time series using default values. The main parameters for the SVM are <code>kernel</code> which controls whether to use a non-linear transformation (default is <code>radial</code>), <code>cost</code> which measures the punishment for wrongly-classified samples (default is 10), and <code>cross</code> which sets the value of the k-fold cross validation (default is 10). .</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the data slightly to reduce noise without reducing variability</span></span>
<span id="cb72-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-2" aria-hidden="true" tabindex="-1"></a>samples_filtered <span class="ot">&lt;-</span> <span class="fu">sits_whittaker</span>(samples_matogrosso_mod13q1, </span>
<span id="cb72-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-3" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">lambda =</span> <span class="fl">0.5</span>,</span>
<span id="cb72-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-4" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">bands_suffix =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb72-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a machine learning model for the mato grosso dataset using SVM</span></span>
<span id="cb72-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The parameters are those of the &quot;e1071:svm&quot; method</span></span>
<span id="cb72-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-7" aria-hidden="true" tabindex="-1"></a>svm_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb72-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">ml_method =</span> <span class="fu">sits_svm</span>())</span>
<span id="cb72-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify using SVM model and plot the result</span></span>
<span id="cb72-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-10" aria-hidden="true" tabindex="-1"></a>class <span class="ot">&lt;-</span> point_mt_4bands <span class="sc">%&gt;%</span> </span>
<span id="cb72-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_whittaker</span>(<span class="at">lambda =</span> <span class="fl">0.5</span>, <span class="at">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb72-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_classify</span>(svm_model) <span class="sc">%&gt;%</span> </span>
<span id="cb72-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-53"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-53-1.png" alt="Classification of time series using SVM" width="70%" />
<p class="caption">
Figure 5.4: Classification of time series using SVM
</p>
</div>
<p>Compared with the output from the random forest classifier, the SVM result includes more classes. In particular, the transition from forest to pasture that occurs after 2004 is more gradual and shows a possible period of land abandonment in 2004 and 2005. Since the training dataset does not contain samples of deforested areas, places where forest is removed will tend to be classified as “Cerrado,” which is the nearest kind of vegetation cover where trees and grasslands are mixed. Also, the classification for 2009 is different from the “Soy-Corn” label assigned from the other years from 2008 to 2017. In this example, the <code>sits_svm()</code> method is more sensitive to the distribution of the samples in the feature space than the random forests algorithm.</p>
</div>
<div id="extreme-gradient-boosting" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Extreme Gradient Boosting</h2>
<p>The boosting method is based on the idea of starting from a weak predictor and then improving performance sequentially by fitting a better model at each iteration. It starts by fitting a simple classifier to the training data, and using the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although both random forests and boosting use trees for classification, there are important differences. The performance of random forests generally increases with the number of trees until it becomes stable. Boosting trees improve on previous result by applying finer divisions that improve the performance <span class="citation">[40]</span>. However, the number of trees grown by boosting techniques has to be limited at the risk of overfitting.</p>
<p>Gradient boosting is a variant of boosting methods where the cost function is minimized by gradient descent. Extreme gradient boosting <span class="citation">[26]</span>, called XGBoost, is an efficient approximation to the gradient loss function. XGBoost is considered one of the best statistical learning algorithms available and has won many competitions; it is generally considered to be better than random forests. Actual performance is controlled by the quality of the training data set.</p>
<p>In SITS, the XGBoost method is implemented by the <code>sits_xbgoost()</code> function, which is based on <code>XGBoost</code> R package and has five hyperparameters that require tuning. The <code>sits_xbgoost()</code> function takes the user choices as input to a cross validation to determine suitable values for the predictor.</p>
<p>The learning rate <code>eta</code> varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value <code>gamma</code> specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The <code>max_depth</code> value controls the maximum depth of the trees. Increasing this value will make the model more complex and more likely to overfit (default is 6). The <code>subsample</code> parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The <code>nrounds</code> parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. In order to follow the convergence of the algorithm, users can turn the <code>verbose</code> parameter on.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a machine learning model for the mato grosso dataset using XGBOOST</span></span>
<span id="cb73-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The parameters are those of the &quot;xgboost&quot; package</span></span>
<span id="cb73-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-3" aria-hidden="true" tabindex="-1"></a>xgb_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(samples_filtered, <span class="fu">sits_xgboost</span>(<span class="at">verbose =</span> <span class="dv">0</span>))</span>
<span id="cb73-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify using SVM model and plot the result</span></span>
<span id="cb73-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-5" aria-hidden="true" tabindex="-1"></a>point_mt_4bands <span class="sc">%&gt;%</span> </span>
<span id="cb73-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_whittaker</span>(<span class="at">lambda =</span> <span class="fl">0.50</span>, <span class="at">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb73-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_classify</span>(xgb_model) <span class="sc">%&gt;%</span> </span>
<span id="cb73-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-54"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-54-1.png" alt="Classification of time series using XGBoost" width="70%" />
<p class="caption">
Figure 5.5: Classification of time series using XGBoost
</p>
</div>
<p>In general, the results from the extreme gradient boosting model are similar to the Random Forest model. If desired, users can tune the hyperparameters need to perform using function <code>sits_kfold_validate()</code>. See <a href="https://e-sensing.github.io/sitsbook/validation-and-accuracy-measurements-in-sits.html">Chapter 8</a> for more details.</p>
</div>
<div id="deep-learning-using-multi-layer-perceptrons" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Deep learning using multi-layer perceptrons</h2>
<p>To support deep learning methods, <code>sits</code> uses the <code>keras</code> R package <span class="citation">[43]</span> as a backend. The first method is that of multi-layer perceptron (MLPs). These are the quintessential deep learning models. The goal of a multilayer perceptron is to approximate a function <span class="math inline">\(y = f(x)\)</span> that maps an input <span class="math inline">\(x\)</span> to a category <span class="math inline">\(y\)</span>. An MLP defines a mapping <span class="math inline">\(y = f(x;\theta)\)</span> and learns the value of the parameters <span class="math inline">\(\theta\)</span> that result in the best function approximation <span class="citation">[44]</span>. An MLP consists of types of nodes: an input layer, a set of hidden layers and an output layer. The input layer has the same dimension as the number of the features in the data set. A user-defined number of hidden layers attempt to approximate the best classification function. The output layer makes a decision about which class should be assigned to the input.</p>
<p>In <code>sits</code>, users build MLP models using <code>sits_mlp()</code>. Since there is no proven model for classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the <code>layers</code> parameters, which is a list of integer values. The size of the list is the number of layers and each element of the list indicates the number of nodes per layer.</p>
<p>The choice of the number of layers depends on the inherent separability of the data set to be classified. For data sets where the classes have different signatures, a shallow model (with 3 layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. The user should be aware that some models with many hidden layers may take a long time to train and may not be able to converge. The suggestion is to start with 3 layers and test different options of number of neurons per layer, before increasing the number of layers.</p>
<p>MLP models also need to include the activation function (<code>activation</code>). The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices <span class="citation">[44]</span>, we recommend the use of the “relu” and “elu” functions.</p>
<p>Users can also define the optimization method(<code>optimizer</code>), which defines the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function <span class="citation">[45]</span>. Based on experience with image time series, we recommend that users start by using the default method provided by <code>sits</code>, which is the <code>optimizer_adam</code> method. Please refer to the <code>keras</code> package documentation for more information.</p>
<p>Another relevant parameter is the list of dropout rates (<code>dropout</code>). Dropout is a technique for randomly dropping units from the neural network during training <span class="citation">[46]</span>. By randomly discarding some neurons, dropout reduces overfitting. Since the purpose of a cascade of neural nets is to improve learning as more data is acquired, discarding some neurons may seem a waste of resources. In practice, dropout prevents an early convergence to a local minimum <span class="citation">[44]</span>. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.</p>
<p>The following example shows how to use <code>sits_mlp()</code>. The default parameters for have been chosen based on <span class="citation">[47]</span>, which proposes the use of multi-layer perceptrons as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter <code>layers</code>; (b) Using the “relu” activation function; (c) dropout rates of 10%, 20%, and 30% for the layers; (d) the “optimizer_adam” as optimizer (default value); (e) a number of training steps (<code>epochs</code>) of 100; (f) a <code>batch_size</code> of 64, which indicates how many time series are used for input at a given steps; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set side for validation.</p>
<p>In our experience, if the training dataset is of good quality, using 3 to 5 layers is a reasonable compromise. Further increase on the number of layers will not improve the model. To simplify the output generation, the <code>verbose</code> option has been turned off. The default value is “on.” After the model has been generated, we plot its training history.
In this and in the following examples of using deep learning classifiers, both the training samples and the point to be classified are filtered with <code>sits_whittaker()</code> with a small smoothing parameter (lambda = 0.5). Since deep learning classifiers are not as robust as Random Forest or XGBoost, the right amount of smoothing improves their detection power in case of noisy data.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train a machine learning model for the Mato Grosso data using an MLP model</span></span>
<span id="cb74-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-3" aria-hidden="true" tabindex="-1"></a>mlp_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(samples_filtered, </span>
<span id="cb74-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-4" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">sits_mlp</span>(</span>
<span id="cb74-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-5" aria-hidden="true" tabindex="-1"></a>                        <span class="at">layers           =</span> <span class="fu">c</span>(<span class="dv">512</span>, <span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb74-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">activation       =</span> <span class="st">&quot;relu&quot;</span>,</span>
<span id="cb74-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">dropout_rates    =</span> <span class="fu">c</span>(<span class="fl">0.10</span>, <span class="fl">0.20</span>, <span class="fl">0.30</span>),</span>
<span id="cb74-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">epochs           =</span> <span class="dv">100</span>,</span>
<span id="cb74-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">batch_size       =</span> <span class="dv">64</span>,</span>
<span id="cb74-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">verbose          =</span> <span class="dv">0</span>,</span>
<span id="cb74-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">validation_split =</span> <span class="fl">0.2</span>) )</span>
<span id="cb74-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="co"># show training evolution</span></span>
<span id="cb74-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mlp_model)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-55-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb75-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-2" aria-hidden="true" tabindex="-1"></a>point_mt_6bands <span class="sc">%&gt;%</span> </span>
<span id="cb75-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_select</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb75-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_whittaker</span>(<span class="at">lambda =</span> <span class="fl">0.5</span>, <span class="at">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb75-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_classify</span>(mlp_model) <span class="sc">%&gt;%</span> </span>
<span id="cb75-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-56-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The multi-layer perceptron model is able to capture more subtle changes than the random forests and XGBoost models. For example, the transition from Forest to Pasture as estimated by the model is not abrupt and takes more than one year. In 2004, the time series corresponds to that of a degraded forest. Since there are no samples for “Forest Degradation,” the model assigns this series to a class that is neither “Forest” nor “Pasture,” which in our case is “Cerrado.” This indicates that users should include samples of “Forest Degradation” to improve classification. Although the model mixes the “Soy_Corn” and “Soy_Millet” classes, the distinction between their temporal signatures is quite subtle. Also in this case, this suggests the need to improve the number of samples. In this examples, the MLP model shows an increase in the sensitivity compared to previous models. We recommend that the users compare different configurations, since the MLP model is sensitive to changes in its parameters.</p>
</div>
<div id="temporal-convolutional-neural-network-tempcnn" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Temporal Convolutional Neural Network (TempCNN)</h2>
<p>Convolutional neural networks (CNN) are a variety of deep learning methods where a convolution filter (sliding window) is applied to the input data. In the case of time series, a 1D CNN works by applying a moving window to the series. Using convolution filters is a way to incorporate temporal autocorrelation information in the classification. The result of the convolution is another time series. <span class="citation">[48]</span> states that the use of 1D-CNN for time series classification improves on the use of multi-layer perceptrons, since the classifier is able to represent temporal relationships. 1D-CNNs with a suitable convolution window make the classifier more robust to moderate noise, e.g. intermittent presence of clouds.</p>
<p>The Use of 1D CNNs for satellite image time series classification is proposed in <span class="citation">[2]</span>. The “TempCNN” architecture has three 1D convolutional layers (each with 64 units), one dense layer of 256 units and a final softmax layer for classification (see figure). The kernel size of the convolution filters is set to 5. The authors use a combination of different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalisation. In the tempCNN paper <span class="citation">[2]</span>, the authors compare favourably the tempCNN model with the Recurrent Neural Network proposed by <span class="citation">[29]</span> for land use classification. The figure below shows the architecture of the tempCNN model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-57"></span>
<img src="images/tempcnn.png" alt="Structure of tempCNN architecture (source: Pelletier et al.(2019))" width="80%" height="80%" />
<p class="caption">
Figure 5.6: Structure of tempCNN architecture (source: Pelletier et al.(2019))
</p>
</div>
<p>The function <code>sits_TempCNN()</code> implements the model. The code has been derived from the Python source provided by the authors (<a href="https://github.com/charlotte-pel/temporalCNN" class="uri">https://github.com/charlotte-pel/temporalCNN</a>). Most of the parameters corresponds to those chosen by <span class="citation">[2]</span>. The parameter <code>cnn_layers</code> controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 64 units. The parameter <code>cnn_kernels</code> indicates the size of the convolution kernels; the default values are kernels of size 5. Activation for all 1D-CNN layers is set by <code>cnn_activation</code> (default = “relu”). The parameter <code>cnn_L2_rate</code> controls the regularization rate, with a default of “1e-06.” The dropout rates for each 1D-CNN layer are controlled individually by the parameter <code>cnn_dropout_rates</code>. Based on discussions with the author, the defaults in <code>sits</code> range from 10% to 30%. The <code>validation_split</code> controls the size of the test set, relative to the full data set. We recommend to set aside at least 20% of the samples for validation.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train a machine learning model using tempCNN</span></span>
<span id="cb76-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-2" aria-hidden="true" tabindex="-1"></a>tCNN_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(samples_filtered, </span>
<span id="cb76-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-3" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">sits_TempCNN</span>(</span>
<span id="cb76-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">cnn_layers           =</span> <span class="fu">c</span>(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">64</span>),</span>
<span id="cb76-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">cnn_kernels          =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb76-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">cnn_activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb76-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">cnn_L2_rate          =</span> <span class="fl">1e-06</span>,</span>
<span id="cb76-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">cnn_dropout_rates    =</span> <span class="fu">c</span>(<span class="fl">0.10</span>, <span class="fl">0.20</span>, <span class="fl">0.30</span>),</span>
<span id="cb76-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-9" aria-hidden="true" tabindex="-1"></a>                          <span class="at">epochs               =</span> <span class="dv">100</span>,</span>
<span id="cb76-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">batch_size           =</span> <span class="dv">64</span>,</span>
<span id="cb76-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-11" aria-hidden="true" tabindex="-1"></a>                          <span class="at">validation_split     =</span> <span class="fl">0.2</span>,</span>
<span id="cb76-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-12" aria-hidden="true" tabindex="-1"></a>                          <span class="at">verbose              =</span> <span class="dv">0</span>) )</span>
<span id="cb76-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-14" aria-hidden="true" tabindex="-1"></a><span class="co"># show training evolution</span></span>
<span id="cb76-15"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tCNN_model)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-58-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Then, we classify a 16-year time series using the TempCNN model</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify using TempCNN model and plot the result</span></span>
<span id="cb77-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-2" aria-hidden="true" tabindex="-1"></a>class <span class="ot">&lt;-</span> point_mt_6bands <span class="sc">%&gt;%</span> </span>
<span id="cb77-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_select</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb77-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_whittaker</span>(<span class="at">lambda =</span> <span class="fl">0.5</span>, <span class="at">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb77-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_classify</span>(tCNN_model) <span class="sc">%&gt;%</span> </span>
<span id="cb77-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-59-1.png" width="70%" style="display: block; margin: auto;" />
While the result of the TempCNN model using default parameters is similar to that of the MLP model, it has has the potential to better explore the time series data than the the MLP model. In our experience, TempCNN models are a reliable way for classifying image time series<span class="citation">[49]</span>. Recent work which compares different models also provides evidence of a satisfactory behavior<span class="citation">[31]</span>.</p>
</div>
<div id="residual-1d-cnn-networks-resnet" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Residual 1D CNN Networks (ResNet)</h2>
<p>The Residual Network (ResNet) is a 1D convolution neural network (CNN) proposed by <span class="citation">[47]</span>, based on the idea proposed by <span class="citation">[50]</span> for image recognition. The ResNet architecture is composed of 11 layers, with three blocks of three 1D CNN layers each (see figure below). Each block corresponds to a 1D CNN architecture. The output of each block is combined with a shortcut that links its output to its input, called a “skip connection.” The purpose of combining the input layer of each block with its output layer (after the convolutions) is to avoid the so-called “vanishing gradient problem.” This issue occurs in deep networks as he neural network’s weights are updated based on the partial derivative of the error function. If the gradient is too small, the weights will not be updated, stopping the training<span class="citation">[51]</span>. Skip connections aim to avoid vanishing gradients from occurring, allowing deep networks to be trained.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-60"></span>
<img src="images/resnet.png" alt="Structure of ResNet architecture (source: Wang et al.(2017))" width="80%" height="80%" />
<p class="caption">
Figure 5.7: Structure of ResNet architecture (source: Wang et al.(2017))
</p>
</div>
<p>In <code>sits</code>, the ResNet is implemented using the <code>sits_ResNet()</code> function. The default parameters are those proposed by <span class="citation">[47]</span>, and we also benefited from the code provided by <span class="citation">[32]</span>. The first parameter is <code>blocks</code>, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128 filters. Users can control the number of blocks and filter size by changing this parameter. The parameter <code>kernels</code> controls the size the of kernels of the three layers inside each block. We have found out that it is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is “relu,” which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is the same as proposed in <span class="citation">[47]</span> and <span class="citation">[32]</span>.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train a machine learning model using ResNet</span></span>
<span id="cb78-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-2" aria-hidden="true" tabindex="-1"></a>resnet_model <span class="ot">&lt;-</span> <span class="fu">sits_train</span>(samples_filtered, </span>
<span id="cb78-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-3" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">sits_ResNet</span>(</span>
<span id="cb78-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">blocks               =</span> <span class="fu">c</span>(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">128</span>),</span>
<span id="cb78-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">kernels              =</span> <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">3</span>),</span>
<span id="cb78-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">activation           =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb78-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">epochs               =</span> <span class="dv">150</span>,</span>
<span id="cb78-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">batch_size           =</span> <span class="dv">64</span>,</span>
<span id="cb78-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-9" aria-hidden="true" tabindex="-1"></a>                          <span class="at">validation_split     =</span> <span class="fl">0.2</span>,</span>
<span id="cb78-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">verbose              =</span> <span class="dv">0</span>) )</span>
<span id="cb78-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-11" aria-hidden="true" tabindex="-1"></a><span class="co"># show training evolution</span></span>
<span id="cb78-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(resnet_model)</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-61-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Then, we classify a 16-year time series using the ResNet model.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb79-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-2" aria-hidden="true" tabindex="-1"></a>point_mt_4bands <span class="ot">&lt;-</span> <span class="fu">sits_select</span>(point_mt_6bands, </span>
<span id="cb79-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb79-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-4" aria-hidden="true" tabindex="-1"></a>class.tb <span class="ot">&lt;-</span> point_mt_4bands <span class="sc">%&gt;%</span> </span>
<span id="cb79-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_whittaker</span>(<span class="at">lambda =</span> <span class="fl">0.5</span>, <span class="at">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb79-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sits_classify</span>(resnet_model) <span class="sc">%&gt;%</span> </span>
<span id="cb79-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="at">bands =</span> <span class="fu">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-62-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="considerations-on-model-choice" class="section level2" number="5.10">
<h2><span class="header-section-number">5.10</span> Considerations on model choice</h2>
<p>The development of machine learning methods for classification of satellite image time series is an ongoing task. There is a lot of recent work using methods such as convolutional neural networks <span class="citation">[28]</span>, long short term memory convolutional networks (LSTM-FCN)<span class="citation">[52]</span> and temporal self-attention <span class="citation">[30]</span>. Given the rapid evolution of the field with new methods still being developed, there are few references that offer a comparison between different machine learning methods. Most works on the literature <span class="citation">[32]</span> compare methods for generic time series classification. Their insights are not directly applicable for satellite image time series data, which have different properties than the time series available in comparison archives such as the UCR dataset.</p>
<p>In the specific case of satellite image time series, <span class="citation">[31]</span> presents a comparative study between seven deep neural networks for classification of agricultural crops, using random forests (RF) as a baseline. The dataset is composed of Sentinel-2 images over Britanny, France. Their results indicate that a slight difference between the best model (attention-based transformer model) over TempCNN, ResNet and RF. Attention-based models obtain accuracy ranging from 80-81%, TempCNN get 78-80%, and RF gets 78%. Based on this result and also on the authors’ experience, we make the following recommendations:</p>
<ol style="list-style-type: decimal">
<li><p>Random forests provide a good baseline for image time series classification and should be included in users’ assessments.</p></li>
<li><p>XGBoost is an worthy alternative to Random forests. In principle, XGBoost is more sensitive to data variations at the cost of possible overfitting.</p></li>
<li><p>TempCNN is a reliable model with reasonable training time, which is close to the state-of-the-art in deep learning classifiers for image time series.</p></li>
<li><p>Given the small differences between the models, the best means of improving classification performance is to provide an accurate and reliable training data set. Each class should have enough samples to account for spatial and temporal variability. Using clustering methods (<a href="https://e-sensing.github.io/sitsbook/time-series-clustering-to-improve-the-quality-of-training-samples.html">Chapter 4</a> ) to improve sample quality is highly recommended.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="time-series-clustering-to-improve-the-quality-of-training-samples.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["sitsbook.pdf"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
