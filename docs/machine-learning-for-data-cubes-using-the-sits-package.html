<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />
  <meta name="generator" content="bookdown 0.21.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Machine Learning for Data Cubes using the SITS package | sits: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The output format for this example is bookdown::gitbook.</p>" />
  <meta name="twitter:image" content="/images/cover.png" />



<meta name="date" content="2021-03-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="time-series-clustering-to-improve-the-quality-of-training-samples.html"/>
<link rel="next" href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SITS Book</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#workflow"><i class="fa fa-check"></i><b>1.1</b> Workflow</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#handling-data-cubes-in-sits"><i class="fa fa-check"></i><b>1.2</b> Handling Data Cubes in <strong>sits</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis"><i class="fa fa-check"></i><b>1.2.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#using-stac-to-access-image-data-cubes"><i class="fa fa-check"></i><b>1.2.2</b> Using STAC to Access Image Data Cubes</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#defining-a-data-cube-using-files"><i class="fa fa-check"></i><b>1.2.3</b> Defining a data cube using files</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#handling-satellite-image-time-series-in-sits"><i class="fa fa-check"></i><b>1.3</b> Handling satellite image time series in <strong>sits</strong></a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#data-structure"><i class="fa fa-check"></i><b>1.3.1</b> Data structure</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#obtaining-time-series-data"><i class="fa fa-check"></i><b>1.3.2</b> Obtaining time series data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#filtering-techniques"><i class="fa fa-check"></i><b>1.4</b> Filtering techniques</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#clustering-for-sample-quality-control-using-self-organizing-maps"><i class="fa fa-check"></i><b>1.5</b> Clustering for sample quality control using self-organizing maps</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#classification-using-machine-learning"><i class="fa fa-check"></i><b>1.6</b> Classification using machine learning</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#validation-techniques"><i class="fa fa-check"></i><b>1.7</b> Validation techniques</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#cube-classification"><i class="fa fa-check"></i><b>1.8</b> Cube classification</a><ul>
<li class="chapter" data-level="1.8.1" data-path="introduction.html"><a href="introduction.html#steps-for-cube-classification"><i class="fa fa-check"></i><b>1.8.1</b> Steps for cube classification</a></li>
<li class="chapter" data-level="1.8.2" data-path="introduction.html"><a href="introduction.html#adjustments-for-improved-performance"><i class="fa fa-check"></i><b>1.8.2</b> Adjustments for improved performance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="introduction.html"><a href="introduction.html#smoothing-and-labelling-of-raster-data-after-classification"><i class="fa fa-check"></i><b>1.9</b> Smoothing and Labelling of raster data after classification</a></li>
<li class="chapter" data-level="1.10" data-path="introduction.html"><a href="introduction.html#final-remarks"><i class="fa fa-check"></i><b>1.10</b> Final remarks</a></li>
<li class="chapter" data-level="1.11" data-path="introduction.html"><a href="introduction.html#acknowledgements"><i class="fa fa-check"></i><b>1.11</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html"><i class="fa fa-check"></i><b>2</b> Acessing time series information in SITS</a><ul>
<li class="chapter" data-level="2.1" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#data-structures-for-satellite-time-series"><i class="fa fa-check"></i><b>2.1</b> Data structures for satellite time series</a></li>
<li class="chapter" data-level="2.2" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#utilities-for-handling-time-series"><i class="fa fa-check"></i><b>2.2</b> Utilities for handling time series</a></li>
<li class="chapter" data-level="2.3" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#time-series-visualisation"><i class="fa fa-check"></i><b>2.3</b> Time series visualisation</a></li>
<li class="chapter" data-level="2.4" data-path="acessing-time-series-information-in-sits.html"><a href="acessing-time-series-information-in-sits.html#obtaining-time-series-data-from-data-cubes"><i class="fa fa-check"></i><b>2.4</b> Obtaining time series data from data cubes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html"><i class="fa fa-check"></i><b>3</b> Satellite Image Time Series Filtering with SITS</a><ul>
<li class="chapter" data-level="3.1" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#filtering-techniques-in-sits"><i class="fa fa-check"></i><b>3.1</b> Filtering techniques in SITS</a></li>
<li class="chapter" data-level="3.2" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#common-interface-to-sits-filter-functions"><i class="fa fa-check"></i><b>3.2</b> Common interface to SITS filter functions</a><ul>
<li class="chapter" data-level="3.2.1" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#savitzkygolay-filter"><i class="fa fa-check"></i><b>3.2.1</b> Savitzky–Golay filter</a></li>
<li class="chapter" data-level="3.2.2" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#whittaker-filter"><i class="fa fa-check"></i><b>3.2.2</b> Whittaker filter</a></li>
<li class="chapter" data-level="3.2.3" data-path="satellite-image-time-series-filtering-with-sits.html"><a href="satellite-image-time-series-filtering-with-sits.html#envelope-filter"><i class="fa fa-check"></i><b>3.2.3</b> Envelope filter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html"><i class="fa fa-check"></i><b>4</b> Time Series Clustering to Improve the Quality of Training Samples</a><ul>
<li class="chapter" data-level="4.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.1</b> Clustering for sample quality control</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#hierachical-clustering-for-sample-quality-control"><i class="fa fa-check"></i><b>4.2</b> Hierachical clustering for Sample Quality Control</a><ul>
<li class="chapter" data-level="4.2.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#creating-a-dendogram"><i class="fa fa-check"></i><b>4.2.1</b> Creating a dendogram</a></li>
<li class="chapter" data-level="4.2.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-a-dendrogram-to-evaluate-sample-quality"><i class="fa fa-check"></i><b>4.2.2</b> Using a dendrogram to evaluate sample quality</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-self-organizing-maps-for-sample-quality"><i class="fa fa-check"></i><b>4.3</b> Using Self-organizing Maps for Sample Quality</a><ul>
<li class="chapter" data-level="4.3.1" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#introduction-to-self-organizing-maps"><i class="fa fa-check"></i><b>4.3.1</b> Introduction to Self-organizing Maps</a></li>
<li class="chapter" data-level="4.3.2" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#using-som-for-removing-class-noise"><i class="fa fa-check"></i><b>4.3.2</b> Using SOM for removing class noise</a></li>
<li class="chapter" data-level="4.3.3" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#comparing-global-accuracy-of-original-and-clean-samples"><i class="fa fa-check"></i><b>4.3.3</b> Comparing Global Accuracy of Original and Clean Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="time-series-clustering-to-improve-the-quality-of-training-samples.html"><a href="time-series-clustering-to-improve-the-quality-of-training-samples.html#conclusion"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html"><i class="fa fa-check"></i><b>5</b> Machine Learning for Data Cubes using the SITS package</a><ul>
<li class="chapter" data-level="5.1" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#machine-learning-classification"><i class="fa fa-check"></i><b>5.1</b> Machine learning classification</a></li>
<li class="chapter" data-level="5.2" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#data-used-in-the-machine-learning-examples"><i class="fa fa-check"></i><b>5.2</b> Data used in the machine learning examples</a></li>
<li class="chapter" data-level="5.3" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#visualizing-samples"><i class="fa fa-check"></i><b>5.3</b> Visualizing Samples</a></li>
<li class="chapter" data-level="5.4" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#common-interface-to-machine-learning-and-deeplearning-models"><i class="fa fa-check"></i><b>5.4</b> Common interface to machine learning and deeplearning models</a></li>
<li class="chapter" data-level="5.5" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#random-forests"><i class="fa fa-check"></i><b>5.5</b> Random forests</a></li>
<li class="chapter" data-level="5.6" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#support-vector-machines"><i class="fa fa-check"></i><b>5.6</b> Support Vector Machines</a></li>
<li class="chapter" data-level="5.7" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#extreme-gradient-boosting"><i class="fa fa-check"></i><b>5.7</b> Extreme Gradient Boosting</a></li>
<li class="chapter" data-level="5.8" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#deep-learning-using-multi-layer-perceptrons"><i class="fa fa-check"></i><b>5.8</b> Deep learning using multi-layer perceptrons</a></li>
<li class="chapter" data-level="5.9" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#d-convolutional-neural-networks"><i class="fa fa-check"></i><b>5.9</b> 1D Convolutional Neural Networks</a></li>
<li class="chapter" data-level="5.10" data-path="machine-learning-for-data-cubes-using-the-sits-package.html"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#residual-1d-cnn-networks-resnet"><i class="fa fa-check"></i><b>5.10</b> Residual 1D CNN Networks (ResNet)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><i class="fa fa-check"></i><b>6</b> Classification of Images in Data Cubes using Satellite Image Time Series</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#image-data-cubes-as-the-basis-for-big-earth-observation-data-analysis-1"><i class="fa fa-check"></i><b>6.1</b> Image data cubes as the basis for big Earth observation data analysis</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#defining-a-data-cube-using-files-organised-as-raster-bricks"><i class="fa fa-check"></i><b>6.2</b> Defining a data cube using files organised as raster bricks</a></li>
<li class="chapter" data-level="6.3" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#classification-using-machine-learning-1"><i class="fa fa-check"></i><b>6.3</b> Classification using machine learning</a></li>
<li class="chapter" data-level="6.4" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#cube-classification-1"><i class="fa fa-check"></i><b>6.4</b> Cube classification</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#steps-for-cube-classification-1"><i class="fa fa-check"></i><b>6.4.1</b> Steps for cube classification</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#adjustments-for-improved-performance-1"><i class="fa fa-check"></i><b>6.4.2</b> Adjustments for improved performance</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-of-images-in-data-cubes-using-satellite-image-time-series.html"><a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html#final-remarks-1"><i class="fa fa-check"></i><b>6.5</b> Final remarks</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><i class="fa fa-check"></i><b>7</b> Post classification smoothing using Bayesian techniques in SITS</a><ul>
<li class="chapter" data-level="7.1" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#overview-of-bayesian-estimattion"><i class="fa fa-check"></i><b>7.2</b> Overview of Bayesian estimattion</a><ul>
<li class="chapter" data-level="7.2.1" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#smmothing-using-bayes-rule"><i class="fa fa-check"></i><b>7.2.1</b> Smmothing using Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="post-classification-smoothing-using-bayesian-techniques-in-sits.html"><a href="post-classification-smoothing-using-bayesian-techniques-in-sits.html#use-of-bayesian-smoothing-in-sits"><i class="fa fa-check"></i><b>7.3</b> Use of Bayesian smoothing in SITS</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html"><i class="fa fa-check"></i><b>8</b> Validation and accuracy measurements in SITS</a><ul>
<li class="chapter" data-level="8.1" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#validation-techniques-1"><i class="fa fa-check"></i><b>8.1</b> Validation techniques</a></li>
<li class="chapter" data-level="8.2" data-path="validation-and-accuracy-measurements-in-sits.html"><a href="validation-and-accuracy-measurements-in-sits.html#comparing-different-validation-methods"><i class="fa fa-check"></i><b>8.2</b> Comparing different validation methods</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><strong>sits</strong>: Data Analysis and Machine Learning for Data Cubes using Satellite Image Time Series</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-for-data-cubes-using-the-sits-package" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Machine Learning for Data Cubes using the SITS package</h1>
<hr />
<p>This chapter presents the machine learning techniques available in SITS. The main use for machine learning in SITS is for classification of land use and land cover. These machine learning methods available in SITS include linear and quadratic discrimination analysis, support vector machines, random forests, deep learning and neural networks.</p>
<hr />
<div id="machine-learning-classification" class="section level2">
<h2><span class="header-section-number">5.1</span> Machine learning classification</h2>
<p><code>sits</code> has support for a variety of machine learning techniques: linear discriminant analysis, quadratic discriminant analysis, multinomial logistic regression, random forests, boosting, support vector machines, and deep learning. The deep learning methods include multi-layer perceptrons, 1D convolution neural networks and mixed approaches such as TempCNN <span class="citation">(Pelletier, Webb, and Petitjean <a href="#ref-Pelletier2019" role="doc-biblioref">2019</a>)</span> . In a recent review of machine learning methods to classify remote sensing data <span class="citation">(Maxwell, Warner, and Fang <a href="#ref-Maxwell2018" role="doc-biblioref">2018</a>)</span>, the authors note that many factors influence the performance of these classifiers, including the size and quality of the training dataset, the dimension of the feature space, and the choice of the parameters. We support both  and  approaches. Therefore, the <code>sits</code> package provides functionality to explore the full depth of satellite image time series data.</p>
<p>When used in  approach, <code>sits</code> treats time series as a feature vector. To be consistent, the procedure aligns all time series from different years by its time proximity considering an given cropping schedule. Once aligned, the feature vector is formed by all pixel “bands”. The idea is to have as many temporal attributes as possible, increasing the dimension of the classification space. In this scenario, statistical learning models are the natural candidates to deal with high-dimensional data: learning to distinguish all land cover and land use classes from trusted samples exemplars (the training data) to infer classes of a larger data set.</p>
<p>SITS provides support for the classification of both individual time series as well as data cubes. The following machine learning methods are available in SITS:</p>
<ul>
<li>Linear discriminant analysis (<code>sits_lda</code>)</li>
<li>Quadratic discriminant analysis (<code>sits_qda</code>)</li>
<li>Multinomial logit and its variants ‘lasso’ and ‘ridge’ (<code>sits_mlr</code>)</li>
<li>Support vector machines (<code>sits_svm</code>)</li>
<li>Random forests (<code>sits_rfor</code>)</li>
<li>Extreme gradient boosting (<code>sits_xgboost</code>)</li>
<li>Deep learning (DL) using multi-layer perceptrons (<code>sits_deeplearning</code>)</li>
<li>DL with 1D convolutional neural networks (<code>sits_FCN</code>)</li>
<li>DL using 1D version of ResNet (<code>sits_ResNet</code>)</li>
<li>DL combining 1D CNN and multi-layer perceptron networks (<code>sits_TempCNN</code>)</li>
<li>DL using a combination of long-short term memory (LSTM) and 1D CNN (<code>sits_LSTM-FCN</code>)</li>
</ul>
</div>
<div id="data-used-in-the-machine-learning-examples" class="section level2">
<h2><span class="header-section-number">5.2</span> Data used in the machine learning examples</h2>
<p>For the machine learning examples, we use a data set containing a sits tibble with time series samples from Brazilian Mato Grosso State (Amazon and Cerrado biomes). The samples are from many sources. It has 9 classes (“Cerrado”, “Fallow_Cotton”, “Forest”, “Millet_Cotton”, “Pasture”, “Soy_Corn”, “Soy_Cotton”, “Soy_Fallow”, “Soy_Millet”). Each time series comprehends 12 months (23 data points) from MOD13Q1 product, and has 6 bands (“ndvi”, “evi”, “blue”, “red”, “nir”, “mir”. The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” <span class="citation">(Picoli et al. <a href="#ref-Picoli2018" role="doc-biblioref">2018</a>)</span>, and is available in the R package “sitsdata”, which is downloadable from the website associated to the “e-sensing” project. The examples below use two out of six bands (“ndvi”, “evi”) for training and classification. In practice, we suggest that users include additionally at least the “nir” and “mir” bands.</p>
</div>
<div id="visualizing-samples" class="section level2">
<h2><span class="header-section-number">5.3</span> Visualizing Samples</h2>
<p>One useful way of describing and understanding the samples is by plotting them. A direct way of doing so is using the <code>plot</code> function. When applied to a large data sample, the result is the set of all samples for each label and each band, as shown in the example below, where we plot the raw distribution of the samples with “Forest” label in the “ndvi” band.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-1"></a><span class="kw">data</span>(<span class="st">&quot;samples_matogrosso_mod13q1&quot;</span>)</span>
<span id="cb63-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-2"></a></span>
<span id="cb63-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-3"></a><span class="co"># Select a subset of the samples to be plotted</span></span>
<span id="cb63-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-4"></a><span class="co"># Retrieve the set of samples for the Mato Grosso region </span></span>
<span id="cb63-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-5"></a>samples_matogrosso_mod13q1 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb63-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-6"></a><span class="st">    </span><span class="kw">sits_select</span>(<span class="dt">bands =</span> <span class="st">&quot;NDVI&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb63-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-7"></a><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">filter</span>(label <span class="op">==</span><span class="st"> &quot;Forest&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb63-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb63-8"></a><span class="st">    </span><span class="kw">plot</span>()</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-39-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In the above plot, the thick red line is the median value for each time instance and the yellow lines are the first and third interquartile ranges. Visually, one can see that samples labelled as “Forest” are distinguishable from those of “Cerrado” and “Pasture”; in turn, these latter classes have many similar features and required sophisticated methods for distinction.</p>
<p>An alternative to visualise the samples is to estimate a statistical approximation to an idealized pattern based on a generalised additive model (GAM). A GAM is a linear model in which the linear predictor depends linearly on a smooth function of the predictor variables
<span class="math display">\[
y = \beta_{i} + f(x) + \epsilon, \epsilon \sim N(0, \sigma^2).
\]</span>
The function <code>sits_patterns</code> uses a GAM to predict a smooth, idealized approximation to the time series associated to the each label, for all bands. This function is based on the R package <code>dtwSat</code><span class="citation">(Maus et al. <a href="#ref-Maus2019" role="doc-biblioref">2019</a>)</span>, which implements the TWDTW time series matching method described in <span class="citation">Maus et al. (<a href="#ref-Maus2016" role="doc-biblioref">2016</a>)</span>. The resulting patterns can be viewed using <code>plot</code>.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-1"></a><span class="co"># Select a subset of the samples to be plotted</span></span>
<span id="cb64-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-2"></a>samples_matogrosso_mod13q1 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb64-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-3"></a><span class="st">    </span><span class="kw">sits_patterns</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb64-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb64-4"></a><span class="st">    </span><span class="kw">plot</span>()</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-40-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The resulting plots provide some insights over the time series behaviour of each class. While the response of the “Forest” class is quite distinctive, there are similarities between the double-cropping classes (“Soy-Corn”, “Soy-Millet”, “Soy-Sunflower” and “Soy-Corn”) and between the “Cerrado” and “Pasture” classes. This could suggest that additional information, more bands, or higher-resolution data could be considered to provide a better basis for time series samples that can better distinguish the intended classes. Despite these limitations, the best machine learning algorithms can provide good performance even in the above case.</p>
</div>
<div id="common-interface-to-machine-learning-and-deeplearning-models" class="section level2">
<h2><span class="header-section-number">5.4</span> Common interface to machine learning and deeplearning models</h2>
<p>The SITS package provides a common interface to all machine learning models, using the <code>sits_train</code> function. this function takes two parameters: the input data samples and the ML method (<code>ml_method</code>), as shown below. After the model is estimated, it can be used to classify individual time series or full data cubes using the <code>sits_classify</code> function. In the examples that follow, we show how to apply each method for the classification of a single time series. Then, we disscuss how to classify full data cubes.</p>
<p>When a dataset of time series organised as a SITS tibble is taken as input to the classifier, the result is the same tibble with one additional column (“predicted”), which contains the information on what labels are have been assigned for each interval. The following examples illustrate how to train a dataset and classify an individual time series using the different machine learning techniques. First we use the <code>sits_train</code> function with two parameters: the training dataset (described above) and the chosen machine learning model (in this case, a random forest classifier). The trained model is then used to classify a time series from Mato Grosso Brazilian state, using <code>sits_classify</code>. The results can be shown in text format using the function <code>sits_show_prediction</code> or graphically using <code>plot</code>.</p>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">5.5</span> Random forests</h2>
<p>The Random forest uses the idea of <em>decision trees</em> as its base model. It combines many decision trees via <em>bootstrap</em> procedure and <em>stochastic feature selection</em>, developing a population of somewhat uncorrelated base models. The final classification model is obtained by a majority voting schema. This procedure decreases the classification variance, improving prediction of individual decision trees.</p>
<p>Random forest training process is essentially nondeterministic. It starts by growing trees through repeatedly random sampling-with-replacement the observations set. At each growing tree, the random forest considers only a fraction of the original attributes to decide where to split a node, according to a <em>purity criterion</em>. This criterion is used to identify relevant features and to perform variable selection. This decreases the correlation among trees and improves the prediction performance. Two often-used impurity criteria are the <em>Gini</em> index and the <em>permutation</em> measure. The Gini index considers the contribution of each variable which improves the spliting criteria for building tress. Permutation increases the importance of variables that have a positive effect on the prediction accuracy. The splitting process continues until the tree reaches some given minimum nodes size or a minimum impurity index value.</p>
<p>One of the advantages of the random forest model is that the classification performance is mostly dependent on the number of decision trees to grow and of the “importance” parameter, which controls the purity variable importance measures. SITS provides a <code>sits_rfor</code> function which is a front-end to the <code>ranger</code> package<span class="citation">(Wright and Ziegler <a href="#ref-Wright2017" role="doc-biblioref">2017</a>)</span>; its two main parameters are: <code>num_trees</code> (number of trees to grow) and <code>importance</code>, the variable importance criterion. Possible values for <code>importance</code> are: <code>none</code>, <code>impurity</code> (Gini index), and <code>permutation</code>, the default being <code>impurity</code>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-1"></a><span class="co"># Retrieve the set of samples (provided by EMBRAPA) from the </span></span>
<span id="cb65-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-2"></a><span class="co"># Mato Grosso region for train the Random Forest model.</span></span>
<span id="cb65-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-3"></a>rfor_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, <span class="kw">sits_rfor</span>())</span>
<span id="cb65-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-4"></a><span class="co"># Classify using Random Forest model and plot the result</span></span>
<span id="cb65-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-5"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb65-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-6"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-7"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.2</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-8"></a><span class="st">    </span><span class="kw">sits_classify</span>(rfor_model)</span>
<span id="cb65-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-9"></a><span class="co"># plot classification</span></span>
<span id="cb65-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-10"></a>class.tb <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb65-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb65-11"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/eval-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-1"></a><span class="co"># show the results of the prediction</span></span>
<span id="cb66-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb66-2"></a><span class="kw">sits_show_prediction</span>(class.tb)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 17 x 3
#&gt;    from       to         class   
#&gt;    &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   
#&gt;  1 2000-09-13 2001-08-29 Forest  
#&gt;  2 2001-09-14 2002-08-29 Forest  
#&gt;  3 2002-09-14 2003-08-29 Forest  
#&gt;  4 2003-09-14 2004-08-28 Pasture 
#&gt;  5 2004-09-13 2005-08-29 Pasture 
#&gt;  6 2005-09-14 2006-08-29 Pasture 
#&gt;  7 2006-09-14 2007-08-29 Pasture 
#&gt;  8 2007-09-14 2008-08-28 Pasture 
#&gt;  9 2008-09-13 2009-08-29 Pasture 
#&gt; 10 2009-09-14 2010-08-29 Soy_Corn
#&gt; 11 2010-09-14 2011-08-29 Soy_Corn
#&gt; 12 2011-09-14 2012-08-28 Soy_Corn
#&gt; 13 2012-09-13 2013-08-29 Soy_Corn
#&gt; 14 2013-09-14 2014-08-29 Soy_Corn
#&gt; 15 2014-09-14 2015-08-29 Soy_Corn
#&gt; 16 2015-09-14 2016-08-28 Soy_Corn
#&gt; 17 2016-09-13 2017-08-29 Soy_Corn</code></pre>
<p>The result shows the tendency of the random forest classifier to be robust to outliers and to be able to deal with irrelevant inputs <span class="citation">(Hastie, Tibshirani, and J. <a href="#ref-Hastie2009" role="doc-biblioref">2009</a>)</span>. Performs internal variable selection helps the results be robust to outliers and noise, a common feature in image time series. However, despite being robust, random forest tend to overemphasize some variables and thus rarely turn out to be the classifier with the smallest error. One reason is that the performance of random forest tends to stabilise after a part of the trees are grown <span class="citation">(Hastie, Tibshirani, and J. <a href="#ref-Hastie2009" role="doc-biblioref">2009</a>)</span>. Random forest classifiers can be quite useful to provide a baseline to compare with more sophisticated methods.</p>
</div>
<div id="support-vector-machines" class="section level2">
<h2><span class="header-section-number">5.6</span> Support Vector Machines</h2>
<p>Given a multidimensional data set, the Support Vector Machine (SVM) method finds an optimal separation hyperplane that minimizes misclassifications <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes1995" role="doc-biblioref">1995</a>)</span>. Hyperplanes are linear <span class="math inline">\({(p-1)}\)</span>-dimensional boundaries and define linear partitions in the feature space. The solution for the hyperplane coefficients depends only on those samples that violates the maximum margin criteria, the so-called <em>support vectors</em>. All other points far away from the hyperplane does not exert any influence on the hyperplane coefficients which let SVM less sensitive to outliers.</p>
<p>For data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. In this manner, the new classification model, despite having a linear boundary on the enlarged feature space, generally translates its hyperplane to a nonlinear boundaries in the original attribute space. The use of kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space an hence can improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been widely applied to classify remote sensing data <span class="citation">(Mountrakis, Im, and Ogole <a href="#ref-Mountrakis2011" role="doc-biblioref">2011</a>)</span>.</p>
<p>In <code>sits</code>, SVM is implemented as a wrapper of <code>e1071</code> R package that uses the <code>LIBSVM</code> implementation <span class="citation">(Chang and Lin <a href="#ref-Chang2011" role="doc-biblioref">2011</a>)</span>, <code>sits</code> adopts the <em>one-against-one</em> method for multiclass classification. For a <span class="math inline">\(q\)</span> class problem, this method creates <span class="math inline">\({q(q-1)/2}\)</span> SVM binary models, one for each class pair combination and tests any unknown input vectors throughout all those models. The overall result is computed by a voting scheme.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-1"></a><span class="co"># Train a machine learning model for the mato grosso dataset using SVM</span></span>
<span id="cb68-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-2"></a><span class="co"># The parameters are those of the &quot;e1071:svm&quot; method</span></span>
<span id="cb68-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-3"></a>svm_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb68-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-4"></a>                        <span class="dt">ml_method =</span> <span class="kw">sits_svm</span>(<span class="dt">kernel =</span> <span class="st">&quot;radial&quot;</span>,</span>
<span id="cb68-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-5"></a>                                             <span class="dt">cost =</span> <span class="dv">10</span>))</span>
<span id="cb68-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-6"></a><span class="co"># Classify using SVM model and plot the result</span></span>
<span id="cb68-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-7"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb68-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-8"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.25</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb68-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-9"></a><span class="st">    </span><span class="kw">sits_classify</span>(svm_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb68-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb68-10"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-42-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The result is mostly consistent of what one could expect by visualising the time series. The area started out as a forest in 2000, it was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2008 onwards. However, the result shows some inconsistencies. First, since the training dataset does not contain a samples of deforested areas, places where forest is removed will tend to be classified as “Cerrado”, which is the nearest kind of vegetation cover where trees and grasslands are mixed. This misinterpretation needs to be corrected in post-processing by applying a time-dependent rule (see the main SITS vignette and the post-processing methods vignette). Also, the classification for year 2009 is “Soy-Millet”, which is different from the “Soy-Corn” label assigned from the other years from 2008 to 2017. To test if this result is inconsistent, one could apply spatial post-processing techniques, as discussed in the main SITS vignette and in the post-processing one.</p>
<p>One of the drawbacks of using the <code>sits_svm</code> method is its sensitivity to its parameters. Using a linear or a polynomial kernel fails to produce good results. If one varies the parameter <code>cost</code> (cost of contraints violation) from 100 to 1, the results can be strinkgly different. Such sensitity to the input parameters points to a limitation when using the SVM method for classifying time series.</p>
</div>
<div id="extreme-gradient-boosting" class="section level2">
<h2><span class="header-section-number">5.7</span> Extreme Gradient Boosting</h2>
<p>Boosting techniques are based on the idea of starting from a weak predictor and then improving performance sequentially by fitting better model at each iteration. It starts by fitting a simple classifier to the training data. Then it uses the residuals of the regression to build a better prediction. Typically, the base classifier is a regression tree. Although both random forests and boosting use trees for classification, there is an important difference. In the random forest classifier, the same random logic for tree selections is applied at every step <span class="citation">(Efron and Hastie <a href="#ref-Efron2016" role="doc-biblioref">2016</a>)</span>. Boosting trees are built to improve on previous result, by applying finer divisions that improve the performance. The performance of random forests generally increases with the number of trees until it becomes stable; however, the number of trees grown by boosting techniques cannot be too large, at the risk of overfitting the model.</p>
<p>Gradient boosting is a variant of boosting methods where the cost function is minimized by agradient descent algorithm. Extreme gradient boosting <span class="citation">(Chen and Guestrin <a href="#ref-Chen2016" role="doc-biblioref">2016</a>)</span>, called “XGBoost”, improves by using an efficient approximation to the gradient loss function. The algorithm is fast and accurate. XGBoost is considered one of the best statistical learning algorithms available and has won many competitions; it is generally considered to be better than SVM and random forests. However, actual performance is controlled by the quality of the training dataset.</p>
<p>In SITS, the XGBoost method is implemented by the <code>sits_xbgoost()</code> function, which is based on “XGBoost” R package and has five parameters that require tuning. The learning rate <code>eta</code> varies from 0 to 1, but show be kept small (default is 0.3) to avoid overfitting. The minimim loss value <code>gamma</code> specifies the minimum reduction required to make a split. Its default is 0, but increasing it makes the algorithm more conservative. The maximum depth of a tree <code>max_depth</code> controls how deep tress are to be built. In principle, it should not be largem since higher depth trees lead to overfitting (default is 6.0). The <code>subsample</code> parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The <code>nrounds</code> parameters controls the maximum number of boosting interactions; its default is 100, which has proven to be sufficient in the SITS. In order to follow the convergence of the algorithm, users can turn the <code>verbose</code> parameter on.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-1"></a><span class="co"># Train a machine learning model for the mato grosso dataset using XGBOOST</span></span>
<span id="cb69-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-2"></a><span class="co"># The parameters are those of the &quot;xgboost&quot; package</span></span>
<span id="cb69-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb69-3"></a>xgb_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, <span class="kw">sits_xgboost</span>())</span></code></pre></div>
<pre><code>#&gt; [1]  train-mlogloss:2.005125+0.000446    test-mlogloss:2.019629+0.001908 
#&gt; Multiple eval metrics are present. Will use test_mlogloss for early stopping.
#&gt; Will train until test_mlogloss hasn&#39;t improved in 20 rounds.
#&gt; 
#&gt; [11] train-mlogloss:0.579518+0.002834    test-mlogloss:0.703878+0.017437 
#&gt; [21] train-mlogloss:0.170263+0.000784    test-mlogloss:0.310254+0.024448 
#&gt; [31] train-mlogloss:0.070655+0.000705    test-mlogloss:0.203802+0.025234 
#&gt; [41] train-mlogloss:0.041752+0.000707    test-mlogloss:0.169864+0.027169 
#&gt; [51] train-mlogloss:0.033388+0.000574    test-mlogloss:0.157831+0.028268 
#&gt; [61] train-mlogloss:0.031609+0.000494    test-mlogloss:0.155319+0.028702 
#&gt; [71] train-mlogloss:0.030999+0.000488    test-mlogloss:0.154814+0.028795 
#&gt; [81] train-mlogloss:0.030533+0.000447    test-mlogloss:0.154455+0.028818 
#&gt; [91] train-mlogloss:0.030241+0.000534    test-mlogloss:0.153996+0.028472 
#&gt; [100]    train-mlogloss:0.030032+0.000590    test-mlogloss:0.153889+0.028567</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb71-1"></a><span class="co"># Classify using SVM model and plot the result</span></span>
<span id="cb71-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb71-2"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb71-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb71-3"></a><span class="st">    </span><span class="kw">sits_whittaker</span>(<span class="dt">lambda =</span> <span class="fl">0.25</span>, <span class="dt">bands_suffix =</span> <span class="st">&quot;&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb71-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb71-4"></a><span class="st">    </span><span class="kw">sits_classify</span>(xgb_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb71-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb71-5"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-43-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In general, the results from the extreme gradient boosting model are similar to the Random Forest model. However, for each specific study, users need to perform validation. See the function <code>sits_kfold_validate</code> for more details.</p>
</div>
<div id="deep-learning-using-multi-layer-perceptrons" class="section level2">
<h2><span class="header-section-number">5.8</span> Deep learning using multi-layer perceptrons</h2>
<p>Using the <code>keras</code> package <span class="citation">(Chollet and Allaire <a href="#ref-Chollet2018" role="doc-biblioref">2018</a>)</span> as a backend, SITS supports the following =deep learning techniques, as described in this section and the next ones. The first method is that of feedforward neural networks, or multi-layer perceptron (MLPs). These are the quintessential deep learning models. The goal of a multilayer perceptrons is to approximate some function <span class="math inline">\(f\)</span>. For example, for a classifier <span class="math inline">\(y =f(x)\)</span> maps an input <span class="math inline">\(x\)</span> to a category <span class="math inline">\(y\)</span>. A feedforward network defines a mapping <span class="math inline">\(y = f(x;\theta)\)</span> and learns the value of the parameters <span class="math inline">\(\theta\)</span> that result in the best function approximation. These models are called feedforward because information flows through the function being evaluated from <span class="math inline">\(x\)</span>, through the intermediate computations used to define <span class="math inline">\(f\)</span>, and finally to the output <span class="math inline">\(y\)</span>. There are no feedback connections in which outputs of the model are fed back into itself <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span>.</p>
<p>Specifying a MLP requires some work on customization, which requires some amount of trial-and-error by the user, since there is no proven model for classification of satellite image time series. The most important decision is the number of layers in the model. Initial tests indicate that 3 to 5 layers are enough to produce good results. The choice of the number of layers depends on the inherent separability of the data set to be classified. For data sets where the classes have different signatures, a shallow model (with 3 layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. The user should be aware that some models with many hidden layers may take a long time to train and may not be able to converge. The suggestion is to start with 3 layers and test different options of number of neurons per layer, before increasing the number of layers.</p>
<p>Three other important parameters for an MLP are: (a) the activation function; (b) the optimization method; (c) the dropout rate. The activation function the activation function of a node defines the output of that node given an input or set of inputs. Following standard practices <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span>, we recommend the use of the “relu” and “elu” functions. The optimization method is a crucial choice, and the most common choices are gradient descent algorithm. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function <span class="citation">(Ruder <a href="#ref-Ruder2016" role="doc-biblioref">2016</a>)</span>. Based on experience with image time series, we recommend that users start by using the default method provided by <code>sits</code>, which is the <code>optimizer_adam</code> method. Please refer to the <code>keras</code> package documentation for more information.</p>
<p>The dropout rates have a huge impact on the performance of MLP classifiers. Dropout is a technique for randomly dropping units from the neural network during training <span class="citation">(Srivastava et al. <a href="#ref-Srivastava2014" role="doc-biblioref">2014</a>)</span>. By randomly discarding some neurons, dropout reduces overfitting. It is a counter-intuitive idea that works well. Since the purpose of a cascade of neural nets is to improve learning as more data is acquired, discarding some of these neurons may seem a waste of resources. In fact, as experience has shown <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow2016" role="doc-biblioref">2016</a>)</span>, this procedures prevents an early convergence of the optimization to a local minimum. Thus, in practice, dropout rates between 50% and 20% are recommended for each layer.</p>
<p>In the following example, we classify the same data set using an example of the <code>deep learning</code> method. The parameters for the MLP are: (a) Three layers with 512 neurons each, specified by the parameter <code>layers</code>; (b) Using the ‘elu’ activation function; (c) dropout rates of 50%, 40% and 30% for the layers; (d) the “optimizer_adam” as optimizer (default value); (e) a number of training steps (<code>epochs</code>) of 75; (f) a <code>batch_size</code> of 128, which indicates how many time series are used for input at a given steps; (g) a validation percentage of 20%, which means 20% of the samples will be randomly set side for validation. In practice, users may want to increase the number of epochs and the number of layers. In our experience, if the training dataset is of good quality, using 3 to 5 layers is a reasonable compromise. Further increase on the number of layers will not improve the model. If a better performance is required, users should try to use the convolutional models descibed below. To simplify the vignette generation, the <code>verbose</code> option has been turned off. The default value is on. After the model has been generated, we plot its training history.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-1"></a><span class="co"># train a machine learning model for the Mato Grosso data using an MLP</span></span>
<span id="cb72-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-2"></a>mlp_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb72-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-3"></a>                        <span class="kw">sits_deeplearning</span>(</span>
<span id="cb72-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-4"></a>                        <span class="dt">layers           =</span> <span class="kw">c</span>(<span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">128</span>),</span>
<span id="cb72-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-5"></a>                        <span class="dt">activation       =</span> <span class="st">&quot;elu&quot;</span>,</span>
<span id="cb72-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-6"></a>                        <span class="dt">dropout_rates    =</span> <span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.40</span>, <span class="fl">0.30</span>),</span>
<span id="cb72-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-7"></a>                        <span class="dt">epochs           =</span> <span class="dv">80</span>,</span>
<span id="cb72-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-8"></a>                        <span class="dt">batch_size       =</span> <span class="dv">128</span>,</span>
<span id="cb72-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-9"></a>                        <span class="dt">verbose          =</span> <span class="dv">0</span>,</span>
<span id="cb72-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-10"></a>                        <span class="dt">validation_split =</span> <span class="fl">0.2</span>) )</span>
<span id="cb72-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-11"></a></span>
<span id="cb72-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-12"></a><span class="co"># show training evolution</span></span>
<span id="cb72-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb72-13"></a><span class="kw">plot</span>(mlp_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb73-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb73-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb73-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb73-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-5"></a><span class="st">    </span><span class="kw">sits_classify</span>(mlp_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb73-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb73-6"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-45-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="d-convolutional-neural-networks" class="section level2">
<h2><span class="header-section-number">5.9</span> 1D Convolutional Neural Networks</h2>
<p>Convolutional neural networks (CNN) are a variety of deep learning methods where a convolution filter (sliding window) is applied to the input data. In the case of time series, a 1D CNN works by applying a moving window to the series. Using convolution filters is a way to incorporate temporal autocorrelation information in the classification. The result of the convolution is another time series. <span class="citation">Rußwurm and Korner (<a href="#ref-Russwurm2017" role="doc-biblioref">2017</a>)</span> states that the use of 1D-CNN for time series classification improves on the use of multi-layer perceptrons, since the classifier is able to represent temporal relationships. Also, 1D-CNNs with a suitable convolution window make the classifier more robust to moderate noise, e.g. intermittent presence of clouds.</p>
<p>SITS includes four different variations of 1D-CNN, described in what follows. The first one is a “full Convolutional Neural Network”<span class="citation">(Wang, Yan, and Oates <a href="#ref-Wang2017" role="doc-biblioref">2017</a>)</span>, implemented in the <code>sits_FCN</code> function. FullCNNs are cascading networks, where the size of the input data is kept constant during the convolution. After the nvolutions have been applied, the model includes a global average pooling layer which reduces the number of parameters, and highlights which parts of the input time series contribute the most to the classification <span class="citation">(Fawaz et al. <a href="#ref-Fawaz2019" role="doc-biblioref">2019</a>)</span>. The fullCNN architecture proposed in <span class="citation">Wang, Yan, and Oates (<a href="#ref-Wang2017" role="doc-biblioref">2017</a>)</span> has three convolutional layers. Each layer performs a convolution in its input and uses batch normalization for avoiding premature convergence to a local minimum. Batch normalisation is an alternative to dropout <span class="citation">(Ioffe and Szegedy <a href="#ref-Ioffe2015" role="doc-biblioref">2015</a>)</span>. The result is to a ReLU activation function. The result of the third convolutional block is averaged over the whole time dimension which corresponds to a global average pooling layer. Finally, a traditional softmax classifier is used to get the classification results (see figure below).</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-46"></span>
<img src="images/fullcnn.png" alt="Structure of fullCNN architecture (source: Wang et al.(2017))" width="80%" height="80%" />
<p class="caption">
Figure 5.1: Structure of fullCNN architecture (source: Wang et al.(2017))
</p>
</div>
<p>The <code>sits_FCN</code> function uses the architecture proposed by Wang as its default, and allows the users to experiment with different settings. The <code>layers</code> parameter controls the number of layers and the number of filters in each layer. The <code>kernels</code> parameters controls the size of the convolution kernels for earh layer. In the example below, the first convolution uses 64 filters with a kernel size of 8, followed by a second convolution of 128 filters with a kernel size of 5, and a third and final convolutional layer with 64 filters, each one with a kernel size to 3.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-1"></a><span class="co"># train a machine learning model using deep learning</span></span>
<span id="cb74-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-2"></a>fcn_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb74-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-3"></a>                        <span class="kw">sits_FCN</span>(</span>
<span id="cb74-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-4"></a>                            <span class="dt">layers           =</span> <span class="kw">c</span>(<span class="dv">64</span>, <span class="dv">256</span>, <span class="dv">64</span>),</span>
<span id="cb74-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-5"></a>                            <span class="dt">kernels          =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">3</span>),</span>
<span id="cb74-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-6"></a>                            <span class="dt">activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb74-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-7"></a>                            <span class="dt">L2_rate          =</span> <span class="fl">1e-06</span>,</span>
<span id="cb74-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-8"></a>                            <span class="dt">epochs           =</span> <span class="dv">100</span>,</span>
<span id="cb74-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-9"></a>                            <span class="dt">batch_size       =</span> <span class="dv">128</span>,</span>
<span id="cb74-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-10"></a>                            <span class="dt">verbose          =</span> <span class="dv">0</span>) )</span>
<span id="cb74-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-11"></a><span class="co"># show training evolution</span></span>
<span id="cb74-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb74-12"></a><span class="kw">plot</span>(fcn_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the FCN model</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb75-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb75-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb75-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb75-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-5"></a><span class="st">    </span><span class="kw">sits_classify</span>(fcn_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb75-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb75-6"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-48-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="residual-1d-cnn-networks-resnet" class="section level2">
<h2><span class="header-section-number">5.10</span> Residual 1D CNN Networks (ResNet)</h2>
<p>The Residual Network (ResNet) is a variation of the fullCNN network proposed by <span class="citation">Wang, Yan, and Oates (<a href="#ref-Wang2017" role="doc-biblioref">2017</a>)</span>. ResNet is composed of 11 layers (see figure below). ResNet is a deep network, by default divided in three blocks of three 1D CNN layers each. Each block corresponds to a fullCNN network archicture. The output of each block is combined with a shortcut that links its output to its input. The idea is avoid the so-called “vanishing gradient”, which occurs when a deep learning network is trained based gradient optimization methods<span class="citation">(Hochreiter <a href="#ref-Hochreiter1998" role="doc-biblioref">1998</a>)</span>. As the networks get deeper, otimising them becomes more difficult. Including the input layer of the block at its end is a heuristic that has shown to be effective. In a recent review of time series classification methods using deep learning, Fawaz et al. state the RestNet and fullCNN have the best performance on the UCR/UEA time series test archive <span class="citation">(Fawaz et al. <a href="#ref-Fawaz2019" role="doc-biblioref">2019</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-49"></span>
<img src="images/resnet.png" alt="Structure of ResNet architecture (source: Wang et al.(2017))" width="80%" height="80%" />
<p class="caption">
Figure 5.2: Structure of ResNet architecture (source: Wang et al.(2017))
</p>
</div>
<p>In SITS, the ResNet is implemented using the <code>sits_resnet</code> function. The default parameters are those proposed by <span class="citation">Wang, Yan, and Oates (<a href="#ref-Wang2017" role="doc-biblioref">2017</a>)</span>, and we also benefited from the code provided by <span class="citation">Fawaz et al. (<a href="#ref-Fawaz2019" role="doc-biblioref">2019</a>)</span> ( <a href="https://github.com/hfawaz/dl-4-tsc" class="uri">https://github.com/hfawaz/dl-4-tsc</a>). The first parameter is <code>blocks</code>, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128 filters. Users can control the number of blocks and filter size by changing this parameter. The parameter <code>kernels</code> controls the size the of kernels of the three layers inside each block. We have found out that it is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is “relu”, which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is the same as proposed in <span class="citation">Wang, Yan, and Oates (<a href="#ref-Wang2017" role="doc-biblioref">2017</a>)</span> and <span class="citation">Fawaz et al. (<a href="#ref-Fawaz2019" role="doc-biblioref">2019</a>)</span>. In the case of the 2-band Mato Grosso data set, the estimated accuracy is 95.8%.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-1"></a><span class="co"># train a machine learning model using ResNet</span></span>
<span id="cb76-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb76-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb76-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-4"></a>resnet_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb76-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-5"></a>                       <span class="kw">sits_ResNet</span>(</span>
<span id="cb76-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-6"></a>                          <span class="dt">blocks               =</span> <span class="kw">c</span>(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">64</span>),</span>
<span id="cb76-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-7"></a>                          <span class="dt">kernels              =</span> <span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">3</span>),</span>
<span id="cb76-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-8"></a>                          <span class="dt">activation           =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb76-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-9"></a>                          <span class="dt">epochs               =</span> <span class="dv">100</span>,</span>
<span id="cb76-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-10"></a>                          <span class="dt">batch_size           =</span> <span class="dv">128</span>,</span>
<span id="cb76-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-11"></a>                          <span class="dt">validation_split     =</span> <span class="fl">0.2</span>,</span>
<span id="cb76-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-12"></a>                          <span class="dt">verbose              =</span> <span class="dv">0</span>) )</span>
<span id="cb76-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-13"></a><span class="co"># show training evolution</span></span>
<span id="cb76-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb76-14"></a><span class="kw">plot</span>(resnet_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb77-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb77-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb77-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb77-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-5"></a><span class="st">    </span><span class="kw">sits_classify</span>(resnet_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb77-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb77-6"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-51-1.png" width="70%" style="display: block; margin: auto;" />
## Combined 1D CNN and multi-layer perceptron networks</p>
<p>The combination of 1D CNNs and multi-layer perceptron models for satellite image time series classification was first proposed in <span class="citation">Pelletier, Webb, and Petitjean (<a href="#ref-Pelletier2019" role="doc-biblioref">2019</a>)</span>. The so-called “tempCNN” architecture consists of a number of 1D-CNN layers, similar to the fullCNN model discussed above, whose output is fed into a set of multi-layer perceptrons. The original tempCNN architecture is composed of three 1D convolutional layers (each with 64 units), one dense layer of 256 units and a final softmax layer for classification (see figure). The kernel size of the convolution filters is set to 5. The authors use a combination of different methods to avoid overfitting and reduce the vanishing gradiente effect, including dropout, regularization, and batch normalisation. In the tempCNN paper <span class="citation">(Pelletier, Webb, and Petitjean <a href="#ref-Pelletier2019" role="doc-biblioref">2019</a>)</span>, the authors compare favourably the tempCNN model with the Recurrent Neural Network proposed by <span class="citation">Rußwurm and Körner (<a href="#ref-Russwurm2018" role="doc-biblioref">2018</a>)</span> for land use classification. The figure below shows the architecture of the tempCNN model.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-52"></span>
<img src="images/tempcnn.png" alt="Structure of tempCNN architecture (source: Pelletier et al.(2019))" width="80%" height="80%" />
<p class="caption">
Figure 5.3: Structure of tempCNN architecture (source: Pelletier et al.(2019))
</p>
</div>
<p>The function <code>sits_tempCNN</code> implements the model, using the default parameters proposed by <span class="citation">Pelletier, Webb, and Petitjean (<a href="#ref-Pelletier2019" role="doc-biblioref">2019</a>)</span>. The code has been derived from the Python source provided by the authors (<a href="https://github.com/charlotte-pel/temporalCNN" class="uri">https://github.com/charlotte-pel/temporalCNN</a>). The parameter <code>cnn_layers</code> controls the number of 1D-CNN layers and the size of the filters applied at each layer; the parameter <code>cnn_kernels</code> indicates the size of the convolution kernels. Activation, regularisation for all 1D-CNN layers are set, respectively, by the <code>cnn_activation</code>, <code>cnn_L2_rate</code>. The dropout rates for each 1D-CNN layer are controlled individually by the parameter <code>cnn_dropout_rates</code>. The parameters <code>mlp_layers</code> and <code>mlp_dropout_rates</code> allow the user to set the number and size of the desired MLP layers, as well as their dropout_rates. The activation of the MLP layers is controlled by <code>mlp_activation</code>. By default, the function uses the ADAM optimizer, but any of the optimizers available in the <code>keras</code> package can be used. The <code>validation_split</code> controls the size of the test set, relative to the full data set. We recommend to set aside at least 20% of the samples for validation. In the case of the 2-band Mato Grosso data set, the estimated accuracy is 95.5%.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-1"></a><span class="co"># train a machine learning model using tempCNN</span></span>
<span id="cb78-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-2"></a>tCNN_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb78-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-3"></a>                       <span class="kw">sits_TempCNN</span>(</span>
<span id="cb78-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-4"></a>                          <span class="dt">cnn_layers           =</span> <span class="kw">c</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">32</span>),</span>
<span id="cb78-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-5"></a>                          <span class="dt">cnn_kernels          =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>),</span>
<span id="cb78-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-6"></a>                          <span class="dt">cnn_activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb78-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-7"></a>                          <span class="dt">cnn_L2_rate          =</span> <span class="fl">1e-06</span>,</span>
<span id="cb78-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-8"></a>                          <span class="dt">cnn_dropout_rates    =</span> <span class="kw">c</span>(<span class="fl">0.50</span>, <span class="fl">0.50</span>, <span class="fl">0.50</span>),</span>
<span id="cb78-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-9"></a>                          <span class="dt">mlp_layers           =</span> <span class="kw">c</span>(<span class="dv">256</span>),</span>
<span id="cb78-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-10"></a>                          <span class="dt">mlp_activation       =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb78-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-11"></a>                          <span class="dt">mlp_dropout_rates    =</span> <span class="kw">c</span>(<span class="fl">0.50</span>),</span>
<span id="cb78-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-12"></a>                          <span class="dt">epochs               =</span> <span class="dv">60</span>,</span>
<span id="cb78-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-13"></a>                          <span class="dt">batch_size           =</span> <span class="dv">128</span>,</span>
<span id="cb78-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-14"></a>                          <span class="dt">validation_split     =</span> <span class="fl">0.2</span>,</span>
<span id="cb78-15"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-15"></a>                          <span class="dt">verbose              =</span> <span class="dv">0</span>) )</span>
<span id="cb78-16"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-16"></a></span>
<span id="cb78-17"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-17"></a><span class="co"># show training evolution</span></span>
<span id="cb78-18"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb78-18"></a><span class="kw">plot</span>(tCNN_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb79-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb79-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb79-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb79-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-5"></a><span class="st">    </span><span class="kw">sits_classify</span>(tCNN_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb79-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb79-6"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-54-1.png" width="70%" style="display: block; margin: auto;" />
## LSTM Convolutional Networks for Time Series Classification</p>
<p>Given the success of 1D-CNN networks for time series classification, there have been a number of variants proposed in the literature. One of these variants is the LSTM-CNN network <span class="citation">(Karim et al. <a href="#ref-Karim2018" role="doc-biblioref">2018</a>)</span>, where a fullCNN is combined with long short term memory (LSTM) recurrent neural network. LSTMs are an improved version of recurrent neural networks (RNN). An RNN is a neural network that includes a state vector, which is updated every time step. In this way, a RNN combines an input vector with information that is kept from all previous inputs. One can conceive of RNN as networks that have loops, allowing information to be passed from one step to to the next. In theory, a RNN would be able to handle long-term dependencies between elements of the input vectors. In practice, they are prone to the exhibit the “vanishing gradient” effect. As discussed above, in deep neural networks architectures with gradient descent optimization the gradient function can approach zero, thus impeding training to be done efficiently. LSTM improve on RNN architecture by including the additional feature of being able to regulate whether or not new information should be included on the cell state. LSTM unit also include a forget gate, which is able to discard the previous information stored in the cell state. Thus, a LSTM unit is able to remember values over arbitrary time intervals.</p>
<p><span class="citation">Karim et al. (<a href="#ref-Karim2019b" role="doc-biblioref">2019</a>)</span> consider that LSTM networks are “well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series”. The authors proposed a mixed LSTM-CNN architecture, composed of two paralel data streams: a 3-step CNN such as the one implemented in <code>sits_FCN</code> (see above) combined with a data stream consisting of an LSTM unit, as shown in the figure below. In <span class="citation">Karim et al. (<a href="#ref-Karim2018" role="doc-biblioref">2018</a>)</span>, the authors argue the LSTM-CNN model is capable of a better performance in the UCR/UEA time series test set than architectures such as ResNet and fullCNN.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-55"></span>
<img src="images/lstm-cnn.png" alt="LSTM Fully Convolutional Networks for Time Series Classification (source: Karim et al.(2019))" width="80%" height="80%" />
<p class="caption">
Figure 5.4: LSTM Fully Convolutional Networks for Time Series Classification (source: Karim et al.(2019))
</p>
</div>
<p>In the SITS package, the combined LSTM-CNN architecture is implemented by the <code>sits_LSTM_CNN</code> function. The default values are similar those proposed by <span class="citation">Karim et al. (<a href="#ref-Karim2019b" role="doc-biblioref">2019</a>)</span>. The parameter <code>lstm_units</code> controls the number of units in the LSTM cell at every time step of the network. <span class="citation">Karim et al. (<a href="#ref-Karim2019b" role="doc-biblioref">2019</a>)</span> proposes an LSTM with 8 units, each with a dropout rate of 80%, which are controlled by parameters <code>lstm_units</code> and <code>lstm_dropout</code>. In initial experiments, we got a better performance with an LSTM with 16 units. As proposed by <span class="citation">Karim et al. (<a href="#ref-Karim2019b" role="doc-biblioref">2019</a>)</span>, the CNN layers have filter sizes of {128, 256, 128} and kernel convolution sizes of {8, 5, 3}, controlled by the parameters <code>cnn_layers</code> and <code>cnn_kernels</code>. One should experiment with these parameters, and consider the simulations carried out by <span class="citation">Pelletier, Webb, and Petitjean (<a href="#ref-Pelletier2019" role="doc-biblioref">2019</a>)</span> (see above), where the authors found that an FCN network of sizes {64, 64, 64} with kernels sizes of {5, 5, 5} had best performance in their case study. In this example, the estimated accuracy of the model was 94.7%.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-1"></a>lstm_fcn_model &lt;-<span class="st"> </span><span class="kw">sits_train</span>(samples_matogrosso_mod13q1, </span>
<span id="cb80-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-2"></a>                       <span class="kw">sits_LSTM_FCN</span>(</span>
<span id="cb80-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-3"></a>                          <span class="dt">lstm_units          =</span> <span class="dv">16</span>,</span>
<span id="cb80-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-4"></a>                          <span class="dt">lstm_dropout        =</span> <span class="fl">0.80</span>,</span>
<span id="cb80-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-5"></a>                          <span class="dt">cnn_layers          =</span> <span class="kw">c</span>(<span class="dv">64</span>, <span class="dv">64</span>, <span class="dv">64</span>),</span>
<span id="cb80-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-6"></a>                          <span class="dt">cnn_kernels         =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">3</span>),</span>
<span id="cb80-7"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-7"></a>                          <span class="dt">activation          =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb80-8"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-8"></a>                          <span class="dt">epochs              =</span> <span class="dv">120</span>,</span>
<span id="cb80-9"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-9"></a>                          <span class="dt">batch_size          =</span> <span class="dv">128</span>,</span>
<span id="cb80-10"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-10"></a>                          <span class="dt">validation_split    =</span> <span class="fl">0.2</span>,</span>
<span id="cb80-11"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-11"></a>                          <span class="dt">verbose             =</span> <span class="dv">0</span>) )</span>
<span id="cb80-12"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-12"></a></span>
<span id="cb80-13"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-13"></a><span class="co"># show training evolution</span></span>
<span id="cb80-14"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb80-14"></a><span class="kw">plot</span>(lstm_fcn_model)</span></code></pre></div>
<p>Then, we classify a 16-year time series using the DL model</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb81-1"></a><span class="co"># Classify using DL model and plot the result</span></span>
<span id="cb81-2"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb81-2"></a>point_mt_4bands &lt;-<span class="st"> </span><span class="kw">sits_select</span>(point_mt_6bands, </span>
<span id="cb81-3"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb81-3"></a>                               <span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;NDVI&quot;</span>, <span class="st">&quot;EVI&quot;</span>, <span class="st">&quot;NIR&quot;</span>, <span class="st">&quot;MIR&quot;</span>))</span>
<span id="cb81-4"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb81-4"></a>class.tb &lt;-<span class="st"> </span>point_mt_4bands <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb81-5"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb81-5"></a><span class="st">    </span><span class="kw">sits_classify</span>(lstm_fcn_model) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb81-6"><a href="machine-learning-for-data-cubes-using-the-sits-package.html#cb81-6"></a><span class="st">    </span><span class="kw">plot</span>(<span class="dt">bands =</span> <span class="kw">c</span>(<span class="st">&quot;ndvi&quot;</span>, <span class="st">&quot;evi&quot;</span>))</span></code></pre></div>
<p><img src="sitsbook_files/figure-html/unnamed-chunk-57-1.png" width="70%" style="display: block; margin: auto;" /></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Chang2011">
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2011. “LIBSVM: A Library for Support Vector Machines.” <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em> 2 (3): 27.</p>
</div>
<div id="ref-Chen2016">
<p>Chen, Tianqi, and Carlos Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 785–94.</p>
</div>
<div id="ref-Chollet2018">
<p>Chollet, Francois, and J. J. Allaire. 2018. <em>Deep Learning with R</em>. New York, NY: Manning Publications.</p>
</div>
<div id="ref-Cortes1995">
<p>Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” <em>Machine Learning</em> 20 (3): 273–97.</p>
</div>
<div id="ref-Efron2016">
<p>Efron, Bradley, and Trevor Hastie. 2016. <em>Computer Age Statistical Inference</em>. Vol. 5. Cambridge University Press.</p>
</div>
<div id="ref-Fawaz2019">
<p>Fawaz, Hassan Ismail, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. 2019. “Deep Learning for Time Series Classification: A Review.” <em>Data Mining and Knowledge Discovery</em> 33 (4): 917–63.</p>
</div>
<div id="ref-Goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div id="ref-Hastie2009">
<p>Hastie, T., R. Tibshirani, and Friedman J. 2009. <em>The Elements of Statistical Learning. Data Mining, Inference, and Prediction</em>. New York: Springer.</p>
</div>
<div id="ref-Hochreiter1998">
<p>Hochreiter, Sepp. 1998. “The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions.” <em>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</em> 6 (02): 107–16.</p>
</div>
<div id="ref-Ioffe2015">
<p>Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” <em>arXiv Preprint arXiv:1502.03167</em>.</p>
</div>
<div id="ref-Karim2018">
<p>Karim, Fazle, Somshubra Majumdar, Houshang Darabi, and Shun Chen. 2018. “LSTM Fully Convolutional Networks for Time Series Classification.” <em>IEEE Access</em> 6: 1662–9.</p>
</div>
<div id="ref-Karim2019b">
<p>Karim, Fazle, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. 2019. “Multivariate LSTM-FCNS for Time Series Classification.” <em>Neural Networks</em> 116: 237–45.</p>
</div>
<div id="ref-Maus2016">
<p>Maus, Victor, Gilberto Camara, Ricardo Cartaxo, Alber Sanchez, Fernando M Ramos, and Gilberto R de Queiroz. 2016. “A Time-Weighted Dynamic Time Warping Method for Land-Use and Land-Cover Mapping.” <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em> 9 (8): 3729–39.</p>
</div>
<div id="ref-Maus2019">
<p>Maus, Victor, Gilberto Câmara, Marius Appel, and Edzer Pebesma. 2019. “DtwSat: Time-Weighted Dynamic Time Warping for Satellite Image Time Series Analysis in R.” <em>Journal of Statistical Software</em> 88 (5): 1–31.</p>
</div>
<div id="ref-Maxwell2018">
<p>Maxwell, Aaron E., Timothy A. Warner, and Fang Fang. 2018. “Implementation of Machine-Learning Classification in Remote Sensing: An Applied Review.” <em>International Journal of Remote Sensing</em> 39 (9): 2784–2817. <a href="https://doi.org/10.1080/01431161.2018.1433343">https://doi.org/10.1080/01431161.2018.1433343</a>.</p>
</div>
<div id="ref-Mountrakis2011">
<p>Mountrakis, G., J. Im, and C. Ogole. 2011. “Support Vector Machines in Remote Sensing: A Review.” <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 66 (3): 247–59.</p>
</div>
<div id="ref-Pelletier2019">
<p>Pelletier, Charlotte, Geoffrey I Webb, and François Petitjean. 2019. “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series.” <em>Remote Sensing</em> 11 (5): 523.</p>
</div>
<div id="ref-Picoli2018">
<p>Picoli, Michelle Cristina Araujo, Gilberto Camara, Ieda Sanches, Rolf Simões, Alexandre Carvalho, Adeline Maciel, Alexandre Coutinho, et al. 2018. “Big Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture.” <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 145: 328–39.</p>
</div>
<div id="ref-Ruder2016">
<p>Ruder, Sebastian. 2016. “An Overview of Gradient Descent Optimization Algorithms.” <em>arXiv Preprint arXiv:1609.04747</em>.</p>
</div>
<div id="ref-Russwurm2017">
<p>Rußwurm, Marc, and Marco Korner. 2017. “Temporal Vegetation Modelling Using Long Short-Term Memory Networks for Crop Identification from Medium-Resolution Multi-Spectral Satellite Images.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition Workshops</em>, 11–19.</p>
</div>
<div id="ref-Russwurm2018">
<p>Rußwurm, Marc, and Marco Körner. 2018. “Multi-Temporal Land Cover Classification with Sequential Recurrent Encoders.” <em>ISPRS International Journal of Geo-Information</em> 7 (4): 129.</p>
</div>
<div id="ref-Srivastava2014">
<p>Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>The Journal of Machine Learning Research</em> 15 (1): 1929–58.</p>
</div>
<div id="ref-Wang2017">
<p>Wang, Zhiguang, Weizhong Yan, and Tim Oates. 2017. “Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline.” In <em>2017 International Joint Conference on Neural Networks (Ijcnn)</em>, 1578–85. IEEE.</p>
</div>
<div id="ref-Wright2017">
<p>Wright, Marvin, and Andreas Ziegler. 2017. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” <em>Journal of Statistical Software</em> 77 (1): 1–17. <a href="https://doi.org/10.18637/jss.v077.i01">https://doi.org/10.18637/jss.v077.i01</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="time-series-clustering-to-improve-the-quality-of-training-samples.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-of-images-in-data-cubes-using-satellite-image-time-series.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["sitsbook.pdf", "sitsbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
